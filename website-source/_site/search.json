[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Projects",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "index.html#eda",
    "href": "index.html#eda",
    "title": "Projects",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "index.html#ev-insights",
    "href": "index.html#ev-insights",
    "title": "Projects",
    "section": "EV Insights",
    "text": "EV Insights\n\n\n\n\n\n\n\n\n\n\nEV Analysis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "5000-website/ARM/arm.html",
    "href": "5000-website/ARM/arm.html",
    "title": "Association Rule Mining (ARM)",
    "section": "",
    "text": "The feature data ‘X’ for this portion of the analysis consists of text data collected from news articles via the News API. The goal of this analysis is to determine if there are any connections or linkages between different words or phrases when it comes to electric vehicles. Sentiment analysis on the articles is also performed to discern how the media outlets portray these automobile advancements.\nUltimately, I aim to analyze if there are identifiable connections in narrative patterns or sentiment trends. Furthermore, using association rule mining to investigate any correlations amongst certain terms and what they might mean in the given context of the EV industry.",
    "crumbs": [
      "EV Insights",
      "ARM"
    ]
  },
  {
    "objectID": "5000-website/ARM/arm.html#introduction",
    "href": "5000-website/ARM/arm.html#introduction",
    "title": "Association Rule Mining (ARM)",
    "section": "",
    "text": "The feature data ‘X’ for this portion of the analysis consists of text data collected from news articles via the News API. The goal of this analysis is to determine if there are any connections or linkages between different words or phrases when it comes to electric vehicles. Sentiment analysis on the articles is also performed to discern how the media outlets portray these automobile advancements.\nUltimately, I aim to analyze if there are identifiable connections in narrative patterns or sentiment trends. Furthermore, using association rule mining to investigate any correlations amongst certain terms and what they might mean in the given context of the EV industry.",
    "crumbs": [
      "EV Insights",
      "ARM"
    ]
  },
  {
    "objectID": "5000-website/ARM/arm.html#import",
    "href": "5000-website/ARM/arm.html#import",
    "title": "Association Rule Mining (ARM)",
    "section": "Import",
    "text": "Import\nLoad the necessary packages and libraries offered by python to facilitate the NLP and ARM process\n\n\nShow the code\nimport nltk\nimport string\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem import PorterStemmer\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.sentiment import SentimentIntensityAnalyzer\n\nimport os\nimport string\nimport numpy as np\nimport pandas as pd\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.sentiment import SentimentIntensityAnalyzer",
    "crumbs": [
      "EV Insights",
      "ARM"
    ]
  },
  {
    "objectID": "5000-website/ARM/arm.html#read-and-clean-articles",
    "href": "5000-website/ARM/arm.html#read-and-clean-articles",
    "title": "Association Rule Mining (ARM)",
    "section": "Read and clean articles",
    "text": "Read and clean articles\n\n\nShow the code\n# User parameters\ninput_path = '../eda/description.txt'\ncompute_sentiment = True\nsentiment = []  # Average sentiment of each chunk of text\nave_window_size = 5 # Size of scanning window for moving average\n\n# Output file\noutput = 'transactions.txt'\nif os.path.exists(output): os.remove(output)\n\n# Initialize\nnltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('wordnet')\nlemmatizer = WordNetLemmatizer()\nsia = SentimentIntensityAnalyzer()\n\n# Add more stopwords\nstopwords = stopwords.words('english')\nadd = ['mr', 'mrs', 'wa', 'dr', 'said', 'back', 'could', 'one', 'looked', 'like', 'know', 'around', 'dont', 'a', 'the', 'is', 'are']\nstopwords.extend(add)\n\ndef read_and_clean(path, START=0, STOP=None):\n    global sentiment\n\n    with open(path) as file:\n        text = file.read()\n\n    #REMOVE HEADER, AND NEW LINES\n    text = text.replace(\"'\", \"\")  # wasn't --&gt; wasnt\n    lines = text.splitlines()\n    lines = lines[START:STOP] if STOP else lines[START:]\n    text = ' '.join(lines)\n\n    text = ''.join(filter(lambda x: x in string.printable, text))\n\n    sentences = nltk.tokenize.sent_tokenize(text)\n    print(\"NUMBER OF SENTENCES FOUND:\", len(sentences))\n\n    #CLEAN AND LEMMATIZE\n    keep = '0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ'\n    \n    new_sentences = []\n    vocabulary = []\n\n    for sentence in sentences:\n        new_sentence = ''\n        words = nltk.word_tokenize(sentence)\n\n        for word in words:\n            word = word.lower()  # Convert to lowercase first\n            word = ''.join(char for char in word if char in keep)\n            new_word = lemmatizer.lemmatize(word)\n\n            if new_word not in stopwords and len(new_word) &gt; 0:\n                new_sentence += ',' + new_word if new_sentence else new_word\n                if new_word not in vocabulary:\n                    vocabulary.append(new_word)\n\n        new_sentences.append(new_sentence.split(\",\"))\n\n        if compute_sentiment:\n            sentiment_scores = sia.polarity_scores(sentence)\n            sentiment.append((sentiment_scores['pos'], sentiment_scores['neg'], sentiment_scores['neu'], sentiment_scores['compound']))\n\n        if len(new_sentence.split(',')) &gt; 2:\n            with open(output, \"a\") as f:\n                f.write(new_sentence + \"\\n\")\n\n    sentiment = np.array(sentiment)\n    if sentiment.size &gt; 0:\n        print(\"TOTAL AVERAGE SENTIMENT:\", np.mean(sentiment, axis=0))\n    else:\n        print(\"TOTAL AVERAGE SENTIMENT: No sentiment data available\")\n    print(\"VOCAB LENGTH\", len(vocabulary))\n    return new_sentences\n\ntransactions = read_and_clean(input_path)\nprint(transactions[:5])\n\n\nNUMBER OF SENTENCES FOUND: 99\nTOTAL AVERAGE SENTIMENT: [0.10641414 0.02876768 0.86480808 0.3293798 ]\nVOCAB LENGTH 1089\n[['ahead', 'sixthgeneration', 'honda', 'crvs', 'malaysian', 'launch', 'expected', 'later', 'month', 'honda', 'malaysia', 'ha', 'previewing', 'suv', 'via', 'series', 'customer', 'showcase', 'selected', 'dealership', 'across', 'country'], ['preview', 'post', '2024', 'honda', 'cr', 'bn'], ['frank', 'recently', 'reported', 'electric', 'vehicle', 'ev', 'sale', 'way', 'u', 'vehicle', 'continue', 'city', 'unveils', 'section', 'road', 'allows', 'ev', 'driver', 'wirelessly', 'charge', 'vehicle', 'drive', 'lcnb', 'corp', 'decreased', 'holding', 'tesla', 'inc', 'nasdaq', 'tsla', 'free', 'report', '176', 'second', 'quarter', 'according', 'recent', 'filing', 'security', 'exchange', 'commission'], ['firm', 'owned', '4270', 'share', 'electric', 'vehicle', 'producer', 'stock', 'many', 'younger', 'buyer', 'opting', 'preowned', 'car', 'new', '45m', 'sold', 'india', 'last', 'year'], ['startup', 'usedcar', 'segment', 'focusing', 'techbased', 'solution', 'ancillary', 'service', 'unit', 'economics', 'tap', 'largely', 'unorganised', 'market']]\n\n\n[nltk_data] Downloading package punkt to\n[nltk_data]     /Users/isfarbaset/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /Users/isfarbaset/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to\n[nltk_data]     /Users/isfarbaset/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!",
    "crumbs": [
      "EV Insights",
      "ARM"
    ]
  },
  {
    "objectID": "5000-website/ARM/arm.html#visualize-sentiment",
    "href": "5000-website/ARM/arm.html#visualize-sentiment",
    "title": "Association Rule Mining (ARM)",
    "section": "Visualize sentiment",
    "text": "Visualize sentiment\n\n\nShow the code\n# see what the sentiment tuple looks like\nprint(sentiment[:5])\n\n\n[[0.     0.     1.     0.    ]\n [0.     0.     1.     0.    ]\n [0.074  0.     0.926  0.6705]\n [0.073  0.     0.927  0.296 ]\n [0.075  0.     0.925  0.1779]]\n\n\n\n\nShow the code\n# Assign positive and negetive sentiments\npositive_sentiment = [score[0] for score in sentiment]\nnegative_sentiment = [score[1] for score in sentiment]\n\n\n\n\nShow the code\ndef moving_ave(y, w=100):\n    # Compute the moving average of a signal y\n    # code ref:https://stackoverflow.com/questions/13728392/moving-average-or-running-mean\n    return np.convolve(y, np.ones(w)/w, mode='valid')\n\n\n\n\nShow the code\npositive_moving_avg = moving_ave(positive_sentiment, ave_window_size)\nnegative_moving_avg = moving_ave(negative_sentiment, ave_window_size)",
    "crumbs": [
      "EV Insights",
      "ARM"
    ]
  },
  {
    "objectID": "5000-website/ARM/arm.html#re-format-output",
    "href": "5000-website/ARM/arm.html#re-format-output",
    "title": "Association Rule Mining (ARM)",
    "section": "Re-format output",
    "text": "Re-format output\n\n\nShow the code\ndef reformat_results(results):\n\n    #CLEAN-UP RESULTS \n    keep=[]\n    for i in range(0,len(results)):\n        # print(\"=====================================\")\n        # print(results[i])\n        # print(len(list(results[i])))\n        for j in range(0,len(list(results[i]))):\n            # print(results)\n            if(j&gt;1):\n                for k in range(0,len(list(results[i][j]))):\n                    if(len(results[i][j][k][0])!=0):\n                        #print(len(results[i][j][k][0]),results[i][j][k][0])\n                        rhs=list(results[i][j][k][0])\n                        lhs=list(results[i][j][k][1])\n                        conf=float(results[i][j][k][2])\n                        lift=float(results[i][j][k][3])\n                        keep.append([rhs,lhs,supp,conf,supp*conf,lift])\n                        # keep.append()\n            if(j==1):\n                supp=results[i][j]\n\n    return pd.DataFrame(keep, columns =[\"rhs\",\"lhs\",\"supp\",\"conf\",\"supp x conf\",\"lift\"])",
    "crumbs": [
      "EV Insights",
      "ARM"
    ]
  },
  {
    "objectID": "5000-website/ARM/arm.html#utility-function-convert-to-networkx-object",
    "href": "5000-website/ARM/arm.html#utility-function-convert-to-networkx-object",
    "title": "Association Rule Mining (ARM)",
    "section": "Utility function: Convert to NetworkX object",
    "text": "Utility function: Convert to NetworkX object\n\n\nShow the code\ndef convert_to_network(df):\n    print(df)\n\n    #BUILD GRAPH\n    G = nx.DiGraph()  # DIRECTED\n    for row in df.iterrows():\n        # for column in df.columns:\n        lhs=\"_\".join(row[1][0])\n        rhs=\"_\".join(row[1][1])\n        conf=row[1][3]; #print(conf)\n        if(lhs not in G.nodes): \n            G.add_node(lhs)\n        if(rhs not in G.nodes): \n            G.add_node(rhs)\n\n        edge=(lhs,rhs)\n        if edge not in G.edges:\n            G.add_edge(lhs, rhs, weight=conf)\n\n    # print(G.nodes)\n    # print(G.edges)\n    return G",
    "crumbs": [
      "EV Insights",
      "ARM"
    ]
  },
  {
    "objectID": "5000-website/ARM/arm.html#utility-function-plot-networkx-object",
    "href": "5000-website/ARM/arm.html#utility-function-plot-networkx-object",
    "title": "Association Rule Mining (ARM)",
    "section": "Utility function: Plot NetworkX object",
    "text": "Utility function: Plot NetworkX object\n\n\nShow the code\ndef plot_network(G):\n    #SPECIFIY X-Y POSITIONS FOR PLOTTING\n    pos=nx.random_layout(G)\n\n    #GENERATE PLOT\n    fig, ax = plt.subplots()\n    fig.set_size_inches(15, 15)\n\n    #assign colors based on attributes\n    weights_e   = [G[u][v]['weight'] for u,v in G.edges()]\n\n    #SAMPLE CMAP FOR COLORS \n    cmap=plt.cm.get_cmap('Blues')\n    colors_e    = [cmap(G[u][v]['weight']*10) for u,v in G.edges()]\n\n    #PLOT\n    nx.draw(\n    G,\n    edgecolors=\"black\",\n    edge_color=colors_e,\n    node_size=2000,\n    linewidths=2,\n    font_size=8,\n    font_color=\"white\",\n    font_weight=\"bold\",\n    width=weights_e,\n    with_labels=True,\n    pos=pos,\n    ax=ax\n    )\n    ax.set(title='EV Articles')\n    plt.show()",
    "crumbs": [
      "EV Insights",
      "ARM"
    ]
  },
  {
    "objectID": "5000-website/ARM/arm.html#train-arm-model",
    "href": "5000-website/ARM/arm.html#train-arm-model",
    "title": "Association Rule Mining (ARM)",
    "section": "Train ARM model",
    "text": "Train ARM model\n\n\nShow the code\nprint(\"Transactions:\",pd.DataFrame(transactions))\nresults = list(apriori(transactions, min_support=0.1, min_confidence=0.0, min_lift=0, min_length=1))     #RUN APRIORI ALGORITHM\npd_results=reformat_results(results)\nprint(\"Results\\n\",pd_results)\nG=convert_to_network(pd_results.head(200))\n\n\nTransactions:                0                1          2         3          4         5   \\\n0           ahead  sixthgeneration      honda      crvs  malaysian    launch   \n1         preview             post       2024     honda         cr        bn   \n2           frank         recently   reported  electric    vehicle        ev   \n3            firm            owned       4270     share   electric   vehicle   \n4         startup          usedcar    segment  focusing  techbased  solution   \n..            ...              ...        ...       ...        ...       ...   \n94  institutional         investor     bought      2805      share       com   \n95            get            recap     recent    health   economic   science   \n96            era           moving   molecule    energy     coming       end   \n97         moving         electron  efficient     lower     impact      None   \n98       electric          vehicle     proved       far         le  reliable   \n\n               6                7           8          9   ...    48    49  \\\n0        expected            later       month      honda  ...  None  None   \n1            None             None        None       None  ...  None  None   \n2            sale              way           u    vehicle  ...  None  None   \n3        producer            stock        many    younger  ...  None  None   \n4       ancillary          service        unit  economics  ...  None  None   \n..            ...              ...         ...        ...  ...   ...   ...   \n94            get            recap      recent     health  ...  None  None   \n95  environmental            story     special    edition  ...  None  None   \n96      electrify       everything  everywhere       None  ...  None  None   \n97           None             None        None       None  ...  None  None   \n98        average  gasolinepowered         car      truck  ...  None  None   \n\n      50    51    52    53    54    55    56    57  \n0   None  None  None  None  None  None  None  None  \n1   None  None  None  None  None  None  None  None  \n2   None  None  None  None  None  None  None  None  \n3   None  None  None  None  None  None  None  None  \n4   None  None  None  None  None  None  None  None  \n..   ...   ...   ...   ...   ...   ...   ...   ...  \n94  None  None  None  None  None  None  None  None  \n95  None  None  None  None  None  None  None  None  \n96  None  None  None  None  None  None  None  None  \n97  None  None  None  None  None  None  None  None  \n98  None  None  None  None  None  None  None  None  \n\n[99 rows x 58 columns]\nResults\n                                                       rhs           lhs  \\\n0                                                   [13f]   [according]   \n1                                             [according]         [13f]   \n2                                                   [13f]      [filing]   \n3                                                [filing]         [13f]   \n4                                                   [13f]        [free]   \n...                                                   ...           ...   \n105249  [nasdaq, commission, free, recent, quarter, ex...         [inc]   \n105250  [according, commission, recent, inc, quarter, ...        [free]   \n105251  [according, commission, free, recent, inc, qua...    [exchange]   \n105252  [according, free, recent, inc, quarter, exchan...  [commission]   \n105253  [commission, free, recent, inc, quarter, excha...   [according]   \n\n            supp      conf  supp x conf      lift  \n0       0.101010  1.000000     0.101010  4.304348  \n1       0.101010  0.434783     0.043917  4.304348  \n2       0.101010  1.000000     0.101010  5.823529  \n3       0.101010  0.588235     0.059418  5.823529  \n4       0.101010  1.000000     0.101010  3.807692  \n...          ...       ...          ...       ...  \n105249  0.111111  1.000000     0.111111  4.950000  \n105250  0.111111  1.000000     0.111111  3.807692  \n105251  0.111111  1.000000     0.111111  5.823529  \n105252  0.111111  1.000000     0.111111  5.823529  \n105253  0.111111  1.000000     0.111111  4.304348  \n\n[105254 rows x 6 columns]\n                     rhs                lhs     supp      conf  supp x conf  \\\n0                  [13f]        [according]  0.10101  1.000000     0.101010   \n1            [according]              [13f]  0.10101  0.434783     0.043917   \n2                  [13f]           [filing]  0.10101  1.000000     0.101010   \n3               [filing]              [13f]  0.10101  0.588235     0.059418   \n4                  [13f]             [free]  0.10101  1.000000     0.101010   \n..                   ...                ...      ...       ...          ...   \n195     [according, 13f]           [filing]  0.10101  1.000000     0.101010   \n196        [filing, 13f]        [according]  0.10101  1.000000     0.101010   \n197  [filing, according]              [13f]  0.10101  0.588235     0.059418   \n198                [13f]  [free, according]  0.10101  1.000000     0.101010   \n199          [according]        [free, 13f]  0.10101  0.434783     0.043917   \n\n         lift  \n0    4.304348  \n1    4.304348  \n2    5.823529  \n3    5.823529  \n4    3.807692  \n..        ...  \n195  5.823529  \n196  4.304348  \n197  5.823529  \n198  4.714286  \n199  4.304348  \n\n[200 rows x 6 columns]",
    "crumbs": [
      "EV Insights",
      "ARM"
    ]
  },
  {
    "objectID": "5000-website/ARM/arm.html#visualize-the-results",
    "href": "5000-website/ARM/arm.html#visualize-the-results",
    "title": "Association Rule Mining (ARM)",
    "section": "Visualize the results",
    "text": "Visualize the results",
    "crumbs": [
      "EV Insights",
      "ARM"
    ]
  },
  {
    "objectID": "5000-website/about.html",
    "href": "5000-website/about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "5000-website/index.html",
    "href": "5000-website/index.html",
    "title": "EV Analysis",
    "section": "",
    "text": "The data question I’m seeking to explore is related to Electric Vehicles: their environmental impact, market share, and performance compared to gasoline vehicles. Over the past few years, auto manufacturers have started to add EVs to their lineup in an effort to meet the rising demand. This shift in consumer demand gives rise to many interesting questions to explore.",
    "crumbs": [
      "EV Insights",
      "Introduction"
    ]
  },
  {
    "objectID": "5000-website/index.html#introduction",
    "href": "5000-website/index.html#introduction",
    "title": "EV Analysis",
    "section": "",
    "text": "The data question I’m seeking to explore is related to Electric Vehicles: their environmental impact, market share, and performance compared to gasoline vehicles. Over the past few years, auto manufacturers have started to add EVs to their lineup in an effort to meet the rising demand. This shift in consumer demand gives rise to many interesting questions to explore.",
    "crumbs": [
      "EV Insights",
      "Introduction"
    ]
  },
  {
    "objectID": "5000-website/index.html#data-analyses-procedure",
    "href": "5000-website/index.html#data-analyses-procedure",
    "title": "EV Analysis",
    "section": "Data Analyses Procedure",
    "text": "Data Analyses Procedure\nTo answer some of the questions arising from the gradual dominance of electric vehicles in the market, several data processing steps will be taken. The tabs on the left list out the steps of taken form the data storytelling jourey. I start off my doing some inital research to formulate questions to start off. The journals used for the inital research and the questions formed based on it, are listed below. Here’s what to expect from each subsequent section in the report:\nData Gathering: contains the various sources that were used to collect the relevant datasets to answer some of the questions. The data sources inclue Yahoo Finance, Rapid API, Cars API, News API and Wikipedia. Details of the text data and the record data from these sources are discussed in details. This is the heart of the portfolio since it the source that leads to the insights that are eventually gained, allows performing the machine learning algoithms and helps in formulating solid conclusions.\nData Cleaning: The data collected from the various sources were in raw structures. This tab details out the steps taken to shape the raw data files into a cleaner version which could then be used for analysis purposes.\nData Exploration: This tab lists out the exploratory data analysis carried out based on the data collected from the sources. Python and R were used to formulate visualisations and conclusions to extract interesting insights from the data that was collected\nNaïve Bayes: Here record data and text data are classified according the Naïve Bayes theorem. The text data used for this classfication purpose is collected from web crawling wikipedia and looking for content that contains the keywords ‘electric vehicle’ and ‘gasoline-powered’. The objective of carrying out the Naïve Bayes algorithm was to analyze how well the text content can be classified into two distinct groups.\nDimensionality Reduction: The objective of this phase of the project was to evaluate and compare how PCA and t-SNE perform when it comes to reducing dimensions present in large datasets, while still keeping the essential information intact.\nClustering: Kmeans, dbscan and hierchical clustering algorithms carried out to understand the groupings present in the dataset based on the features. Keen on understanding if these mechanisms can take the feature set and clearly identify the distinct types of fuels (gas/electric) the cars have based on those.\nDecision Trees: The objective of this section of the project is to observe how classification and regression methologies can be used in text datasets. The classification decision tree was used to classify the different group of labels ‘electric’, ‘hybrid’ and ‘gasoline’ that exist in the automotive industry and the regression was used on the record dataset that contained information regarding different cars’ quantitative data.\nARM: The goal was to uncover the patterns found amongst different keywords present in news articles surrounding electric vehicles. The words that may be closely related can be indicative regarding the context within which the topic of EVs are used in. It can also provide insight into which words show up often while reporting on EVs, wheather these words have a positive or negative connotation and which direction the conversation needs to be steered to if needed.",
    "crumbs": [
      "EV Insights",
      "Introduction"
    ]
  },
  {
    "objectID": "5000-website/index.html#academic-journal-research",
    "href": "5000-website/index.html#academic-journal-research",
    "title": "EV Analysis",
    "section": "Academic Journal Research",
    "text": "Academic Journal Research\nThe incentive presented by the government greatly impacts the demand of EVs. Government regulation and environmental prospect, in particular will drive the adoption of transportation electrification (Yong et al. 2015). The environmental benefits of EVs immensely influence their dominance in the market. This can be visualized by collecting the financial data available on sources like Yahoo. Electrifying transportation is a promising approach to alleviate the climate change issue (Yong et al. 2015).\nStudies are being carried out to analyze the impact of EVs take up, with focus on economic, environment and technical issues on power grids (Situ 2009). It will lso be intersting to observe the efforsts made by the US to promote EVs compared to the rest of the world and how swiftly these vehicles can become ubiquitous. In fact, as OEM introduce more EV model to the end consumer by 2012, the presents of electric vehicle will be widely seen and recognized (Situ 2009).",
    "crumbs": [
      "EV Insights",
      "Introduction"
    ]
  },
  {
    "objectID": "5000-website/index.html#questions",
    "href": "5000-website/index.html#questions",
    "title": "EV Analysis",
    "section": "Questions",
    "text": "Questions\n10 questions looking to answer so far:\n\n\nCan EVs completely replace gasoline vehicles in the future?\nHow much has the EV market grown in the past 5 years?\nHow would competition drive car prices in the EV market?\nWhere does the US EV market stand compared to other nations?\nWhich group of people are more likely to buy EVs?\nHow will EV impact the price of gasoline?\nHow well do EVs perform compared to a gasoline car?\nWhat is the cost of ownership of a EV vs gasoline car?\nCan everyone afford EVs?\nWhat are the environmental impacts of EVs?\nHow are governments helping to promote EVs?",
    "crumbs": [
      "EV Insights",
      "Introduction"
    ]
  },
  {
    "objectID": "5000-website/data-cleaning/data_cleaning.html",
    "href": "5000-website/data-cleaning/data_cleaning.html",
    "title": "Data Cleaning",
    "section": "",
    "text": "The objective of this section is to carry out some data cleaning procedures on the previously collected text and record data. The datasets used here are the text data collected from News API and the record data collected from Rapid API and Cars API.",
    "crumbs": [
      "EV Insights",
      "Data Cleaning"
    ]
  },
  {
    "objectID": "5000-website/data-cleaning/data_cleaning.html#data-from-rapid-api",
    "href": "5000-website/data-cleaning/data_cleaning.html#data-from-rapid-api",
    "title": "Data Cleaning",
    "section": "Data from Rapid API:",
    "text": "Data from Rapid API:\nWith this dataset we carry out the following data cleaning procedures: - Converting the json file into a dataframe. - Clean the column names by renaming them and stripping any whitespaces. - Drop unneccessary columns - Save the dataframe\n\n\nShow the code\nimport json\nimport pandas as pd\n\n# Load JSON file\nwith open('output.json', 'r') as f:\n    data = json.load(f)\n\n# Create a Pandas DataFrame from the entire JSON data\ndf = pd.json_normalize(data)\n\n# Renaming columns and stripping any spaces\ndf.rename(columns=lambda x: x.strip(), inplace=True)\ndf=df.rename(columns = {'_id':'id'})\ndf=df.rename(columns = {'VIN (1-10)':'VIN'})\n\n# Drop columns that are not needed:\n# df = df.drop(['Base MSRP', 'Legislative District', 'DOL Vehicle ID', 'Vehicle Location', 'Electric Utility', '2020 Census Tract'], axis=1)\n\n# Print the DataFrame\ndisplay(df)\n\n# saving the dataframe\ndf.to_csv('ev-output.csv')\n\n\n\n\n\n\n\n\n\n\nid\nVIN\nCounty\nCity\nState\nPostal Code\nModel Year\nMake\nModel\nElectric Vehicle Type\nClean Alternative Fuel Vehicle (CAFV) Eligibility\nElectric Range\nBase MSRP\nLegislative District\nDOL Vehicle ID\nVehicle Location\nElectric Utility\n2020 Census Tract\n\n\n\n\n0\n64b8150c3f35c83376286f21\n1N4AZ0CP5D\nKitsap\nBremerton\nWA\n98310\n2013\nNISSAN\nLEAF\nBattery Electric Vehicle (BEV)\nClean Alternative Fuel Vehicle Eligible\n75\n0\n23\n214384901\nPOINT (-122.61136499999998 47.575195000000065)\nPUGET SOUND ENERGY INC\n53035080400\n\n\n1\n64b8150c3f35c83376286f22\n1N4AZ1CP8K\nKitsap\nPort Orchard\nWA\n98366\n2019\nNISSAN\nLEAF\nBattery Electric Vehicle (BEV)\nClean Alternative Fuel Vehicle Eligible\n150\n0\n26\n271008636\nPOINT (-122.63926499999997 47.53730000000007)\nPUGET SOUND ENERGY INC\n53035092300\n\n\n2\n64b8150c3f35c83376286f23\n5YJXCAE28L\nKing\nSeattle\nWA\n98199\n2020\nTESLA\nMODEL X\nBattery Electric Vehicle (BEV)\nClean Alternative Fuel Vehicle Eligible\n293\n0\n36\n8781552\nPOINT (-122.394185 47.63919500000003)\nCITY OF SEATTLE - (WA)|CITY OF TACOMA - (WA)\n53033005600\n\n\n3\n64b8150c3f35c83376286f24\nSADHC2S1XK\nThurston\nOlympia\nWA\n98503\n2019\nJAGUAR\nI-PACE\nBattery Electric Vehicle (BEV)\nClean Alternative Fuel Vehicle Eligible\n234\n0\n2\n8308492\nPOINT (-122.8285 47.03646)\nPUGET SOUND ENERGY INC\n53067011628\n\n\n4\n64b8150c3f35c83376286f25\nJN1AZ0CP9B\nSnohomish\nEverett\nWA\n98204\n2011\nNISSAN\nLEAF\nBattery Electric Vehicle (BEV)\nClean Alternative Fuel Vehicle Eligible\n73\n0\n21\n245524527\nPOINT (-122.24128499999995 47.91088000000008)\nPUGET SOUND ENERGY INC\n53061041901\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n95\n64b8150c3f35c83376286f80\n1N4AZ0CP8G\nKing\nBellevue\nWA\n98006\n2016\nNISSAN\nLEAF\nBattery Electric Vehicle (BEV)\nClean Alternative Fuel Vehicle Eligible\n84\n0\n41\n35325\nPOINT (-122.16936999999996 47.571015000000045)\nPUGET SOUND ENERGY INC||CITY OF TACOMA - (WA)\n53033025007\n\n\n96\n64b8150c3f35c83376286f81\n5UXKT0C53J\nYakima\nZillah\nWA\n98953\n2018\nBMW\nX5\nPlug-in Hybrid Electric Vehicle (PHEV)\nNot eligible due to low battery range\n13\n0\n15\n153996167\nPOINT (-120.26033999999999 46.40493500000008)\nPACIFICORP\n53077002201\n\n\n97\n64b8150c3f35c83376286f82\n5YJSA1H23F\nSnohomish\nLynnwood\nWA\n98036\n2015\nTESLA\nMODEL S\nBattery Electric Vehicle (BEV)\nClean Alternative Fuel Vehicle Eligible\n208\n0\n1\n7821215\nPOINT (-122.31667499999998 47.81936500000006)\nPUGET SOUND ENERGY INC\n53061051929\n\n\n98\n64b8150c3f35c83376286f83\n5YJSA1E21J\nSkagit\nAnacortes\nWA\n98221\n2018\nTESLA\nMODEL S\nBattery Electric Vehicle (BEV)\nClean Alternative Fuel Vehicle Eligible\n249\n0\n40\n285304512\nPOINT (-122.61530499999998 48.50127500000008)\nPUGET SOUND ENERGY INC\n53057940600\n\n\n99\n64b8150c3f35c83376286f84\n1G6RL1E40G\nThurston\nRochester\nWA\n98579\n2016\nCADILLAC\nELR\nPlug-in Hybrid Electric Vehicle (PHEV)\nClean Alternative Fuel Vehicle Eligible\n40\n0\n35\n161629142\nPOINT (-123.09574999999995 46.82114000000007)\nPUGET SOUND ENERGY INC\n53067012610\n\n\n\n\n100 rows × 18 columns",
    "crumbs": [
      "EV Insights",
      "Data Cleaning"
    ]
  },
  {
    "objectID": "5000-website/data-cleaning/data_cleaning.html#data-from-cars-api",
    "href": "5000-website/data-cleaning/data_cleaning.html#data-from-cars-api",
    "title": "Data Cleaning",
    "section": "Data from Cars API:",
    "text": "Data from Cars API:\n\n\nShow the code\nrecord_data = pd.read_csv('cars-data.csv')\n\nrecord_data.head()\n\n\n\n\n\n\n\n\n\n\nUnnamed: 0\ncity_mpg\nclass\ncombination_mpg\ncylinders\ndisplacement\ndrive\nfuel_type\nhighway_mpg\nmake\nmodel\ntransmission\nyear\n\n\n\n\n0\n0\n18\nmidsize car\n21\n4.0\n2.2\nfwd\ngas\n26\ntoyota\nCamry\na\n1993\n\n\n1\n1\n19\nmidsize car\n22\n4.0\n2.2\nfwd\ngas\n27\ntoyota\nCamry\nm\n1993\n\n\n2\n2\n16\nmidsize car\n19\n6.0\n3.0\nfwd\ngas\n22\ntoyota\nCamry\na\n1993\n\n\n3\n3\n16\nmidsize car\n18\n6.0\n3.0\nfwd\ngas\n22\ntoyota\nCamry\nm\n1993\n\n\n4\n4\n18\nmidsize-large station wagon\n21\n4.0\n2.2\nfwd\ngas\n26\ntoyota\nCamry\na\n1993\n\n\n\n\n\n\n\n\n\nLook for NaN values\n\n\nShow the code\nnan_count = record_data.isna().sum()\n\nprint(nan_count)\n\n\nUnnamed: 0           0\ncity_mpg             0\nclass                0\ncombination_mpg      0\ncylinders          124\ndisplacement       124\ndrive                8\nfuel_type            0\nhighway_mpg          0\nmake                 0\nmodel                0\ntransmission         0\nyear                 0\ndtype: int64\n\n\n\n\nDouble check datatypes\n\n\nShow the code\nrecord_data.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 719 entries, 0 to 718\nData columns (total 13 columns):\n #   Column           Non-Null Count  Dtype  \n---  ------           --------------  -----  \n 0   Unnamed: 0       719 non-null    int64  \n 1   city_mpg         719 non-null    int64  \n 2   class            719 non-null    object \n 3   combination_mpg  719 non-null    int64  \n 4   cylinders        595 non-null    float64\n 5   displacement     595 non-null    float64\n 6   drive            711 non-null    object \n 7   fuel_type        719 non-null    object \n 8   highway_mpg      719 non-null    int64  \n 9   make             719 non-null    object \n 10  model            719 non-null    object \n 11  transmission     719 non-null    object \n 12  year             719 non-null    int64  \ndtypes: float64(2), int64(5), object(6)\nmemory usage: 73.2+ KB\n\n\n\n\nConvert to desired datatype\n\n\nShow the code\n# Convert all 'object' type columns to 'string'\nfor col in record_data.select_dtypes(include=['object']).columns:\n    record_data[col] = record_data[col].astype('string')\n\n# Verify the changes\nrecord_data.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 719 entries, 0 to 718\nData columns (total 13 columns):\n #   Column           Non-Null Count  Dtype  \n---  ------           --------------  -----  \n 0   Unnamed: 0       719 non-null    int64  \n 1   city_mpg         719 non-null    int64  \n 2   class            719 non-null    string \n 3   combination_mpg  719 non-null    int64  \n 4   cylinders        595 non-null    float64\n 5   displacement     595 non-null    float64\n 6   drive            711 non-null    string \n 7   fuel_type        719 non-null    string \n 8   highway_mpg      719 non-null    int64  \n 9   make             719 non-null    string \n 10  model            719 non-null    string \n 11  transmission     719 non-null    string \n 12  year             719 non-null    int64  \ndtypes: float64(2), int64(5), string(6)\nmemory usage: 73.2 KB\n\n\n\n\nDrop unnecessary columns\n\n\nShow the code\n# Dropping non-numerical and unnecessary columns\nrecord_data = record_data.drop(columns=['Unnamed: 0'])\n\n\n\n\nReplace NaN values\n\n\nShow the code\n# Replace continuous missing values with mean of the column. check for Nan values again.\n\ncols = ['displacement', 'cylinders']\nrecord_data[cols] = record_data[cols].fillna(record_data[cols].mean())\n\nnan_count = record_data.isna().sum()\nprint(nan_count)\n\n\ncity_mpg           0\nclass              0\ncombination_mpg    0\ncylinders          0\ndisplacement       0\ndrive              8\nfuel_type          0\nhighway_mpg        0\nmake               0\nmodel              0\ntransmission       0\nyear               0\ndtype: int64\n\n\n\n\nShow the code\n# Replace categorical missing values with mode of the column. check for Nan values again.\n\nrecord_data['drive'] = record_data['drive'].fillna(record_data['drive'].mode().iloc[0])\n\nnan_count = record_data.isna().sum()\nprint(nan_count)\n\n\ncity_mpg           0\nclass              0\ncombination_mpg    0\ncylinders          0\ndisplacement       0\ndrive              0\nfuel_type          0\nhighway_mpg        0\nmake               0\nmodel              0\ntransmission       0\nyear               0\ndtype: int64",
    "crumbs": [
      "EV Insights",
      "Data Cleaning"
    ]
  },
  {
    "objectID": "5000-website/data-gathering/news-api.html#data-source-news-api",
    "href": "5000-website/data-gathering/news-api.html#data-source-news-api",
    "title": "Gathering Latest EV News Article Data",
    "section": "Data source: News API",
    "text": "Data source: News API\n\n\nShow the code\nimport requests\nimport json\n\n# Base URL for the API endpoint\nurl = 'https://newsapi.org/v2/everything'\n\n# Set all query parameters in a dictionary\nparams = {\n    'q': 'electric-vehicles',  # Updated query\n    'from': '2023-11-04',      # Starting date\n    'sortBy': 'publishedAt',   # Sorting criterion\n    'apiKey': '481b1e4a75874d2f9a23e3329031364c'  \n}\n\n# Make the GET request with the URL and parameters\nresponse = requests.get(url, params=params)\n\n# Convert the response to JSON format\njson_data = response.json()\n\n# Print the JSON response\nprint(json_data)\n\n# Save the results in a JSON file\nwith open('electric_vehicles_news_data.json', 'w') as json_file:\n    json.dump(json_data, json_file, indent=4)\n\nprint(\"Data saved in electric_vehicles_news_data.json\")\n\n\n{'status': 'ok', 'totalResults': 6276, 'articles': [{'source': {'id': None, 'name': \"Paul Tan's Automotive News\"}, 'author': 'Anthony Lim', 'title': '2024 Honda CR-V in Malaysia – initial spec details, four variants, 1.5L S, E, V AWD turbo, 2.0L e:HEV RS hybrid', 'description': 'Ahead of the sixth-generation Honda CR-V‘s Malaysian launch, which is expected later this month, Honda Malaysia has been previewing the SUV via a series of customer showcases at selected dealerships across the country. The previews, […]\\nThe post 2024 Honda CR…', 'url': 'https://paultan.org/2023/12/04/2024-honda-cr-v-in-malaysia-initial-spec-details-four-variants-1-5l-s-e-v-awd-turbo-2-0l-ehev-rs-hybrid/', 'urlToImage': 'https://paultan.org/image/2023/12/2024-Honda-CR-V-Ad-Screenshot-1200x618.jpg', 'publishedAt': '2023-12-03T17:23:43Z', 'content': 'Ahead of the sixth-generation Honda CR-V‘s Malaysian launch, which is expected later this month, Honda Malaysia has been previewing the SUV via a series of customer showcases at selected dealerships … [+6290 chars]'}, {'source': {'id': None, 'name': 'Activistpost.com'}, 'author': 'Activist Post', 'title': 'City Unveils Section of Road that Allows EV Drivers to Wirelessly Charge Their Vehicles as They Drive Over It', 'description': 'By B.N. Frank Recently it was reported that not only electric vehicle (EV) sales are way down in the U.S., the vehicles themselves continue to...\\nCity Unveils Section of Road that Allows EV Drivers to Wirelessly Charge Their Vehicles as They Drive Over It', 'url': 'https://www.activistpost.com/2023/12/city-unveils-section-of-road-that-allows-ev-drivers-to-wirelessly-charge-their-vehicles-as-they-drive-over-it.html', 'urlToImage': 'https://www.activistpost.com/wp-content/uploads/2023/06/electric-vehicle-7-pixa-1024x659-1.jpg', 'publishedAt': '2023-12-03T17:22:14Z', 'content': 'By B.N. Frank\\r\\nRecently it was reported that not only electric vehicle (EV) sales are way down in the U.S., the vehicles themselves continue to be woefully unreliable.\\xa0 Of course, its not breaking ne… [+4811 chars]'}, {'source': {'id': None, 'name': 'Yahoo Entertainment'}, 'author': None, 'title': 'WSJ Opinion: Buyers Just Say No to Electric Vehicles', 'description': None, 'url': 'https://consent.yahoo.com/v2/collectConsent?sessionId=1_cc-session_f23356c2-f7f7-49b1-9b50-6420928f8168', 'urlToImage': None, 'publishedAt': '2023-12-03T17:07:00Z', 'content': 'Si vous cliquez sur «\\xa0Tout accepter\\xa0», nos partenaires (y compris 239 qui font partie du Cadre de transparence et de consentement dIAB) et nous utiliserons également des témoins et vos données person… [+982 chars]'}, {'source': {'id': None, 'name': 'ETF Daily News'}, 'author': 'MarketBeat News', 'title': 'Tesla, Inc. (NASDAQ:TSLA) Shares Sold by Lcnb Corp', 'description': 'Lcnb Corp decreased its holdings in Tesla, Inc. (NASDAQ:TSLA – Free Report) by 17.6% in the second quarter, according to its most recent filing with the Securities and Exchange Commission. The firm owned 4,270 shares of the electric vehicle producer’s stock a…', 'url': 'https://www.etfdailynews.com/2023/12/03/tesla-inc-nasdaqtsla-shares-sold-by-lcnb-corp/', 'urlToImage': 'https://www.americanbankingnews.com/wp-content/timthumb/timthumb.php?src=https://www.marketbeat.com/logos/tesla-inc-logo.png?v=20221020135629&w=240&h=240&zc=2', 'publishedAt': '2023-12-03T16:42:44Z', 'content': 'Lcnb Corp decreased its holdings in Tesla, Inc. (NASDAQ:TSLA – Free Report) by 17.6% in the second quarter, according to its most recent filing with the Securities and Exchange Commission. The firm o… [+6594 chars]'}, {'source': {'id': None, 'name': 'BusinessLine'}, 'author': 'Jyoti Banthia', 'title': 'Steering used-cars sellers onto the profit lane', 'description': 'Many younger buyers are opting for pre-owned cars over new ones, with 4.5M sold in India last year. Start-ups in the used-car segment are focusing on tech-based solutions, ancillary services, and unit economics to tap into the largely unorganised market. Cars…', 'url': 'https://www.thehindubusinessline.com/specials/emerging-entrepreneurs/steering-used-cars-sellers-onto-the-profit-lane/article67601348.ece', 'urlToImage': 'https://bl-i.thgim.com/public/incoming/lg8c8e/article67601372.ece/alternates/LANDSCAPE_1200/used%20car2_Spark.jpg', 'publishedAt': '2023-12-03T16:27:00Z', 'content': 'When 30-year-old Vishal Sangwan, a chartered accountant with a major FMCG company, decided to buy an SUV, he opted for a pre-owned one. The waiting period for a new one was 13 months. I didnt mind bu… [+5632 chars]'}, {'source': {'id': None, 'name': 'The National Interest '}, 'author': 'Brent M. Eastwood', 'title': \"4,200 Bullets Per Minute: The A-10 Warthog's GAU-8 Avenger Gun Is a Monster\", 'description': 'If you are operating an enemy tank, the deep, buzzing belch of cannon fire from an A-10 Thunderbolt II may be the last thing you ever hear.\\nThe A-10, better known as the Warthog, has a rotary cannon called the GAU-8 Avenger that can sustain 600 revolutions an…', 'url': 'https://nationalinterest.org/blog/buzz/4200-bullets-minute-10-warthogs-gau-8-avenger-gun-monster-207697', 'urlToImage': 'https://nationalinterest.org/sites/default/files/main_images/A-10%20Warthog%20%281%29_0.jpg', 'publishedAt': '2023-12-03T16:26:09Z', 'content': 'If you are operating an enemy tank, the deep, buzzing belch of cannon fire from an A-10 Thunderbolt II may be the last thing you ever hear.\\r\\nThe A-10, better known as the Warthog, has a rotary cannon… [+3874 chars]'}, {'source': {'id': None, 'name': 'ETF Daily News'}, 'author': 'MarketBeat News', 'title': 'Albemarle Co. (NYSE:ALB) Shares Acquired by Fund Management at Engine No. 1 LLC', 'description': 'Fund Management at Engine No. 1 LLC lifted its position in shares of Albemarle Co. (NYSE:ALB – Free Report) by 7.5% during the 2nd quarter, according to the company in its most recent filing with the Securities and Exchange Commission. The institutional inves…', 'url': 'https://www.etfdailynews.com/2023/12/03/albemarle-co-nysealb-shares-acquired-by-fund-management-at-engine-no-1-llc/', 'urlToImage': 'https://www.americanbankingnews.com/wp-content/timthumb/timthumb.php?src=https://www.marketbeat.com/logos/albemarle-corp-logo.gif&w=240&h=240&zc=2', 'publishedAt': '2023-12-03T16:18:49Z', 'content': 'Fund Management at Engine No. 1 LLC lifted its position in shares of Albemarle Co. (NYSE:ALB – Free Report) by 7.5% during the 2nd quarter, according to the company in its most recent filing with the… [+6268 chars]'}, {'source': {'id': None, 'name': 'ETF Daily News'}, 'author': 'MarketBeat News', 'title': 'Ghisallo Capital Management LLC Invests $2 Million in EVgo, Inc. (NYSE:EVGO)', 'description': 'Ghisallo Capital Management LLC purchased a new stake in EVgo, Inc. (NYSE:EVGO – Free Report) during the 2nd quarter, according to its most recent disclosure with the Securities and Exchange Commission (SEC). The institutional investor purchased 500,000 share…', 'url': 'https://www.etfdailynews.com/2023/12/03/ghisallo-capital-management-llc-invests-2-million-in-evgo-inc-nyseevgo/', 'urlToImage': 'https://www.americanbankingnews.com/wp-content/timthumb/timthumb.php?src=https://www.marketbeat.com/logos/evgo-inc-logo.png&w=240&h=240&zc=2', 'publishedAt': '2023-12-03T16:08:45Z', 'content': 'Ghisallo Capital Management LLC purchased a new stake in EVgo, Inc. (NYSE:EVGO – Free Report) during the 2nd quarter, according to its most recent disclosure with the Securities and Exchange Commissi… [+3598 chars]'}, {'source': {'id': None, 'name': 'CleanTechnica'}, 'author': 'Zachary Shahan', 'title': 'This Awesome Water Capsule Should Be Electric! Oh, Wait …', 'description': 'There are products that transcend markets, that pull new people into long established markets. Clearly, the products have to be ... [continued]\\nThe post This Awesome Water Capsule Should Be Electric! Oh, Wait … appeared first on CleanTechnica.', 'url': 'https://cleantechnica.com/2023/12/03/this-awesome-water-capsule-should-be-electric-oh-wait/', 'urlToImage': 'https://cleantechnica.com/wp-content/uploads/2023/12/Jet-Capsule-ZERO-5.jpeg', 'publishedAt': '2023-12-03T16:06:24Z', 'content': 'Sign up for daily news updates from CleanTechnica on email. Or follow us on Google News!\\r\\nThere are products that transcend markets, that pull new people into long established markets. Clearly, the p… [+2687 chars]'}, {'source': {'id': None, 'name': 'ETF Daily News'}, 'author': 'MarketBeat News', 'title': 'Union Square Park Capital Management LLC Acquires 438,831 Shares of Garrett Motion Inc. (NYSE:GTX)', 'description': 'Union Square Park Capital Management LLC boosted its holdings in shares of Garrett Motion Inc. (NYSE:GTX – Free Report) by 246.2% during the 2nd quarter, according to its most recent filing with the Securities and Exchange Commission (SEC). The institutional …', 'url': 'https://www.etfdailynews.com/2023/12/03/union-square-park-capital-management-llc-acquires-438831-shares-of-garrett-motion-inc-nysegtx/', 'urlToImage': 'https://www.americanbankingnews.com/wp-content/timthumb/timthumb.php?src=https://www.marketbeat.com/logos/garrett-motion-inc-logo.PNG?v=20201124120545&w=240&h=240&zc=2', 'publishedAt': '2023-12-03T16:04:42Z', 'content': 'Union Square Park Capital Management LLC boosted its holdings in shares of Garrett Motion Inc. (NYSE:GTX – Free Report) by 246.2% during the 2nd quarter, according to its most recent filing with the … [+3790 chars]'}, {'source': {'id': None, 'name': 'ETF Daily News'}, 'author': 'MarketBeat News', 'title': 'Partners Capital Investment Group LLP Takes $532,000 Position in Plug Power Inc. (NASDAQ:PLUG)', 'description': 'Partners Capital Investment Group LLP bought a new stake in Plug Power Inc. (NASDAQ:PLUG – Free Report) in the second quarter, according to its most recent disclosure with the Securities & Exchange Commission. The institutional investor bought 51,249 shares o…', 'url': 'https://www.etfdailynews.com/2023/12/03/partners-capital-investment-group-llp-takes-532000-position-in-plug-power-inc-nasdaqplug/', 'urlToImage': 'https://www.americanbankingnews.com/wp-content/timthumb/timthumb.php?src=https://www.marketbeat.com/logos/plug-power-inc-logo.png?v=20230810085100&w=240&h=240&zc=2', 'publishedAt': '2023-12-03T16:00:45Z', 'content': 'Partners Capital Investment Group LLP bought a new stake in Plug Power Inc. (NASDAQ:PLUG – Free Report) in the second quarter, according to its most recent disclosure with the Securities &amp; Exchan… [+5131 chars]'}, {'source': {'id': None, 'name': 'ETF Daily News'}, 'author': 'MarketBeat News', 'title': 'Partners Capital Investment Group LLP Purchases Shares of 3,125 Tesla, Inc. (NASDAQ:TSLA)', 'description': 'Partners Capital Investment Group LLP purchased a new stake in shares of Tesla, Inc. (NASDAQ:TSLA – Free Report) in the second quarter, according to its most recent filing with the Securities & Exchange Commission. The firm purchased 3,125 shares of the elect…', 'url': 'https://www.etfdailynews.com/2023/12/03/partners-capital-investment-group-llp-purchases-shares-of-3125-tesla-inc-nasdaqtsla/', 'urlToImage': 'https://www.americanbankingnews.com/wp-content/timthumb/timthumb.php?src=https://www.marketbeat.com/logos/tesla-inc-logo.png?v=20221020135629&w=240&h=240&zc=2', 'publishedAt': '2023-12-03T16:00:42Z', 'content': 'Partners Capital Investment Group LLP purchased a new stake in shares of Tesla, Inc. (NASDAQ:TSLA – Free Report) in the second quarter, according to its most recent filing with the Securities &amp; E… [+5831 chars]'}, {'source': {'id': None, 'name': 'Biztoc.com'}, 'author': 'theguardian.com', 'title': 'London’s biggest minicab firm Addison Lee makes emissions U-turn', 'description': 'London’s biggest minicab company has U-turned on plans for all its cars to produce zero emissions this year, blaming a lack of public chargers in the capital. Addison Lee said it had spent £30m on new Volkswagen Multivans, plug-in hybrid electric vehicles (PH…', 'url': 'https://biztoc.com/x/dc80c6c925b7a759', 'urlToImage': 'https://c.biztoc.com/p/dc80c6c925b7a759/s.webp', 'publishedAt': '2023-12-03T16:00:06Z', 'content': 'Londons biggest minicab company has U-turned on plans for all its cars to produce zero emissions this year, blaming a lack of public chargers in the capital.Addison Lee said it had spent £30m on new … [+295 chars]'}, {'source': {'id': None, 'name': 'ETF Daily News'}, 'author': 'MarketBeat News', 'title': 'Mobiv Acquisition Corp (NASDAQ:MOBV) Shares Sold by Kim LLC', 'description': 'Kim LLC cut its position in Mobiv Acquisition Corp (NASDAQ:MOBV – Free Report) by 48.7% in the second quarter, HoldingsChannel reports. The institutional investor owned 158,000 shares of the company’s stock after selling 150,000 shares during the period. Kim …', 'url': 'https://www.etfdailynews.com/2023/12/03/mobiv-acquisition-corp-nasdaqmobv-shares-sold-by-kim-llc/', 'urlToImage': 'https://www.americanbankingnews.com/wp-content/timthumb/timthumb.php?src=https://www.marketbeat.com/logos/mobiv-acquisition-corp-logo.png?v=20231114125739&w=240&h=240&zc=2', 'publishedAt': '2023-12-03T15:58:41Z', 'content': 'Kim LLC cut its position in Mobiv Acquisition Corp (NASDAQ:MOBV – Free Report) by 48.7% in the second quarter, HoldingsChannel reports. The institutional investor owned 158,000 shares of the company’… [+2211 chars]'}, {'source': {'id': None, 'name': 'CNA'}, 'author': None, 'title': \"Serbia's Vucic says electric Fiat Panda model to be produced in Serbia\", 'description': 'BELGRADE : Serbian President Aleksandar Vucic said on Sunday he expects automaker Stellantis to produce an electric Fiat Panda city car in the central town Kragujevac. Vucic who spoke after meeting Italian Prime Minister Giorgia Meloni did not give any specif…', 'url': 'https://www.channelnewsasia.com/business/serbias-vucic-says-electric-fiat-panda-model-be-produced-serbia-3962936', 'urlToImage': 'https://onecms-res.cloudinary.com/image/upload/s--My6gSZwH--/fl_relative,g_south_east,l_one-cms:core:watermark:reuters,w_0.1/f_auto,q_auto/c_fill,g_auto,h_676,w_1200/v1/one-cms/core/2023-12-03t154731z_1_lynxmpejb205i_rtroptp_3_kosovo-serbia-shooting-vucic.jpg?itok=xLjq0k5r', 'publishedAt': '2023-12-03T15:47:31Z', 'content': 'BELGRADE : Serbian President Aleksandar Vucic said on Sunday he expects automaker Stellantis to produce an electric Fiat Panda city car in the central town Kragujevac. \\r\\nVucic who spoke after meeting… [+604 chars]'}, {'source': {'id': None, 'name': 'Marketscreener.com'}, 'author': None, 'title': \"Serbia's Vucic says electric Fiat Panda model to be produced in Serbia\", 'description': '(marketscreener.com) Serbian President Aleksandar\\nVucic said on Sunday he expects automaker Stellantis\\nto produce an electric Fiat Panda city car in the central town\\nKragujevac.\\n Vucic who spoke after meeting Italian Prime Minister Giorgia\\nMeloni did not give…', 'url': 'https://www.marketscreener.com/quote/stock/STELLANTIS-N-V-117814143/news/Serbia-s-Vucic-says-electric-Fiat-Panda-model-to-be-produced-in-Serbia-45490323/', 'urlToImage': 'https://www.marketscreener.com/images/reuters/2023-09/2023-09-28T153843Z_1_LYNXMPEJ8R0UH_RTROPTP_3_KOSOVO-SERBIA-SHOOTING-VUCIC.JPG', 'publishedAt': '2023-12-03T15:42:05Z', 'content': 'BELGRADE, Dec 3 (Reuters) - Serbian President Aleksandar\\r\\nVucic said on Sunday he expects automaker Stellantis\\r\\nto produce an electric Fiat Panda city car in the central town\\r\\nKragujevac.\\r\\nVucic who … [+730 chars]'}, {'source': {'id': None, 'name': 'gcaptain.com'}, 'author': 'Reuters', 'title': 'Exxon Mobil Rebuffs Criticism Of Carbon Capture Strategy', 'description': 'DUBAI, Dec 2 (Reuters)\\xa0–\\xa0Exxon Mobil CEO Darren Woods on Saturday rejected the International Energy Agency’s recent claim that using wide-scale carbon capture to fight climate change was an implausible “illusion,”...', 'url': 'https://gcaptain.com/exxon-mobil-rebuffs-criticism-of-carbon-capture-strategy/', 'urlToImage': 'https://gcaptain.com/wp-content/uploads/2021/04/ExxonMobil-ccs.jpg', 'publishedAt': '2023-12-03T15:22:44Z', 'content': 'DUBAI, Dec 2 (Reuters)\\xa0–\\xa0Exxon Mobil CEO Darren Woods on Saturday rejected the International Energy Agency’s recent claim that using wide-scale carbon capture to fight climate change was an implausib… [+3075 chars]'}, {'source': {'id': None, 'name': 'ETF Daily News'}, 'author': 'MarketBeat News', 'title': 'BRITISH COLUMBIA INVESTMENT MANAGEMENT Corp Sells 10,500 Shares of Nio Inc – (NYSE:NIO)', 'description': 'BRITISH COLUMBIA INVESTMENT MANAGEMENT Corp cut its holdings in Nio Inc – (NYSE:NIO – Free Report) by 1.9% during the 2nd quarter, according to its most recent Form 13F filing with the Securities and Exchange Commission. The firm owned 547,200 shares of the c…', 'url': 'https://www.etfdailynews.com/2023/12/03/british-columbia-investment-management-corp-sells-10500-shares-of-nio-inc-nysenio/', 'urlToImage': 'https://www.americanbankingnews.com/wp-content/timthumb/timthumb.php?src=https://www.marketbeat.com/logos/nio-limited-logo.jpg&w=240&h=240&zc=2', 'publishedAt': '2023-12-03T15:22:41Z', 'content': 'BRITISH COLUMBIA INVESTMENT MANAGEMENT Corp cut its holdings in Nio Inc – (NYSE:NIO – Free Report) by 1.9% during the 2nd quarter, according to its most recent Form 13F filing with the Securities and… [+3884 chars]'}, {'source': {'id': None, 'name': 'CleanTechnica'}, 'author': 'Carolyn Fortuna', 'title': 'GM Expects Its Electric Vehicles To Become Profitable In 2025', 'description': 'When GM chief financial officer Paul Jacobson spoke to analysts at a Barclays conference on November 30, he acknowledged the ... [continued]\\nThe post GM Expects Its Electric Vehicles To Become Profitable In 2025 appeared first on CleanTechnica.', 'url': 'https://cleantechnica.com/2023/12/03/gm-expects-its-electric-vehicles-to-become-profitable-in-2025/', 'urlToImage': 'https://cleantechnica.com/wp-content/uploads/2023/12/2024-Chevrolet-Equinox-EV-3LT-116.jpg', 'publishedAt': '2023-12-03T15:19:58Z', 'content': 'Sign up for daily news updates from CleanTechnica on email. Or follow us on Google News!\\r\\nWhen GM chief financial officer Paul Jacobson spoke to analysts at a Barclays conference on November 30, he a… [+7434 chars]'}, {'source': {'id': None, 'name': 'ETF Daily News'}, 'author': 'MarketBeat News', 'title': 'Kayne Anderson Rudnick Investment Management LLC Grows Position in Tesla, Inc. (NASDAQ:TSLA)', 'description': 'Kayne Anderson Rudnick Investment Management LLC grew its position in shares of Tesla, Inc. (NASDAQ:TSLA – Free Report) by 6.4% during the second quarter, according to its most recent filing with the Securities and Exchange Commission (SEC). The fund owned 3,…', 'url': 'https://www.etfdailynews.com/2023/12/03/kayne-anderson-rudnick-investment-management-llc-grows-position-in-tesla-inc-nasdaqtsla/', 'urlToImage': 'https://www.americanbankingnews.com/wp-content/timthumb/timthumb.php?src=https://www.marketbeat.com/logos/tesla-inc-logo.png?v=20221020135629&w=240&h=240&zc=2', 'publishedAt': '2023-12-03T15:18:52Z', 'content': 'Kayne Anderson Rudnick Investment Management LLC grew its position in shares of Tesla, Inc. (NASDAQ:TSLA – Free Report) by 6.4% during the second quarter, according to its most recent filing with the… [+6081 chars]'}, {'source': {'id': None, 'name': 'ETF Daily News'}, 'author': 'MarketBeat News', 'title': 'Cerity Partners LLC Acquires 26,827 Shares of Genuine Parts (NYSE:GPC)', 'description': 'Cerity Partners LLC boosted its holdings in shares of Genuine Parts (NYSE:GPC – Free Report) by 17.8% in the 2nd quarter, according to its most recent Form 13F filing with the SEC. The firm owned 177,325 shares of the specialty retailer’s stock after buying a…', 'url': 'https://www.etfdailynews.com/2023/12/03/cerity-partners-llc-acquires-26827-shares-of-genuine-parts-nysegpc/', 'urlToImage': 'https://www.americanbankingnews.com/wp-content/timthumb/timthumb.php?src=https://www.marketbeat.com/logos/genuine-parts-company-logo.jpg&w=240&h=240&zc=2', 'publishedAt': '2023-12-03T15:02:43Z', 'content': 'Cerity Partners LLC boosted its holdings in shares of Genuine Parts (NYSE:GPC – Free Report) by 17.8% in the 2nd quarter, according to its most recent Form 13F filing with the SEC. The firm owned 177… [+5066 chars]'}, {'source': {'id': None, 'name': 'ETF Daily News'}, 'author': 'MarketBeat News', 'title': 'BorgWarner Inc. (NYSE:BWA) Shares Bought by Cerity Partners LLC', 'description': 'Cerity Partners LLC boosted its holdings in shares of BorgWarner Inc. (NYSE:BWA – Free Report) by 3.1% in the second quarter, according to its most recent disclosure with the Securities and Exchange Commission (SEC). The firm owned 373,783 shares of the auto …', 'url': 'https://www.etfdailynews.com/2023/12/03/borgwarner-inc-nysebwa-shares-bought-by-cerity-partners-llc/', 'urlToImage': 'https://www.americanbankingnews.com/wp-content/timthumb/timthumb.php?src=https://www.marketbeat.com/logos/Borg_Warner_logo.jpg&w=240&h=240&zc=2', 'publishedAt': '2023-12-03T14:58:41Z', 'content': 'Cerity Partners LLC boosted its holdings in shares of BorgWarner Inc. (NYSE:BWA – Free Report) by 3.1% in the second quarter, according to its most recent disclosure with the Securities and Exchange … [+4822 chars]'}, {'source': {'id': None, 'name': 'ETF Daily News'}, 'author': 'MarketBeat News', 'title': 'Renaissance Technologies LLC Increases Stock Position in Rivian Automotive, Inc. (NASDAQ:RIVN)', 'description': 'Renaissance Technologies LLC grew its stake in shares of Rivian Automotive, Inc. (NASDAQ:RIVN – Free Report) by 159.9% during the second quarter, according to the company in its most recent 13F filing with the Securities & Exchange Commission. The fund owned …', 'url': 'https://www.etfdailynews.com/2023/12/03/renaissance-technologies-llc-increases-stock-position-in-rivian-automotive-inc-nasdaqrivn/', 'urlToImage': 'https://www.americanbankingnews.com/wp-content/timthumb/timthumb.php?src=https://www.marketbeat.com/logos/rivian-logo.png?v=20211118130923&w=240&h=240&zc=2', 'publishedAt': '2023-12-03T14:56:46Z', 'content': 'Renaissance Technologies LLC grew its stake in shares of Rivian Automotive, Inc. (NASDAQ:RIVN – Free Report) by 159.9% during the second quarter, according to the company in its most recent 13F filin… [+5630 chars]'}, {'source': {'id': None, 'name': 'ETF Daily News'}, 'author': 'MarketBeat News', 'title': 'Glenview Trust Co Increases Stock Holdings in Tesla, Inc. (NASDAQ:TSLA)', 'description': 'Glenview Trust Co increased its holdings in Tesla, Inc. (NASDAQ:TSLA – Free Report) by 184.4% in the second quarter, according to its most recent Form 13F filing with the Securities & Exchange Commission. The firm owned 6,175 shares of the electric vehicle pr…', 'url': 'https://www.etfdailynews.com/2023/12/03/glenview-trust-co-increases-stock-holdings-in-tesla-inc-nasdaqtsla/', 'urlToImage': 'https://www.americanbankingnews.com/wp-content/timthumb/timthumb.php?src=https://www.marketbeat.com/logos/tesla-inc-logo.png?v=20221020135629&w=240&h=240&zc=2', 'publishedAt': '2023-12-03T14:46:45Z', 'content': 'Glenview Trust Co increased its holdings in Tesla, Inc. (NASDAQ:TSLA – Free Report) by 184.4% in the second quarter, according to its most recent Form 13F filing with the Securities &amp; Exchange Co… [+5766 chars]'}, {'source': {'id': None, 'name': 'Forbes'}, 'author': 'Brett Owens, Contributor, \\n Brett Owens, Contributor\\n https://www.forbes.com/sites/brettowens/', 'title': 'How To Collect Big Yields With Less Heartburn', 'description': 'One of the best ways to find safe dividends that will protect our principal is in \"low-beta\" stocks.', 'url': 'https://www.forbes.com/sites/brettowens/2023/12/03/how-to-collect-big-yields-with-less-heartburn/', 'urlToImage': 'https://imageio.forbes.com/specials-images/imageserve/6568a2000d6ed1bfd1273708/0x0.jpg?format=jpg&height=900&width=1600&fit=bounds', 'publishedAt': '2023-12-03T14:46:00Z', 'content': 'Man suffering from gastric reflux after dinner\\r\\ngetty\\r\\nFirst-level investors think the key to retiring on dividends alone is to find the largest yields they can and ride them into the sunset.\\r\\nBut wh… [+7439 chars]'}, {'source': {'id': None, 'name': 'CleanTechnica'}, 'author': 'Dr. Maximilian Holland', 'title': 'Record 29.7% EV Share In France — Two Top Ten Teslas', 'description': 'November saw a new record 29.7% EV share in France, up from 24.4% year on year. Full electrics alone took ... [continued]\\nThe post Record 29.7% EV Share In France — Two Top Ten Teslas appeared first on CleanTechnica.', 'url': 'https://cleantechnica.com/2023/12/03/record-29-7-ev-share-in-france/', 'urlToImage': 'https://cleantechnica.com/wp-content/uploads/2023/12/November-2023-France-Passenger-Auto-Registrations-WD.png', 'publishedAt': '2023-12-03T14:37:08Z', 'content': 'Sign up for daily news updates from CleanTechnica on email. Or follow us on Google News!\\r\\nNovember saw a new record 29.7% EV share in France, up from 24.4% year on year. Full electrics alone took ove… [+5239 chars]'}, {'source': {'id': None, 'name': 'ETF Daily News'}, 'author': 'MarketBeat News', 'title': 'Natixis Sells 25,609 Shares of Albemarle Co. (NYSE:ALB)', 'description': 'Natixis reduced its holdings in Albemarle Co. (NYSE:ALB – Free Report) by 81.5% in the second quarter, according to the company in its most recent Form 13F filing with the SEC. The institutional investor owned 5,802 shares of the specialty chemicals company’s…', 'url': 'https://www.etfdailynews.com/2023/12/03/natixis-sells-25609-shares-of-albemarle-co-nysealb/', 'urlToImage': 'https://www.americanbankingnews.com/wp-content/timthumb/timthumb.php?src=https://www.marketbeat.com/logos/albemarle-corp-logo.gif&w=240&h=240&zc=2', 'publishedAt': '2023-12-03T14:32:49Z', 'content': 'Natixis reduced its holdings in Albemarle Co. (NYSE:ALB – Free Report) by 81.5% in the second quarter, according to the company in its most recent Form 13F filing with the SEC. The institutional inve… [+6345 chars]'}, {'source': {'id': None, 'name': 'ETF Daily News'}, 'author': 'MarketBeat News', 'title': 'Critical Analysis: VivoPower International (NASDAQ:VVPR) versus NorthWestern Energy Group (NYSE:NWE)', 'description': 'NorthWestern Energy Group (NYSE:NWE – Get Free Report) and VivoPower International (NASDAQ:VVPR – Get Free Report) are both utilities companies, but which is the superior business? We will contrast the two companies based on the strength of their valuation, a…', 'url': 'https://www.etfdailynews.com/2023/12/03/critical-analysis-vivopower-international-nasdaqvvpr-versus-northwestern-energy-group-nysenwe/', 'urlToImage': 'https://www.americanbankingnews.com/wp-content/timthumb/timthumb.php?src=https://www.marketbeat.com/logos/vivopower-international-plc-logo.png?v=20230124141227&w=240&h=240&zc=2', 'publishedAt': '2023-12-03T14:22:46Z', 'content': 'NorthWestern Energy Group (NYSE:NWE – Get Free Report) and VivoPower International (NASDAQ:VVPR – Get Free Report) are both utilities companies, but which is the superior business? We will contrast t… [+5669 chars]'}, {'source': {'id': None, 'name': 'Biztoc.com'}, 'author': 'fordauthority.com', 'title': 'Ford EVs Could Lose Federal Tax Credits Under New Guidelines', 'description': 'While Ford is still devoting considerable resources toward its transition to fully electric vehicles, pricing remains a major issue for buyers. As Ford Authority previously reported, the company has been steadily adjusting prices for the Ford Mustang Mach-E a…', 'url': 'https://biztoc.com/x/bbb75ab45df37937', 'urlToImage': 'https://c.biztoc.com/p/bbb75ab45df37937/og.webp', 'publishedAt': '2023-12-03T14:08:11Z', 'content': 'While Ford is still devoting considerable resources toward its transition to fully electric vehicles, pricing remains a major issue for buyers. As Ford Authority previously reported, the company has … [+290 chars]'}, {'source': {'id': None, 'name': 'ETF Daily News'}, 'author': 'MarketBeat News', 'title': 'Yamaha Motor Co., Ltd. (OTCMKTS:YAMHF) Short Interest Up 16.9% in November', 'description': 'Yamaha Motor Co., Ltd. (OTCMKTS:YAMHF – Get Free Report) saw a large increase in short interest during the month of November. As of November 15th, there was short interest totalling 2,271,300 shares, an increase of 16.9% from the October 31st total of 1,942,2…', 'url': 'https://www.etfdailynews.com/2023/12/03/yamaha-motor-co-ltd-otcmktsyamhf-short-interest-up-16-9-in-november/', 'urlToImage': 'https://www.americanbankingnews.com/wp-content/timthumb/timthumb.php?src=https://www.marketbeat.com/logos/yamaha-motor-co-logo.png&w=240&h=240&zc=2', 'publishedAt': '2023-12-03T14:00:42Z', 'content': 'Yamaha Motor Co., Ltd. (OTCMKTS:YAMHF – Get Free Report) saw a large increase in short interest during the month of November. As of November 15th, there was short interest totalling 2,271,300 shares,… [+1896 chars]'}, {'source': {'id': None, 'name': 'Autoblog'}, 'author': 'Kim Yonick', 'title': 'Wireless EV charging road opens for testing in Detroit', 'description': 'Filed under:\\n Emerging Technologies,Electric,Infrastructure\\n Continue reading Wireless EV charging road opens for testing in Detroit\\nWireless EV charging road opens for testing in Detroit originally appeared on Autoblog on Sun, 3 Dec 2023 09:00:00 EST. Please…', 'url': 'https://www.autoblog.com/2023/12/03/wireless-ev-charging-road-detroit/', 'urlToImage': 'https://s.aolcdn.com/images/dims3/GLOB/legacy_thumbnail/1062x597/format/jpg/quality/100/https://s.aolcdn.com/os/ab/_cms/2023/11/30133720/mdot-electreon-detroit-wireless-charging-road.png', 'publishedAt': '2023-12-03T14:00:00Z', 'content': 'Get ready for a charging boost in downtown Detroit.\\r\\nOn Nov. 29, the Michigan Department of Transportation (MDOT) installed the first-ever wireless electrified roadway. The technology works similar t… [+2790 chars]'}, {'source': {'id': None, 'name': 'Wattsupwiththat.com'}, 'author': 'Guest Blogger', 'title': 'L A Times Article Reveals California Changed Designation of “Clean Energy” to Enhance its Political Influence at COP28', 'description': 'The L A Times article is full of hand waving climate alarmist politically contrived propaganda claims and assertions that are unsupported by well establish and extensively available energy and climate science measured data.\\xa0\\nThe post L A Times Article Reveals…', 'url': 'https://wattsupwiththat.com/2023/12/03/l-a-times-article-reveals-california-changed-designation-of-clean-energy-to-enhance-its-political-influence-at-cop28/', 'urlToImage': 'https://wattsupwiththat.com/wp-content/uploads/2020/12/wuwt-logo.jpg', 'publishedAt': '2023-12-03T14:00:00Z', 'content': 'Guest essay by Larry Hamlin\\r\\nThe\\xa0L A Times published an article revealing\\xa0that the Government of California had changed its designation of “clean energy” apparently to bolster its political influence… [+18051 chars]'}, {'source': {'id': None, 'name': 'Investing.com'}, 'author': 'Investing.com', 'title': 'This week in EVs: Tesla holds Cybertruck launch event | Pro Recap', 'description': 'This week in EVs: Tesla holds Cybertruck launch event | Pro Recap', 'url': 'https://www.investing.com/news/stock-market-news/this-week-in-evs-tesla-holds-cybertruck-launch-event--pro-recap-3248218', 'urlToImage': 'https://i-invdn-com.investing.com/news/moved_LYNXMPEI0P13A_L.jpg', 'publishedAt': '2023-12-03T13:36:33Z', 'content': \"Here is your weekly Pro Recap of the past week's biggest headlines in the electric vehicle space: Teslaholds Cybertruck event; China EV deliveries; Rivian launches leasing.\\r\\nGet news like this in rea… [+3857 chars]\"}, {'source': {'id': None, 'name': 'Richmond.com'}, 'author': 'PETER VALDES-DAPENA CNN', 'title': '‘Broken window’ stickers for Tesla Cybertruck quickly sell out', 'description': 'Back in 2019, Tesla designer Franz von Holzhausen tried to show off the Cybertruck’s shatterproof windows by throwing a metal ball at each of the truck’s left side windows. The unbreakable windows … broke.', 'url': 'https://richmond.com/news/nation-world/business/tesla-cybertruck-broken-window-stickers-elon-musk/article_c56ecf48-fa91-54f5-87ba-81cdb10cccff.html', 'urlToImage': 'https://bloximages.newyork1.vip.townnews.com/richmond.com/content/tncms/assets/v3/editorial/c/56/c56ecf48-fa91-54f5-87ba-81cdb10cccff/656a8f576cc00.preview.jpg?crop=1768%2C928%2C0%2C141&resize=1200%2C630&order=crop%2Cresize', 'publishedAt': '2023-12-03T13:30:00Z', 'content': \"Tesla Motors was responsible for breaking the EV market open for widespread adoption, and it's currently the most popular electric car maker by a significant margin. Tesla is in the process of manufa… [+5847 chars]\"}, {'source': {'id': None, 'name': 'Livemint'}, 'author': 'Economist', 'title': 'Saudi Arabia wants to become a force in electric-vehicle manufacturing', 'description': 'Its ambitions may yet meet a number of obstacles', 'url': 'https://www.livemint.com/industry/saudi-arabia-wants-to-become-a-force-in-electric-vehicle-manufacturing-11701609045918.html', 'urlToImage': 'https://www.livemint.com/lm-img/img/2023/12/03/1600x900/2021-07-13T230031Z_544563574_RC2ZJO9DI9AH_RTRMADP__1701609372390_1701609372656.JPG', 'publishedAt': '2023-12-03T13:17:14Z', 'content': 'When Saudi Arabias autocratic crown prince, Muhammad bin Salman (known as mbs), last year launched Ceer, the kingdoms first electric-vehicle (ev) brand, his ambition was clear. His country was not ju… [+4015 chars]'}, {'source': {'id': None, 'name': 'Autoblog'}, 'author': 'Stephen Williams', 'title': 'New electric Nissan Juke, two other EVs, scheduled for across the pond', 'description': 'Filed under:\\n Tokyo Auto Salon,Nissan,Crossover,Electric,Future Vehicles\\n Continue reading New electric Nissan Juke, two other EVs, scheduled for across the pond\\nNew electric Nissan Juke, two other EVs, scheduled for across the pond originally appeared on Aut…', 'url': 'https://www.autoblog.com/2023/12/03/electric-nissan-juke/', 'urlToImage': 'https://s.aolcdn.com/images/dims3/GLOB/legacy_thumbnail/1062x597/format/jpg/quality/100/https://s.aolcdn.com/os/ab/_cms/2023/12/01140553/Screenshot-2023-12-01-at-1.55.25\\u202fPM.png', 'publishedAt': '2023-12-03T13:00:00Z', 'content': 'The funky Nissan Juke, which found a mostly friendly audience in the United States until it was discontinued six years ago, will be reborn as an electric vehicle for the European market by the end of… [+1888 chars]'}, {'source': {'id': None, 'name': 'Freerepublic.com'}, 'author': 'Asia Times', 'title': 'Nvidia’s automotive chip sales still riding high in China', 'description': 'Nvidia’s AI processor business in China has been all but shuttered by the US Commerce Department, but its DRIVE Orin system-on-a-chip (SoC) for electric autonomous vehicles is still going strong. More than a dozen corporate customers in China contribute to Nv…', 'url': 'https://freerepublic.com/focus/f-news/4200898/posts', 'urlToImage': None, 'publishedAt': '2023-12-03T12:55:05Z', 'content': 'Skip to comments.\\r\\nNvidias automotive chip sales still riding high in ChinaAsia Times ^Posted on 12/03/2023 4:55:05 AM PST by FarCenter\\r\\nNvidias AI processor business in China has been all but shutte… [+1935 chars]'}, {'source': {'id': None, 'name': 'ETF Daily News'}, 'author': 'MarketBeat News', 'title': 'Kestra Advisory Services LLC Has $1.35 Million Stake in Rivian Automotive, Inc. (NASDAQ:RIVN)', 'description': 'Kestra Advisory Services LLC decreased its stake in shares of Rivian Automotive, Inc. (NASDAQ:RIVN – Free Report) by 1.1% in the 2nd quarter, according to the company in its most recent filing with the Securities & Exchange Commission. The firm owned 81,172 s…', 'url': 'https://www.etfdailynews.com/2023/12/03/kestra-advisory-services-llc-has-1-35-million-stake-in-rivian-automotive-inc-nasdaqrivn/', 'urlToImage': 'https://www.americanbankingnews.com/wp-content/timthumb/timthumb.php?src=https://www.marketbeat.com/logos/rivian-logo.png?v=20211118130923&w=240&h=240&zc=2', 'publishedAt': '2023-12-03T12:31:09Z', 'content': 'Kestra Advisory Services LLC decreased its stake in shares of Rivian Automotive, Inc. (NASDAQ:RIVN – Free Report) by 1.1% in the 2nd quarter, according to the company in its most recent filing with t… [+5844 chars]'}, {'source': {'id': 'the-washington-post', 'name': 'The Washington Post'}, 'author': 'Evan Halper', 'title': 'As the planet heats up, big companies balk on their climate pledges', 'description': 'At a crucial moment in the fight against climate change, the world’s biggest companies are not delivering on their commitments to curb warming.', 'url': 'https://www.washingtonpost.com/business/2023/12/03/climate-corporate-cop28/', 'urlToImage': 'https://www.washingtonpost.com/wp-apps/imrs.php?src=https://arc-anglerfish-washpost-prod-washpost.s3.amazonaws.com/public/72MAKLX6HFC7PVBCYV7TWISEVU.JPG&w=1440', 'publishedAt': '2023-12-03T12:25:47Z', 'content': 'Comment on this story\\r\\nComment\\r\\nAdd to your saved stories\\r\\nSave\\r\\nWhen insurance giant AIG rattled the industry last year with an audacious plan to stop writing policies for some of the most heavily p… [+11763 chars]'}, {'source': {'id': 'cbc-news', 'name': 'CBC News'}, 'author': None, 'title': \"The people vs. the ticket giant; the 'Hunger Games' of air travel: CBC's Marketplace cheat sheet\", 'description': \"CBC's Marketplace rounds up the consumer and health news you need from the week.\", 'url': 'https://www.cbc.ca/news/business/marketplace-cheat-sheet-dec1-1.7046400', 'urlToImage': 'https://i.cbc.ca/1.5057974.1700764325!/fileImage/httpImage/image.jpg_gen/derivatives/16x9_620/packing-for-march-break.jpg', 'publishedAt': '2023-12-03T12:00:00Z', 'content': \"Miss something this week? Don't panic. CBC's\\xa0Marketplace\\xa0rounds up the consumer and health news you need.\\r\\nWant this in your inbox?\\xa0Get the\\xa0Marketplace\\xa0newsletter every Friday.\\r\\nThe people vs. the ti… [+6282 chars]\"}, {'source': {'id': None, 'name': 'AutoExpress'}, 'author': 'Mike Rutherford', 'title': '‘Forget leasing a car, a cut-price van should be your next everyday vehicle’', 'description': 'With some huge savings to be had, Mike Rutherford thinks a van could be the perfect vehicle', 'url': 'https://www.autoexpress.co.uk/opinion/361664/forget-leasing-car-cut-price-van-should-be-your-next-everyday-vehicle', 'urlToImage': 'https://media.autoexpress.co.uk/image/private/s--X-WVjvBW--/f_auto,t_content-image-full-desktop@1/v1701432086/autoexpress/2023/12/Opinion - Fiat Scudo.jpg', 'publishedAt': '2023-12-03T11:08:36Z', 'content': 'Remember the days when you were a kid? With millions of other scruffy, like-minded nippers, you spent dreamy days of childhood vowing to one day own and drive cars built by some of the great motor ma… [+3015 chars]'}, {'source': {'id': None, 'name': 'Wealthofgeeks.com'}, 'author': 'Jarret Hendrickson', 'title': 'Here’s Why Hyundai and Kia’s “Universal” Drivetrain Concept Could Be a Big Step Forward for Their EVs', 'description': \"It's a bold and unprecedented time for the automotive industry. Automakers are pouring billions of dollars into future electric vehicle (EV) battery production facilities and ... Read More\", 'url': 'https://wealthofgeeks.com/heres-why-hyundai-and-kias-universal-drivetrain-concept-could-be-a-big-step-forward-for-their-evs/', 'urlToImage': 'https://wealthofgeeks.com/wp-content/uploads/2023/12/hyundai-ev.jpg', 'publishedAt': '2023-12-03T11:00:44Z', 'content': \"It's a bold and unprecedented time for the automotive industry. Automakers are pouring billions of dollars into future electric vehicle (EV) battery production facilities and charging stations all ov… [+2728 chars]\"}, {'source': {'id': 'newsweek', 'name': 'Newsweek'}, 'author': 'Jake Lingeman', 'title': '2024 Lotus Eletre Review: All-electric, High-tech Simplicity', 'description': \"Lotus' first electric SUV has supercar power and handling with modern charging and operating systems.\", 'url': 'https://www.newsweek.com/2024-lotus-eletre-review-all-electric-high-tech-simplicity-1847711', 'urlToImage': 'https://d.newsweek.com/en/full/2315716/lotus-eletre.jpg', 'publishedAt': '2023-12-03T11:00:01Z', 'content': 'Though millions of sedans are sold each year, SUVs and crossovers are now far more popular prodding even exotic and boutique automakers to produce high-riding vehicles of their own. Ferrari, Lamborgh… [+7107 chars]'}, {'source': {'id': None, 'name': 'Forbes'}, 'author': 'James Morris, Contributor, \\n James Morris, Contributor\\n https://www.forbes.com/sites/jamesmorris/', 'title': 'Tesla Model 3 Highland Test Driven: Finally The Killer Update?', 'description': 'The Highland upgrade is here at last. Is it enough to make the Model 3 a contender again, or will the lack of indicator stalks and ultrasonic sensors put buyers off?', 'url': 'https://www.forbes.com/sites/jamesmorris/2023/12/03/tesla-model-3-highland-test-driven-finally-the-killer-update/', 'urlToImage': 'https://imageio.forbes.com/specials-images/imageserve/656b981a5e8b53c471459804/0x0.jpg?format=jpg&height=900&width=1600&fit=bounds', 'publishedAt': '2023-12-03T10:00:06Z', 'content': 'The Tesla Model 3 is the car that spearheaded the EV revolution. The Model S and X made EVs desirable, while the Nissan Leaf and Renault Zoe showed they could be regular everyday transport. But the M… [+10921 chars]'}, {'source': {'id': None, 'name': 'EURACTIV'}, 'author': 'Alexandra Brzozowski', 'title': 'Will the EU-China summit be another ‘dialogue of the deaf’?', 'description': \"In this week’s edition:\\xa0EU-China summit preview, Hungary doubling down on Ukraine and a leak on the EU executive's defence industry inquiry.\", 'url': 'https://www.euractiv.com/section/global-europe/news/will-the-eu-china-summit-be-another-dialogue-of-the-deaf/', 'urlToImage': 'https://www.euractiv.com/wp-content/uploads/sites/2/2023/12/Example-1-800x450.png', 'publishedAt': '2023-12-03T09:30:40Z', 'content': 'Welcome to EURACTIVs Global Europe Brief, your weekly update on the EU from a global perspective.\\r\\nYou can subscribe to receive our newsletter\\xa0here.\\r\\nIn this weeks edition:\\xa0EU-China summit preview, H… [+10227 chars]'}, {'source': {'id': None, 'name': 'ETF Daily News'}, 'author': 'MarketBeat News', 'title': 'CarMax (NYSE:KMX) PT Lowered to $60.00 at JPMorgan Chase & Co.', 'description': 'CarMax (NYSE:KMX – Free Report) had its target price lowered by JPMorgan Chase & Co. from $70.00 to $60.00 in a research report report published on Wednesday, Benzinga reports. The firm currently has an underweight rating on the stock. Other research analysts…', 'url': 'https://www.etfdailynews.com/2023/12/03/carmax-nysekmx-pt-lowered-to-60-00-at-jpmorgan-chase-co/', 'urlToImage': 'https://www.americanbankingnews.com/wp-content/timthumb/timthumb.php?src=https://www.marketbeat.com/logos/carmax-inc-logo.png?v=20221109132551&w=240&h=240&zc=2', 'publishedAt': '2023-12-03T08:30:44Z', 'content': 'CarMax (NYSE:KMX – Free Report) had its target price lowered by JPMorgan Chase &amp; Co. from $70.00 to $60.00 in a research report report published on Wednesday, Benzinga reports. The firm currently… [+4212 chars]'}, {'source': {'id': None, 'name': 'Autocar'}, 'author': 'Autocar', 'title': \"Driving a Lunaz bin lorry: the UK's biggest new EV\", 'description': 'Lunaz will deliver more than 200 of these next year\\n\\n\\nLunaz is taking knackered diesel bin lorries and giving them a new life as EVs. We take one for a drive \\n\\nNot sure about you, but any vehicle with 5000lb ft of torque making weekly tours of my local street…', 'url': 'https://www.autocar.co.uk/car-news/features/lunaz-upcycled-electric-vehicle-bin-lorry', 'urlToImage': 'https://www.autocar.co.uk/sites/autocar.co.uk/files/images/car-reviews/first-drives/legacy/lunaz_bin_lorry_front_three_quarter_lead.jpg', 'publishedAt': '2023-12-03T08:01:45Z', 'content': 'Not sure about you, but any vehicle with 5000lb ft of torque making weekly tours of my local streets would pique my interest.\\r\\nAfter all, that kind of ship-pulling capability is around double what ev… [+2147 chars]'}, {'source': {'id': None, 'name': 'AllAfrica - Top Africa News'}, 'author': 'africarenewal@un.org (Africa Renewal)', 'title': 'Africa: Africa Day At COP28 to Focus On Financing Climate Action and Green Growth', 'description': '[Africa Renewal] 2 Dec will be Africa Day under the theme \\'Scaling up Financing for Climate Action and Green Growth in Africa\"', 'url': 'https://allafrica.com/stories/202312030015.html', 'urlToImage': 'https://cdn08.allafrica.com/static/images/structure/aa-logo-rgba-no-text-square.png', 'publishedAt': '2023-12-03T07:21:41Z', 'content': '2 Dec will be Africa Day under the theme \\'Scaling up Financing for Climate Action and Green Growth in Africa\"\\r\\nThe African Union Commission, in collaboration with the United Nations Economic Commissi… [+3815 chars]'}, {'source': {'id': None, 'name': 'Wallpaper*'}, 'author': 'Jonathan Bell', 'title': 'This tiny electric delivery van concept could solve a puzzle for urban businesses', 'description': 'HW Electro Puzzle electric delivery van is a multifunctional micro-machine for small businesses of the future', 'url': 'https://www.wallpaper.com/transportation/hw-electro-puzzle-tiny-electric-delivery-van-concept', 'urlToImage': 'https://cdn.mos.cms.futurecdn.net/moknjPtEaFBgxRjeFeQ9N5-1200-80.jpg', 'publishedAt': '2023-12-03T07:00:54Z', 'content': 'Today’s scooter-strewn and lorry-laden streetscapes are a taste of things to come. The city of the future might be not abuzz with flying cars, but it’ll certainly be awash in micro-delivery vehicles.… [+1891 chars]'}, {'source': {'id': 'the-times-of-india', 'name': 'The Times of India'}, 'author': 'IANS', 'title': \"'Electrifying and eventful': Powered by a busy 2023, EV players on a high\", 'description': 'During the year, the Central government had issued notice to several electric two wheeler manufacturers asking them to refund about Rs 500 crore as undue subsidy claims . The companies also refunded the cost of chargers to their customers as that has to be pr…', 'url': 'https://economictimes.indiatimes.com/industry/renewables/electrifying-and-eventful-powered-by-a-busy-2023-ev-players-on-a-high/articleshow/105696759.cms', 'urlToImage': 'https://img.etimg.com/thumb/msid-105696987,width-1200,height-630,imgsize-64072,overlay-economictimes/photo.jpg', 'publishedAt': '2023-12-03T06:37:39Z', 'content': 'The year 2023 could be termed as electrifying and eventful for the Indian electric vehicle (EV) industry. While the industry saw good volume growth, it also saw several government actions, mainly in … [+7298 chars]'}, {'source': {'id': None, 'name': 'Naturalnews.com'}, 'author': 'Ramon Tomey', 'title': 'Auto dealers urge Biden to step on the brakes for ELECTRIC CAR mandates, citing decline in demand', 'description': 'Almost 4,000 automotive dealers called on the Biden administration to “tap the brakes” on its electric vehicle (EV) mandate, citing a drop in demand for electric cars. The group of 3,882 dealers – under the aegis of EV Voice of the Customer – wrote an open le…', 'url': 'https://www.naturalnews.com/2023-12-03-dealers-urge-biden-reconsider-electric-vehicle-mandates.html', 'urlToImage': 'https://www.naturalnews.com/wp-content/uploads/sites/91/2023/12/Electric-Car.jpg', 'publishedAt': '2023-12-03T06:00:00Z', 'content': 'Almost 4,000 automotive dealers called on the Biden administration to \"tap the brakes\" on its electric vehicle (EV) mandate, citing a drop in demand for electric cars.\\r\\nThe group of 3,882 dealers und… [+4462 chars]'}, {'source': {'id': None, 'name': 'Naturalnews.com'}, 'author': 'Ethan Huff', 'title': 'In two years, China plans to unleash mass-produced humanoid robots to replace human workers', 'description': 'By the year 2025, communist China plans to unleash large number of humanoid robots, meaning robots that look and act like people but are just walking computers and circuitry. In a race with Elon Musk’s Tesla and other Western companies working on similar tech…', 'url': 'https://www.naturalnews.com/2023-12-03-china-mass-produce-humanoid-robots-replace-humans.html', 'urlToImage': 'https://www.naturalnews.com/wp-content/uploads/sites/91/2023/12/Drywall-construction-robot.jpg', 'publishedAt': '2023-12-03T06:00:00Z', 'content': 'By the year 2025, communist China plans to unleash large number of humanoid robots, meaning robots that look and act like people but are just walking computers and circuitry.\\r\\nIn a race with Elon Mus… [+3672 chars]'}, {'source': {'id': None, 'name': 'The Japan Times'}, 'author': 'Nithin Coca', 'title': \"How Japan's renewable underestimates are impacting Asia's energy transition\", 'description': 'Before the dust has even settled on key COP28 climate talks in the United Arab Emirates, Japan will host the first summit later this month on a zero carbon emission framework with Australia and Southeast Asian nations in Tokyo.', 'url': 'https://www.japantimes.co.jp/environment/2023/12/03/resources/japan-impact-asia-energy/', 'urlToImage': 'https://www.japantimes.co.jp/japantimes/uploads/images/2023/12/03/266734.jpg', 'publishedAt': '2023-12-03T05:09:00Z', 'content': 'Before the dust has even settled on key COP28 climate talks in the United Arab Emirates, Japan will host the first summit later this month on a zero carbon emission framework with Australia and South… [+10723 chars]'}, {'source': {'id': None, 'name': 'SoraNews24'}, 'author': 'Dale Roll', 'title': 'Now you can request a Tesla when you Uber in Japan with their new Premium EV option', 'description': 'Travel in luxury in the Tokyo area with this new Uber feature! With public transportation so readily available in metropolitan areas in Japan, you may not have a need to call a taxi that often. Now you have an extra reason to, though, as Uber Japan’s recently…', 'url': 'https://soranews24.com/2023/12/03/now-you-can-request-a-tesla-when-you-uber-in-japan-with-their-new-premium-ev-option/', 'urlToImage': 'https://soranews24.com/wp-content/uploads/sites/3/2023/12/uber-premium-ev.jpg?w=580&h=305&crop=1', 'publishedAt': '2023-12-03T05:00:39Z', 'content': 'Travel in luxury in the Tokyo area with this new Uber feature!\\r\\nWith public transportation so readily available in metropolitan areas in Japan, you may not have a need to call a taxi that often. Now … [+1637 chars]'}, {'source': {'id': None, 'name': 'Fast Company'}, 'author': 'Kristen Hawley', 'title': 'Silicon Valley’s $445 million robot pizza revolution that wasn’t', 'description': '“What if there was a better way to make and deliver pizza to you?” asked CNBC’s Jim Cramer—shirtsleeves rolled up, wagging his finger at the camera—as he passionately set up a segment of his popular Mad Money show in June 2018. “Turns out, there is, which bri…', 'url': 'https://www.fastcompany.com/90979001/zume-pizza-silicon-valley-445-million-robot-revolution-that-wasnt', 'urlToImage': 'https://images.fastcompany.net/image/upload/w_1280,f_auto,q_auto,fl_lossy/wp-cms/uploads/2023/12/p-1-90979001-zume-pizza-silicon-valley-445-million-robot-revolution-that-wasnt.jpg', 'publishedAt': '2023-12-03T05:00:00Z', 'content': '“What if there was a better way to make and deliver pizza to you?” asked CNBC’s Jim Cramer—shirtsleeves rolled up, wagging his finger at the camera—as he passionately set up a segment of his popular … [+29224 chars]'}, {'source': {'id': None, 'name': 'Moneycontrol'}, 'author': 'Faizan Javed', 'title': 'Willing to help other EV players adopt our charging connector free of cost: Ather Energy CBO', 'description': 'The electric two-wheeler maker is also working to increase its own charging network to 2,500 by March 2024 from the current 1,600 even as it is in talks with other manufacturers for adoption of its charging connector.', 'url': 'https://www.moneycontrol.com/news/business/willing-to-help-other-ev-players-adopt-our-charging-connector-free-of-cost-ather-energy-cbo-11845861.html', 'urlToImage': 'https://images.moneycontrol.com/static-mcnews/2023/03/electric-car-1458836_1280-770x433.jpg', 'publishedAt': '2023-12-03T04:46:57Z', 'content': 'Ather Energy is open to provide free help to players in the electric vehicle space to adopt its charging connector, which has been recognised as standard by the Bureau of Indian Standards, to acceler… [+3167 chars]'}, {'source': {'id': 'the-times-of-india', 'name': 'The Times of India'}, 'author': 'PTI', 'title': 'Willing to help other EV players adopt our charging connector free of cost: Ather Energy CBO', 'description': \"In October this year, the Bureau of Indian Standards (BIS) had approved Ather Energy's indigenously developed AC and DC combined charging connector as standard for light electric vehicles (LEVs) -- electric two- and three-wheelers, along with micro cars.\", 'url': 'https://economictimes.indiatimes.com/industry/renewables/willing-to-help-other-ev-players-adopt-our-charging-connector-free-of-cost-ather-energy-cbo/articleshow/105693644.cms', 'urlToImage': 'https://img.etimg.com/thumb/msid-105693661,width-1200,height-630,imgsize-60246,overlay-economictimes/photo.jpg', 'publishedAt': '2023-12-03T04:41:20Z', 'content': 'Ather Energy is open to provide free help to players in the electric vehicle space to adopt its charging connector, which has been recognised as standard by the Bureau of Indian Standards, to acceler… [+3038 chars]'}, {'source': {'id': 'the-times-of-india', 'name': 'The Times of India'}, 'author': 'PTI', 'title': 'Willing to help other EV players adopt our charging connector free of cost: Ather Energy CBO', 'description': 'The electric two-wheeler maker is also working to increase its own charging network to 2,500 by March 2024 from the current 1,600 even as it is in talks with other manufacturers for adoption of its charging connector.', 'url': 'https://economictimes.indiatimes.com/tech/technology/willing-to-help-other-ev-players-adopt-our-charging-connector-free-of-cost-ather-energy-cbo/articleshow/105693557.cms', 'urlToImage': 'https://img.etimg.com/thumb/msid-105693593,width-1200,height-630,imgsize-1146938,overlay-ettech/photo.jpg', 'publishedAt': '2023-12-03T04:37:38Z', 'content': 'Ather Energy is open to provide free help to players in the electric vehicle space to adopt its charging connector, which has been recognised as standard by the Bureau of Indian Standards, to acceler… [+3437 chars]'}, {'source': {'id': None, 'name': 'Wattsupwiththat.com'}, 'author': 'Guest Blogger', 'title': 'Ellenbogen: New York State’s Energy\\xa0Transition', 'description': 'Richard Ellenbogen recently gave an important presentation on New York State’s Energy Transition that details his concerns with the net -zero mandate of the\\xa0Climate Leadership and Community Protection Act\\xa0(CLCPA).\\xa0 I think it is important that his message get…', 'url': 'https://wattsupwiththat.com/2023/12/02/ellenbogen-new-york-states-energy-transition/', 'urlToImage': 'https://wattsupwiththat.com/wp-content/uploads/2020/12/wuwt-logo.jpg', 'publishedAt': '2023-12-03T02:00:00Z', 'content': 'From the Pragmatic Environmentalist of New York\\r\\nRoger Caiazza\\r\\nRichard Ellenbogen recently gave an important presentation on New York States Energy Transition that details his concerns with the net … [+19966 chars]'}, {'source': {'id': None, 'name': 'Moneycontrol'}, 'author': 'Reuters', 'title': 'GM believes many of its EVs will qualify for tax credits in 2024', 'description': \"&quot;Due to GM's historic investments in the U.S and efforts to build more secure and resilient supply chains we believe GM is well positioned to maintain the consumer purchase incentive for many of our EVs in 2024 and beyond,&quot; the automaker said after …\", 'url': 'https://www.moneycontrol.com/news/world/gm-believes-many-of-its-evs-will-qualify-for-tax-credits-in-2024-11845511.html', 'urlToImage': 'https://images.moneycontrol.com/static-mcnews/2019/11/general-motors-logo-770x433.jpg', 'publishedAt': '2023-12-03T00:43:49Z', 'content': 'General Motors said Friday that it expects many of its electric vehicles to qualify for U.S. tax credits next year after new stricter rules limiting Chinese battery content take effect.\\r\\n\"Due to GM\\'s… [+416 chars]'}, {'source': {'id': None, 'name': 'The Japan Times'}, 'author': 'The Japan Times', 'title': 'Japan lifts tsunami warning after strong quake jolts Philippines', 'description': 'Japan lifted all tsunami warnings on Sunday, hours after a powerful magnitude 7.6 earthquake — followed by four major aftershocks — struck the southern Philippines, sending residents fleeing from coastal areas.', 'url': 'https://www.japantimes.co.jp/news/2023/12/03/japan/science-health/quake-philippines-aftershocks/', 'urlToImage': 'https://www.japantimes.co.jp/japantimes/uploads/images/2023/12/03/266977.png', 'publishedAt': '2023-12-03T00:42:00Z', 'content': 'Manila/Tokyo Japan lifted all tsunami warnings on Sunday, hours after a powerful magnitude 7.6 earthquake followed by four major aftershocks struck the southern Philippines, sending residents fleeing… [+3924 chars]'}, {'source': {'id': None, 'name': 'Slashdot.org'}, 'author': 'EditorDavid', 'title': \"EV Owners Report 'Far More' Problems Than Conventional Car Owners, Says Consumer Reports\", 'description': 'Consumer Reports awarded a \"recommended\" rating to Tesla\\'s Modey Y this year, \"with owners reporting fewer issues with its suspension, in-car electronics and general build quality than in previous years\". Tesla\\'s Model 3 also earned a \"recommended\" rating. \\n\\n…', 'url': 'https://tech.slashdot.org/story/23/12/02/2236216/ev-owners-report-far-more-problems-than-conventional-car-owners-says-consumer-reports', 'urlToImage': 'https://a.fsdn.com/sd/topics/transportation_64.png', 'publishedAt': '2023-12-02T23:34:00Z', 'content': 'Consumer Reports awarded a \"recommended\" rating to Tesla\\'s Modey Y this year, \"with owners reporting fewer issues with its suspension, in-car electronics and general build quality than in previous ye… [+2917 chars]'}, {'source': {'id': 'abc-news', 'name': 'ABC News'}, 'author': 'The Associated Press', 'title': 'Winter weather in Pacific Northwest cuts power to thousands in Seattle', 'description': 'Thousands of people woke up in homes without electricity in the the greater Seattle area after a night of rain and wind', 'url': 'https://abcnews.go.com/US/wireStory/winter-weather-pacific-northwest-cuts-power-thousands-seattle-105332608', 'urlToImage': 'https://s.abcnews.com/images/US/abc_news_default_2000x2000_update_16x9_992.jpg', 'publishedAt': '2023-12-02T21:58:03Z', 'content': 'SEATTLE -- Winter weather brought high winds and snow to parts of the Pacific Northwest, knocking out power in some areas and dumping fresh snow across the Cascade Range.\\r\\nThousands of households wer… [+2303 chars]'}, {'source': {'id': None, 'name': '[Removed]'}, 'author': None, 'title': '[Removed]', 'description': '[Removed]', 'url': 'https://removed.com', 'urlToImage': None, 'publishedAt': '1970-01-01T00:00:00Z', 'content': '[Removed]'}, {'source': {'id': None, 'name': '[Removed]'}, 'author': None, 'title': '[Removed]', 'description': '[Removed]', 'url': 'https://removed.com', 'urlToImage': None, 'publishedAt': '1970-01-01T00:00:00Z', 'content': '[Removed]'}, {'source': {'id': None, 'name': 'Yanko Design'}, 'author': 'Ida Torres', 'title': 'Solar electric minivan lets you power up in emergency situations', 'description': 'Solar electric minivan lets you power up in emergency situationsIn the old days, the only function of vehicles was to transport people and goods to their desired location. But as technology evolves, we expect...', 'url': 'https://www.yankodesign.com/2023/12/02/solar-electric-minivan-lets-you-power-up-in-emergency-situations/', 'urlToImage': 'https://www.yankodesign.com/images/design_news/2023/12/ev-minivan-lets-you-power-up-in-emergency-situations/1.jpg', 'publishedAt': '2023-12-02T21:45:42Z', 'content': 'In the old days, the only function of vehicles was to transport people and goods to their desired location. But as technology evolves, we expect a lot more from them other than just a way to get from… [+1783 chars]'}, {'source': {'id': 'news-com-au', 'name': 'News.com.au'}, 'author': 'Afp', 'title': 'Major aftershocks in Philippines after 7.6 magnitude earthquake', 'description': 'A powerful magnitude 7.6 earthquake has struck the southern Philippines, triggering fears of a possible tsunami as big aftershocks rumbled in the area.', 'url': 'https://www.news.com.au/technology/environment/i-was-screaming-major-aftershocks-in-philippines-after-76-magnitude-earthquake/news-story/44b3696399354dc1307246d11345add4', 'urlToImage': 'https://content.api.news/v3/images/bin/4e68868ef55b5129b1b5cf9bf8ae67e3', 'publishedAt': '2023-12-02T21:43:00Z', 'content': 'A powerful magnitude 7.6 earthquake has struck the southern Philippines, triggering fears of a possible tsunami as big aftershocks rumbled in the area.\\r\\nThe initial quake struck at a depth of 32 kilo… [+3716 chars]'}, {'source': {'id': None, 'name': 'Redferret.net'}, 'author': 'Nigel', 'title': 'Solid State Batteries And The Shocking Future Of Our Electric World', 'description': 'As we cruise into the future of electric vehicles (EVs), the buzz around solid-state battery technology is getting louder and more electrifying. Imagine a world where EVs charge faster than your smartphone, drive huge distances, and are safer than your gran o…', 'url': 'https://www.redferret.net/solid-state-batteries-and-the-future/', 'urlToImage': 'https://www.redferret.net/wp-content/uploads/2023/12/QSHero-Feb2022-noLogo-rounded.jpg', 'publishedAt': '2023-12-02T21:16:25Z', 'content': 'Image courtesy of QuantumScape Inc\\r\\nAs we cruise into the future of electric vehicles (EVs), the buzz around solid-state battery technology is getting louder and more electrifying. Imagine a world wh… [+4815 chars]'}, {'source': {'id': None, 'name': 'BMWBLOG'}, 'author': 'Andrei Nedelea', 'title': 'BMW PHEV or EV: Picking the Right Type of Plug-In', 'description': 'BMW offers a wide selection of plug-in hybrids and full electric vehicles these days, but unless you take the time to understand the technical side of things and functional differences between the two types of...\\nFirst published by https://www.bmwblog.com', 'url': 'https://www.bmwblog.com/2023/12/02/bmw-phev-or-ev/', 'urlToImage': 'https://cdn.bmwblog.com/wp-content/uploads/2020/08/2020-BMW-X3-xDrive30e-hybrid-13.jpg', 'publishedAt': '2023-12-02T21:13:58Z', 'content': 'BMW offers a wide selection of plug-in hybrids and full electric vehicles these days, but unless you take the time to understand the technical side of things and functional differences between the tw… [+9175 chars]'}, {'source': {'id': None, 'name': 'Minneapolis Star Tribune'}, 'author': 'Star Tribune Staff', 'title': 'Winter weather in Pacific Northwest cuts power to thousands in Seattle, dumps snow on Cascades', 'description': 'Winter weather brought high winds and snow to parts of the Pacific Northwest, knocking out power in some areas and dumping fresh snow across the Cascade Range.', 'url': 'https://www.startribune.com/winter-weather-in-pacific-northwest-cuts-power-to-thousands-in-seattle-dumps-snow-on-cascades/600323976/', 'urlToImage': 'https://www.startribune.com/static/img/branding/logos/strib-social-card.png?d=1701293163', 'publishedAt': '2023-12-02T20:10:01Z', 'content': 'SEATTLE Winter weather brought high winds and snow to parts of the Pacific Northwest, knocking out power in some areas and dumping fresh snow across the Cascade Range.\\r\\nThousands of households were w… [+2298 chars]'}, {'source': {'id': None, 'name': 'Freerepublic.com'}, 'author': 'Reuters', 'title': 'US opens probe into 73,000 Chevrolet Volt cars over loss of power', 'description': 'WASHINGTON, Dec 1 (Reuters) - A U.S. auto safety regulator said on Friday it is opening an investigation into 73,000 Chevrolet Volt plug-in hybrid cars over reports of abrupt loss of power, failures to restart and other issues. The National Highway Traffic Sa…', 'url': 'https://freerepublic.com/focus/f-news/4200791/posts', 'urlToImage': None, 'publishedAt': '2023-12-02T19:51:13Z', 'content': 'Skip to comments.\\r\\nUS opens probe into 73,000 Chevrolet Volt cars over loss of powerReuters ^\\r\\n | December 1, 2023\\r\\n | David Shepardson\\r\\nPosted on 12/02/2023 11:51:13 AM PST by grundle\\r\\nWASHINGTON, D… [+4598 chars]'}, {'source': {'id': None, 'name': 'KSAT San Antonio'}, 'author': 'Matthew Daly', 'title': \"US targets oil and natural gas industry's role in global warming with new rule on methane emissions\", 'description': \"US targets oil and natural gas industry's role in global warming with new rule on methane emissionsksat.com\", 'url': 'https://www.ksat.com/business/2023/12/02/biden-rule-aims-to-reduce-methane-emissions-targeting-us-oil-and-gas-industry-for-global-warming/', 'urlToImage': 'https://res.cloudinary.com/graham-media-group/image/upload/f_auto/q_auto/c_thumb,w_700/v1/media/gmg/5OUUJTYFYVEKLARTFEYCEKB4OI.jpg?_a=ATAPphC0', 'publishedAt': '2023-12-02T19:49:29Z', 'content': 'WASHINGTON The Biden administration on Saturday issued a final rule aimed at reducing methane emissions, targeting the U.S. oil and natural gas industry for its role in global warming as President Jo… [+6509 chars]'}, {'source': {'id': 'abc-news-au', 'name': 'ABC News (AU)'}, 'author': 'Mark Rigby', 'title': 'Tara tried to save some of her pay going to tax but it ended up costing her thousands', 'description': 'Tara Byrne, a busy professional, was sold a deal that would reduce her taxable income and provide cost-of-living relief, but the car she signed up for ended up costing more than double its showroom price.', 'url': 'https://www.abc.net.au/news/2023-12-03/new-car-novated-lease-tax-saving-ev-finance/103050454', 'urlToImage': 'https://live-production.wcms.abc-cdn.net.au/6d57c70779411b3642894f312cf3a75f?impolicy=wcms_crop_resize&cropH=2144&cropW=3812&xPos=88&yPos=44&width=862&height=485', 'publishedAt': '2023-12-02T19:15:00Z', 'content': \"Tara Byrne was sold on the pros of a novated lease while searching the market for a new car.\\r\\nFive years later, she's an exemplar of why tax-reducing car deals aren't always the cost-of-living saviou… [+5924 chars]\"}, {'source': {'id': None, 'name': 'The Boston Globe'}, 'author': 'Jacob Bogage', 'title': 'The US government can’t quit Elon Musk - even amid erratic behavior', 'description': \"It may not be easy to disentangle the government from Musk's sprawling tech empire, which includes commercial spaceflight firm SpaceX, prolific satellite internet service Starlink, electric automaker Tesla, medical device company Neuralink and the social medi…\", 'url': 'https://www.bostonglobe.com/2023/12/02/nation/us-government-cant-quit-elon-musk-even-amid-erratic-behavior/', 'urlToImage': 'https://bostonglobe-prod.cdn.arcpublishing.com/resizer/g2uI0nVRfWst3gx7almahLjPjZg=/506x0/cloudfront-us-east-1.images.arcpublishing.com/bostonglobe/OJ6DDH5ODZKKDMKUEX6ENWTWDQ.jpg', 'publishedAt': '2023-12-02T19:00:44Z', 'content': 'Sen. Ron Wyden (D-Ore.), another member of the Intelligence Committee and chair of the Senate Finance Committee, said in a statement that federal agencies should \"where possible exercise their discre… [+7899 chars]'}, {'source': {'id': None, 'name': 'CleanTechnica'}, 'author': 'Steve Hanley', 'title': 'CATL Creates Fast Charging Electric Car Skateboard With 1000 Km Range.', 'description': 'CATL has developed an electric car skateboard it says has a range of 1000 kilometers and can add 300 kilometers of range in 5 minutes.\\nThe post CATL Creates Fast Charging Electric Car Skateboard With 1000 Km Range. appeared first on CleanTechnica.', 'url': 'https://cleantechnica.com/2023/12/02/catl-creates-fast-charging-electric-car-skateboard-with-1000-km-range/', 'urlToImage': 'https://cleantechnica.com/wp-content/uploads/2023/12/catlcahssis.jpg', 'publishedAt': '2023-12-02T19:00:02Z', 'content': 'Sign up for daily news updates from CleanTechnica on email. Or follow us on Google News!\\r\\nCATL, the world’s largest battery manufacturer, is not waiting for customers to come knocking on its door to … [+5653 chars]'}, {'source': {'id': None, 'name': 'Wonkette.com'}, 'author': 'Doktor Zoom', 'title': \"Hey, Let's Round Out The Week With Some Good Economy And Climate News!\", 'description': \"There's green to be made in Green.\", 'url': 'https://www.wonkette.com/p/hey-lets-round-out-the-week-with', 'urlToImage': 'https://substackcdn.com/image/fetch/w_1200,h_600,c_fill,f_jpg,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9914f56b-6529-4bb0-8d7a-ed019a931517_900x603.jpeg', 'publishedAt': '2023-12-02T18:55:14Z', 'content': 'Wed been intending to throw several brief nice-times stories about the economy and climate at you the last few days, but then Henry Kissinger died and Ammon Bundy went walkabout and Fox News hosted t… [+4407 chars]'}, {'source': {'id': 'time', 'name': 'Time'}, 'author': 'MATTHEW DALY / AP', 'title': 'U.S. Targets Oil and Natural Gas Industry’s Role in Global Warming With New Final Rule', 'description': 'The Biden administration issued a\\xa0final rule\\xa0aimed at reducing methane emissions, targeting the U.S. oil and natural gas industry.', 'url': 'https://time.com/6342058/us-oil-natural-gas-industrys-global-warming/', 'urlToImage': 'https://api.time.com/wp-content/uploads/2023/12/AP23336496415234.jpg?quality=85', 'publishedAt': '2023-12-02T18:27:28Z', 'content': 'WASHINGTON The Biden administration on Saturday issued a\\xa0final rule\\xa0aimed at reducing methane emissions, targeting the U.S. oil and natural gas industry for its role in global warming as President Jo… [+6249 chars]'}, {'source': {'id': None, 'name': 'Space Daily'}, 'author': None, 'title': 'Map highlights environmental and social costs of rare earths extraction', 'description': 'Barcelona, Spain (SPX) Nov 28, 2023 -\\n\\nA recently released map by the Debt Observatory in Globalization, in collaboration with the EJAtlas of Institute of Environmental Science and Technology of the Universitat Autonoma de Barcelona (ICTA-UAB), the Institute …', 'url': 'https://www.spacedaily.com/reports/Map_highlights_environmental_and_social_costs_of_rare_earths_extraction_999.html', 'urlToImage': 'https://www.spxdaily.com/images-hg/rare-earth-elements-impacts-and-conflicts-map-hg.jpg', 'publishedAt': '2023-12-02T17:52:35Z', 'content': 'Map highlights environmental and social costs of rare earths extractionby Hugo RitmicoBarcelona, Spain (SPX) Nov 28, 2023\\r\\nA recently released map by the Debt Observatory in Globalization, in collabo… [+3582 chars]'}, {'source': {'id': None, 'name': 'KOB'}, 'author': 'Mesha Begay', 'title': \"US targets oil and natural gas industry's role in global warming with new rule on methane emissions\", 'description': 'The United States has taken action to reduce methane emissions from oil and natural gas production. The Environmental Protection Agency says a new Biden administration rule announced Saturday will sharply reduce methane and other harmful air pollutants that c…', 'url': 'https://www.kob.com/news/business-money/us-targets-oil-and-natural-gas-industrys-role-in-global-warming-with-new-rule-on-methane-emissions/', 'urlToImage': 'https://www.kob.com/wp-content/uploads/apimg/2023/12/Climate_Biden_Methane_84759.jpg', 'publishedAt': '2023-12-02T17:49:43Z', 'content': 'WASHINGTON (AP) The Biden administration on Saturday issued a final rule aimed at reducing methane emissions, targeting the U.S. oil and natural gas industry for its role in global warming as Preside… [+6952 chars]'}, {'source': {'id': None, 'name': 'ETF Daily News'}, 'author': 'MarketBeat News', 'title': 'Crawford Investment Counsel Inc. Purchases 3,655 Shares of Power Integrations, Inc. (NASDAQ:POWI)', 'description': 'Crawford Investment Counsel Inc. raised its position in Power Integrations, Inc. (NASDAQ:POWI – Free Report) by 3.3% during the 2nd quarter, according to the company in its most recent Form 13F filing with the Securities and Exchange Commission (SEC). The fir…', 'url': 'https://www.etfdailynews.com/2023/12/02/crawford-investment-counsel-inc-purchases-3655-shares-of-power-integrations-inc-nasdaqpowi/', 'urlToImage': 'https://www.americanbankingnews.com/wp-content/timthumb/timthumb.php?src=https://www.marketbeat.com/logos/power-integrations-inc-logo.PNG?v=20201203130712&w=240&h=240&zc=2', 'publishedAt': '2023-12-02T17:44:42Z', 'content': 'Crawford Investment Counsel Inc. raised its position in Power Integrations, Inc. (NASDAQ:POWI – Free Report) by 3.3% during the 2nd quarter, according to the company in its most recent Form 13F filin… [+7070 chars]'}, {'source': {'id': None, 'name': 'ETF Daily News'}, 'author': 'MarketBeat News', 'title': 'Howe & Rusling Inc. Invests $268,000 in Tesla, Inc. (NASDAQ:TSLA)', 'description': 'Howe & Rusling Inc. acquired a new position in Tesla, Inc. (NASDAQ:TSLA – Free Report) during the 2nd quarter, according to its most recent 13F filing with the Securities and Exchange Commission. The institutional investor acquired 1,024 shares of the electri…', 'url': 'https://www.etfdailynews.com/2023/12/02/howe-rusling-inc-invests-268000-in-tesla-inc-nasdaqtsla/', 'urlToImage': 'https://www.americanbankingnews.com/wp-content/timthumb/timthumb.php?src=https://www.marketbeat.com/logos/tesla-inc-logo.png?v=20221020135629&w=240&h=240&zc=2', 'publishedAt': '2023-12-02T17:42:49Z', 'content': 'Howe &amp; Rusling Inc. acquired a new position in Tesla, Inc. (NASDAQ:TSLA – Free Report) during the 2nd quarter, according to its most recent 13F filing with the Securities and Exchange Commission.… [+5915 chars]'}, {'source': {'id': None, 'name': 'ETF Daily News'}, 'author': 'MarketBeat News', 'title': 'Cerity Partners LLC Boosts Stock Position in Tesla, Inc. (NASDAQ:TSLA)', 'description': 'Cerity Partners LLC boosted its holdings in Tesla, Inc. (NASDAQ:TSLA – Free Report) by 14.0% during the 2nd quarter, according to its most recent disclosure with the Securities and Exchange Commission (SEC). The fund owned 203,763 shares of the electric vehic…', 'url': 'https://www.etfdailynews.com/2023/12/02/cerity-partners-llc-boosts-stock-position-in-tesla-inc-nasdaqtsla/', 'urlToImage': 'https://www.americanbankingnews.com/wp-content/timthumb/timthumb.php?src=https://www.marketbeat.com/logos/tesla-inc-logo.png?v=20221020135629&w=240&h=240&zc=2', 'publishedAt': '2023-12-02T17:42:45Z', 'content': 'Cerity Partners LLC boosted its holdings in Tesla, Inc. (NASDAQ:TSLA – Free Report) by 14.0% during the 2nd quarter, according to its most recent disclosure with the Securities and Exchange Commissio… [+6196 chars]'}, {'source': {'id': None, 'name': 'PR Newswire UK'}, 'author': None, 'title': 'NWTN Celebrates Landmark Collaboration with Autostrad Car Rental Company for Eco-Friendly Transportation', 'description': 'DUBAI, UAE, Dec. 2, 2023 /PRNewswire/ -- NWTN Inc. (Nasdaq: NWTN), an eco-conscious mobility technology company bringing passenger-centric green premium mobility solutions to the world (\"NWTN\" or the \"Company\"), is excited to announce a ground-breaking agreem…', 'url': 'https://www.prnewswire.co.uk/news-releases/nwtn-celebrates-landmark-collaboration-with-autostrad-car-rental-company-for-eco-friendly-transportation-302003841.html', 'urlToImage': 'https://mma.prnewswire.com/media/2291127/Signing.jpg?p=facebook', 'publishedAt': '2023-12-02T17:27:00Z', 'content': 'DUBAI, UAE, Dec. 2, 2023 /PRNewswire/ -- NWTN Inc. (Nasdaq: NWTN), an eco-conscious mobility technology company bringing passenger-centric green premium mobility solutions to the world (\"NWTN\" or the… [+7339 chars]'}, {'source': {'id': None, 'name': 'Biztoc.com'}, 'author': 'cnbc.com', 'title': 'How $100 billion mining giant Rio Tinto is poised to benefit from the EV boom', 'description': \"Copper mines like Rio Tinto's Bingham Canyon mine on the outskirts of Salt Lake City are on the frontline of America's transition to clean energy. Global demand for copper, a major component of electric vehicles, is expected to grow from 25 million metric ton…\", 'url': 'https://biztoc.com/x/c353a247ca6dc4ea', 'urlToImage': 'https://c.biztoc.com/p/c353a247ca6dc4ea/og.webp', 'publishedAt': '2023-12-02T17:16:10Z', 'content': \"Copper mines like Rio Tinto's Bingham Canyon mine on the outskirts of Salt Lake City are on the frontline of America's transition to clean energy.Global demand for copper, a major component of electr… [+305 chars]\"}, {'source': {'id': 'cbc-news', 'name': 'CBC News'}, 'author': None, 'title': 'Carbon tax adds to financial strain for volunteer fire departments', 'description': 'Volunteer fire services in Nova Scotia are facing tighter budgets as a result of the federal carbon tax, prompting calls for an exemption.', 'url': 'https://www.cbc.ca/news/canada/nova-scotia/carbon-tax-financial-strain-volunteer-fire-departments-1.7047350', 'urlToImage': 'https://i.cbc.ca/1.7047358.1701524518!/fileImage/httpImage/image.jpg_gen/derivatives/16x9_620/fire-truck-thank-you.jpg', 'publishedAt': '2023-12-02T17:00:00Z', 'content': 'Volunteer fire services in Nova Scotia are facing tighter budgets as a result of the federal carbon tax, prompting calls for an exemption.\\xa0\\r\\nIn July, the cost of gas and diesel in Nova Scotia increas… [+2709 chars]'}, {'source': {'id': None, 'name': 'ETF Daily News'}, 'author': 'MarketBeat News', 'title': 'Granahan Investment Management LLC Has $10.98 Million Stake in Power Integrations, Inc. (NASDAQ:POWI)', 'description': 'Granahan Investment Management LLC lowered its stake in shares of Power Integrations, Inc. (NASDAQ:POWI – Free Report) by 39.4% in the second quarter, according to its most recent 13F filing with the SEC. The fund owned 115,925 shares of the semiconductor com…', 'url': 'https://www.etfdailynews.com/2023/12/02/granahan-investment-management-llc-has-10-98-million-stake-in-power-integrations-inc-nasdaqpowi-2/', 'urlToImage': 'https://www.americanbankingnews.com/wp-content/timthumb/timthumb.php?src=https://www.marketbeat.com/logos/power-integrations-inc-logo.PNG?v=20201203130712&w=240&h=240&zc=2', 'publishedAt': '2023-12-02T16:48:44Z', 'content': 'Granahan Investment Management LLC lowered its stake in shares of Power Integrations, Inc. (NASDAQ:POWI – Free Report) by 39.4% in the second quarter, according to its most recent 13F filing with the… [+7004 chars]'}, {'source': {'id': None, 'name': 'ETF Daily News'}, 'author': 'MarketBeat News', 'title': 'Granahan Investment Management LLC Has $10.98 Million Stake in Power Integrations, Inc. (NASDAQ:POWI)', 'description': 'Granahan Investment Management LLC lowered its stake in shares of Power Integrations, Inc. (NASDAQ:POWI – Free Report) by 39.4% in the second quarter, according to its most recent 13F filing with the SEC. The fund owned 115,925 shares of the semiconductor com…', 'url': 'https://www.etfdailynews.com/2023/12/02/granahan-investment-management-llc-has-10-98-million-stake-in-power-integrations-inc-nasdaqpowi/', 'urlToImage': 'https://www.americanbankingnews.com/wp-content/timthumb/timthumb.php?src=https://www.marketbeat.com/logos/power-integrations-inc-logo.PNG?v=20201203130712&w=240&h=240&zc=2', 'publishedAt': '2023-12-02T16:48:43Z', 'content': 'Granahan Investment Management LLC lowered its stake in shares of Power Integrations, Inc. (NASDAQ:POWI – Free Report) by 39.4% in the second quarter, according to its most recent 13F filing with the… [+6942 chars]'}, {'source': {'id': None, 'name': 'ETF Daily News'}, 'author': 'MarketBeat News', 'title': 'Graham Capital Management L.P. Makes New $1.82 Million Investment in Power Integrations, Inc. (NASDAQ:POWI)', 'description': 'Graham Capital Management L.P. purchased a new position in Power Integrations, Inc. (NASDAQ:POWI – Free Report) in the 2nd quarter, HoldingsChannel reports. The fund purchased 19,233 shares of the semiconductor company’s stock, valued at approximately $1,821,…', 'url': 'https://www.etfdailynews.com/2023/12/02/graham-capital-management-l-p-makes-new-1-82-million-investment-in-power-integrations-inc-nasdaqpowi/', 'urlToImage': 'https://www.americanbankingnews.com/wp-content/timthumb/timthumb.php?src=https://www.marketbeat.com/logos/power-integrations-inc-logo.PNG?v=20201203130712&w=240&h=240&zc=2', 'publishedAt': '2023-12-02T16:14:54Z', 'content': 'Graham Capital Management L.P. purchased a new position in Power Integrations, Inc. (NASDAQ:POWI – Free Report) in the 2nd quarter, HoldingsChannel reports. The fund purchased 19,233 shares of the se… [+6333 chars]'}, {'source': {'id': 'the-washington-post', 'name': 'The Washington Post'}, 'author': 'Jacob Bogage', 'title': 'Elon Musk’s U.S. government ties are strong despite antisemitic posts', 'description': 'Some lawmakers say the government should reevaluate its relationship with Elon Musk and his firms, which agencies support with billions of dollars in contracts.', 'url': 'https://www.washingtonpost.com/business/2023/12/02/elon-musk-congress-antisemitism/', 'urlToImage': 'https://www.washingtonpost.com/wp-apps/imrs.php?src=https://arc-anglerfish-washpost-prod-washpost.s3.amazonaws.com/public/OMD2WGPXPCGO6CCOP47AIH3OAE_size-normalized.JPG&w=1440', 'publishedAt': '2023-12-02T16:01:27Z', 'content': 'Comment on this story\\r\\nComment\\r\\nAdd to your saved stories\\r\\nSave\\r\\nSome of Elon Musks biggest business clients are rethinking their relationship with his tech empire amid a fresh spurt of controversial… [+8696 chars]'}, {'source': {'id': None, 'name': 'Autoblog'}, 'author': 'Jonathon Ramsey', 'title': \"Tesla Cybertruck musings: Let's consider all the angles\", 'description': \"Filed under:\\nGreen,Tesla,Truck,Electric\\n\\nContinue reading Tesla Cybertruck musings: Let's consider all the angles\\n\\nTesla Cybertruck musings: Let's consider all the angles originally appeared on Autoblog on Sat, 2 Dec 2023 11:00:00 EST. Please see our terms fo…\", 'url': 'https://www.autoblog.com/2023/12/02/tesla-cybertruck-musings-lets-look-at-all-the-angles/', 'urlToImage': 'https://s.aolcdn.com/images/dims3/GLOB/legacy_thumbnail/1062x597/format/jpg/quality/100/https://s.aolcdn.com/os/ab/_cms/2023/11/30161916/Cybertruck-Second-Hero-Desktop.jpg', 'publishedAt': '2023-12-02T16:00:00Z', 'content': \"The Tesla Cybertruck is finally here, or at least a few of them are. Check out our rundown of the livestreamed debut and summary of prices, specs and features, if you haven't already, and my colleagu… [+14398 chars]\"}, {'source': {'id': None, 'name': 'ETF Daily News'}, 'author': 'MarketBeat News', 'title': '28,125 Shares in Li Auto Inc. (NASDAQ:LI) Bought by Eqis Capital Management Inc.', 'description': 'Eqis Capital Management Inc. bought a new stake in shares of Li Auto Inc. (NASDAQ:LI – Free Report) in the 2nd quarter, according to its most recent 13F filing with the Securities & Exchange Commission. The firm bought 28,125 shares of the company’s stock, va…', 'url': 'https://www.etfdailynews.com/2023/12/02/28125-shares-in-li-auto-inc-nasdaqli-bought-by-eqis-capital-management-inc/', 'urlToImage': 'https://www.americanbankingnews.com/wp-content/timthumb/timthumb.php?src=https://www.marketbeat.com/logos/li-auto-inc-logo.png&w=240&h=240&zc=2', 'publishedAt': '2023-12-02T15:56:49Z', 'content': 'Eqis Capital Management Inc. bought a new stake in shares of Li Auto Inc. (NASDAQ:LI – Free Report) in the 2nd quarter, according to its most recent 13F filing with the Securities &amp; Exchange Comm… [+4023 chars]'}, {'source': {'id': None, 'name': 'Biztoc.com'}, 'author': 'thestreet.com', 'title': 'New report highlights a major speedbump to mass electric vehicle adoption', 'description': 'Electric vehicles, according to a new report from Consumer Reports, have 79% more problems than their internal combustion engine (ICE) counterparts. Plug-in hybrids, the report found, have nearly 150% more problems than ICE vehicles, while hybrids have 26% fe…', 'url': 'https://biztoc.com/x/ad2e45fdfe23e44a', 'urlToImage': 'https://c.biztoc.com/p/ad2e45fdfe23e44a/s.webp', 'publishedAt': '2023-12-02T15:52:09Z', 'content': 'Electric vehicles, according to a new report from Consumer Reports, have 79% more problems than their internal combustion engine (ICE) counterparts. Plug-in hybrids, the report found, have nearly 150… [+294 chars]'}, {'source': {'id': None, 'name': 'The Daily Caller'}, 'author': 'David Blackmon', 'title': 'DAVID BLACKMON: Biden Is Following Germany’s Failed Energy Policy Into The Dark', 'description': 'It has become a great rule of thumb to watch what is happening in Germany if you want to understand what will be happening in America in a few years where energy policy is concerned.', 'url': 'https://dailycaller.com/2023/12/02/opinion-biden-is-following-germanys-failed-energy-policy-into-the-dark-david-blackmon/', 'urlToImage': 'https://cdn01.dailycaller.com/wp-content/uploads/2022/04/GettyImages-1239465647-scaled-e1650298302331.jpg', 'publishedAt': '2023-12-02T15:42:24Z', 'content': 'It has become a great rule of thumb to watch what is happening in Germany if you want to understand what will be happening in America in a few years where energy policy is concerned. The Biden energy… [+4878 chars]'}, {'source': {'id': None, 'name': 'Moneycontrol'}, 'author': 'Moneycontrol News', 'title': 'IBLA 2023: Tesla won#39;t want to miss out on EV action in India, says Piyush Goyal', 'description': 'Commerce Minister Piyush Goyal said, the age of electric vehicles has arrived in India. More broadly on the economy, he sees India#39;s per capita income at close to $20,000 by 2047.', 'url': 'https://www.moneycontrol.com/news/business/economy/ibla-2023-tesla-wont-want-to-miss-out-on-ev-action-in-india-says-piyush-goyal-11844541.html', 'urlToImage': 'https://images.moneycontrol.com/static-mcnews/2023/12/Piyush-Goyal-at-IBLA-2023-770x433.jpg', 'publishedAt': '2023-12-02T15:37:40Z', 'content': 'Electric vehicle (EV) major Tesla will not want to miss out on the \"action\" in India, according to Commerce Minister Piyush Goyal. Speaking at the \\'India Business Leader Awards\\' on December 2, Goyal … [+2379 chars]'}, {'source': {'id': None, 'name': 'Slashdot.org'}, 'author': 'EditorDavid', 'title': \"Tesla's New Cybertruck Includes a 'Powershare' Bidirectional Charging Feature\", 'description': 'Tesla\\'s new Cybertruck is more than their first new model since 2020, reports the Verge:\\nTesla announced a new \"Powershare\" vehicle-to-load charging capability, only available on the new Cybertruck. The feature will allow Cybertruck owners to power their camp…', 'url': 'https://tech.slashdot.org/story/23/12/02/0438218/teslas-new-cybertruck-includes-a-powershare-bidirectional-charging-feature', 'urlToImage': 'https://a.fsdn.com/sd/topics/transportation_64.png', 'publishedAt': '2023-12-02T15:34:00Z', 'content': 'Tesla announced a new \"Powershare\" vehicle-to-load charging capability, only available on the new Cybertruck. The feature will allow Cybertruck owners to power their camping equipment, power tools, o… [+1140 chars]'}, {'source': {'id': None, 'name': 'ETF Daily News'}, 'author': 'MarketBeat News', 'title': 'Lombard Odier Asset Management USA Corp Takes Position in CarMax, Inc. (NYSE:KMX)', 'description': 'Lombard Odier Asset Management USA Corp bought a new stake in CarMax, Inc. (NYSE:KMX – Free Report) in the second quarter, according to its most recent filing with the Securities & Exchange Commission. The institutional investor bought 2,805 shares of the com…', 'url': 'https://www.etfdailynews.com/2023/12/02/lombard-odier-asset-management-usa-corp-takes-position-in-carmax-inc-nysekmx/', 'urlToImage': 'https://www.americanbankingnews.com/wp-content/timthumb/timthumb.php?src=https://www.marketbeat.com/logos/carmax-inc-logo.png?v=20221109132551&w=240&h=240&zc=2', 'publishedAt': '2023-12-02T15:30:45Z', 'content': 'Lombard Odier Asset Management USA Corp bought a new stake in CarMax, Inc. (NYSE:KMX – Free Report) in the second quarter, according to its most recent filing with the Securities &amp; Exchange Commi… [+4686 chars]'}, {'source': {'id': None, 'name': 'Roanoke Times'}, 'author': 'Lee Media Studio, The Associated Press', 'title': 'Kraft debuts dairy-free mac and cheese; EV sales to hit record; massive iceberg drifting beyond Antarctic waters | Hot off the Wire podcast', 'description': ' Get a recap of recent health, economic, science and environmental stories through this special edition of our daily news podcast.', 'url': 'https://roanoke.com/news/nation-world/kraft-debuts-dairy-free-mac-and-cheese-ev-sales-to-hit-record-massive-iceberg-drifting/article_1ac3e73b-a8a2-533f-b295-28340108f8e0.html', 'urlToImage': 'https://bloximages.newyork1.vip.townnews.com/roanoke.com/content/tncms/assets/v3/editorial/1/ac/1ac3e73b-a8a2-533f-b295-28340108f8e0/656a01e7773c9.preview.jpg?crop=1736%2C911%2C1%2C84&resize=1200%2C630&order=crop%2Cresize', 'publishedAt': '2023-12-02T15:30:00Z', 'content': 'On this version of Hot off the Wire:\\r\\nKraft Heinz says its bringing dairy-free macaroni and cheese to the U.S. for the first time. The company says the new recipe has the same creamy texture and flav… [+7432 chars]'}, {'source': {'id': None, 'name': 'Richmond.com'}, 'author': 'Lee Media Studio, The Associated Press', 'title': 'Kraft debuts dairy-free mac and cheese; EV sales to hit record; massive iceberg drifting beyond Antarctic waters | Hot off the Wire podcast', 'description': ' Get a recap of recent health, economic, science and environmental stories through this special edition of our daily news podcast.', 'url': 'https://richmond.com/news/nation-world/kraft-debuts-dairy-free-mac-and-cheese-ev-sales-to-hit-record-massive-iceberg-drifting/article_05e87d3b-25ac-5864-922c-2c2ae29a0145.html', 'urlToImage': 'https://bloximages.newyork1.vip.townnews.com/richmond.com/content/tncms/assets/v3/editorial/0/5e/05e87d3b-25ac-5864-922c-2c2ae29a0145/656a01e77d2a8.preview.jpg?crop=1736%2C911%2C1%2C84&resize=1200%2C630&order=crop%2Cresize', 'publishedAt': '2023-12-02T15:30:00Z', 'content': 'On this version of Hot off the Wire:\\r\\nKraft Heinz says its bringing dairy-free macaroni and cheese to the U.S. for the first time. The company says the new recipe has the same creamy texture and flav… [+7391 chars]'}, {'source': {'id': None, 'name': 'Forbes'}, 'author': 'Michael Barnard, Contributor, \\n Michael Barnard, Contributor\\n https://www.forbes.com/sites/michaelbarnard/', 'title': 'Most Pipelines Will Be Replaced With HVDC But Rest Will Be Electrified', 'description': 'The era of moving molecules for energy is coming to an end as we electrify everything everywhere all at once. Moving electrons is more efficient and lower impact.', 'url': 'https://www.forbes.com/sites/michaelbarnard/2023/12/02/most-pipelines-will-be-replaced-with-hvdc-but-rest-will-be-electrified/', 'urlToImage': 'https://imageio.forbes.com/specials-images/imageserve/656aa1fb5ccb562cad2a204d/0x0.jpg?format=jpg&height=900&width=1600&fit=bounds', 'publishedAt': '2023-12-02T15:21:30Z', 'content': 'Pipeline in green landscape\\r\\ngetty\\r\\nThe era of moving molecules for energy is coming to an end as we electrify everything everywhere all at once. Moving electrons is more efficient and lower impact. … [+7887 chars]'}, {'source': {'id': None, 'name': 'Los Angeles Times'}, 'author': 'Tom Krisher', 'title': 'Electric vehicles less reliable, on average, than conventional cars and trucks', 'description': 'Electric vehicles have proved far less reliable, on average, than gasoline-powered cars, trucks and SUVs, according to the latest survey by Consumer Reports.', 'url': 'https://www.latimes.com/business/story/2023-12-02/consumer-reports-electric-vehicles-less-reliable-on-average-than-conventional-cars-and-trucks', 'urlToImage': 'https://ca-times.brightspotcdn.com/dims4/default/9300d65/2147483647/strip/true/crop/6000x3150+0+425/resize/1200x630!/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2Fde%2F8b%2Fcefb025225fbc1bf1184ed83f4d4%2Fc1318a03dd1645e28f861270a2258f97', 'publishedAt': '2023-12-02T15:20:09Z', 'content': 'Electric vehicles have proved far less reliable, on average, than gasoline-powered cars, trucks and SUVs, according to the latest survey by Consumer Reports, which found that EVs from the 2021 throug… [+6165 chars]'}]}\nData saved in electric_vehicles_news_data.json",
    "crumbs": [
      "EV Insights",
      "Data Gathering",
      "News API"
    ]
  },
  {
    "objectID": "5000-website/data-gathering/rapid-api.html",
    "href": "5000-website/data-gathering/rapid-api.html",
    "title": "Rapid API - Automotive car Specs",
    "section": "",
    "text": "This data source offers detailed specifications of various cars. Below, the API call and the corresponding raw json output are shown. The json file’s nodes reveal extensive details about EVs, which could be valuable for future analytics using this dataset.",
    "crumbs": [
      "EV Insights",
      "Data Gathering",
      "Rapid API"
    ]
  },
  {
    "objectID": "5000-website/data-gathering/rapid-api.html#data-source",
    "href": "5000-website/data-gathering/rapid-api.html#data-source",
    "title": "Rapid API - Automotive car Specs",
    "section": "Data Source:",
    "text": "Data Source:",
    "crumbs": [
      "EV Insights",
      "Data Gathering",
      "Rapid API"
    ]
  },
  {
    "objectID": "5000-website/data-gathering/01-wiki-crawl.html",
    "href": "5000-website/data-gathering/01-wiki-crawl.html",
    "title": "Wikipedia Crawl example",
    "section": "",
    "text": "Author: J. Hickman\n\nThis code crawls through wikipedia to get a bunch of text data\nThe code lets the user specify search category topics.\n\nThe more different the topics are, the easier the classification will be.\nFor example, i used (pizza, metallurgy, basketball)\n\nIt then searches wikipedia for articles related to these topics\nLoops over the wikipedia pages and gets the text from the wikipedia pages\nBreaks the text into chunks (based on a user input specifying the number of sentences per chunk)\nEach chunk is cleaned and tagged with a “label” (classification) and a numeric “sentiment score” (regression)\nThese cleaned chunks form a corpus of strings with associated tags\n\npython -m pip install wikipedia_sections\n\nImport\n\n# conda install -c conda-forge wikipedia\n# conda install -c conda-forge wordcloud\n# python -m pip install wikipedia_sections\n\nimport wikipedia\nimport nltk\nimport string \nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem import PorterStemmer\nfrom nltk.sentiment import SentimentIntensityAnalyzer\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport numpy as np\n\n\n# RUN THE FOLLOWING IF YOU HAVEN'T DOWNLOADED THESE BEFORE\nnltk.download('vader_lexicon')\nnltk.download('punkt')\nnltk.download('wordnet')\nnltk.download('omw-1.4')\n\n[nltk_data] Downloading package vader_lexicon to\n[nltk_data]     /Users/isfarbaset/nltk_data...\n[nltk_data]   Package vader_lexicon is already up-to-date!\n[nltk_data] Downloading package punkt to\n[nltk_data]     /Users/isfarbaset/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package wordnet to\n[nltk_data]     /Users/isfarbaset/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to\n[nltk_data]     /Users/isfarbaset/nltk_data...\n[nltk_data]   Package omw-1.4 is already up-to-date!\n\n\nTrue\n\n\n\n\nSet user parameters\n\n# PARAMETERS \nlabel_list=['electric vehicle','gasoline vehicle','hybrid vehicle']\nmax_num_pages=25\nsentence_per_chunk=5\nmin_sentence_length=20\n\n# GET STOPWORDS\n# from nltk.corpus import stopwords\nstop_words=nltk.corpus.stopwords.words('english')\n\n# INITALIZE STEMMER+LEMITZIZER+SIA\nsia = SentimentIntensityAnalyzer()\nstemmer = PorterStemmer()\nlemmatizer = WordNetLemmatizer()\n\n\n\nDefine text cleaning function\n\ndef clean_string(text):\n    # #FILTER OUT UNWANTED CHAR\n    new_text=\"\"\n    # keep=string.printable\n    keep=\" abcdefghijklmnopqrstuvwxyz0123456789\"\n    for character in text:\n        if character.lower() in keep:\n            new_text+=character.lower()\n        else: \n            new_text+=\" \"\n    text=new_text\n    # print(text)\n\n    # #FILTER OUT UNWANTED WORDS\n    new_text=\"\"\n    for word in nltk.tokenize.word_tokenize(text):\n        if word not in nltk.corpus.stopwords.words('english'):\n            #lemmatize \n            tmp=lemmatizer.lemmatize(word)\n            # tmp=stemmer.stem(tmp)\n\n            # update word if there is a change\n            # if(tmp!=word): print(tmp,word)\n            \n            word=tmp\n            if len(word)&gt;1:\n                if word in [\".\",\",\",\"!\",\"?\",\":\",\";\"]:\n                    #remove the last space\n                    new_text=new_text[0:-1]+word+\" \"\n                else: #add a space\n                    new_text+=word.lower()+\" \"\n    text=new_text.strip()\n    return text\n\n# clean_string('the word \"pizza\" first appeared in a Latin text from the town of Gaeta, then still part of the Byzantine Empire, in 997 AD; the text states that a tenant of certain property is to give the bishop of Gaeta duodecim pizze (\"twelve pizzas\") every Christmas Day, and another twelve every Easter Sunday.Suggested etymologies include:')\n\n\n\nPreform a wikipedia crawl\n\n#INITIALIZE \ncorpus=[]  # list of strings (input variables X)\ntargets=[] # list of targets (labels or response variables Y)\n\n#--------------------------\n# LOOP OVER TOPICS \n#--------------------------\nfor label in label_list:\n\n    #SEARCH FOR RELEVANT PAGES \n    titles=wikipedia.search(label,results=max_num_pages)\n    print(\"Pages for label =\",label,\":\",titles)\n\n    #LOOP OVER WIKI-PAGES\n    for title in titles:\n        try:\n            print(\" \",title)\n            wiki_page = wikipedia.page(title, auto_suggest=True)\n\n            # LOOP OVER SECTIONS IN ARTICLE AND GET PAGE TEXT\n            for section in wiki_page.sections:\n                text=wiki_page.section(section); #print(text)\n\n                #BREAK IN TO SENTANCES \n                sentences=nltk.tokenize.sent_tokenize(text)\n                counter=0\n                text_chunk=''\n\n                #LOOP OVER SENTENCES \n                for sentence in sentences:\n                    if len(sentence)&gt;min_sentence_length:\n                        if(counter%sentence_per_chunk==0 and counter!=0):\n                            # PROCESS COMPLETED CHUNK \n                            \n                            # CLEAN STRING\n                            text_chunk=clean_string(text_chunk)\n\n                            # REMOVE LABEL IF IN STRING (MAKES IT TOO EASY)\n                            text_chunk=text_chunk.replace(label,\"\")\n                            \n                            # REMOVE ANY DOUBLE SPACES\n                            text_chunk=' '.join(text_chunk.split()).strip()\n\n                            #UPDATE CORPUS \n                            corpus.append(text_chunk)\n\n                            #UPDATE TARGETS\n                            score=sia.polarity_scores(text_chunk)\n                            target=[label,score['compound']]\n                            targets.append(target)\n\n                            #print(\"TEXT\\n\",text_chunk,target)\n\n                            # RESET CHUNK FOR NEXT ITERATION \n                            text_chunk=sentence\n                        else:\n                            text_chunk+=sentence\n                        #print(\"--------\\n\", sentence)\n                        counter+=1\n\n        except:\n            print(\"WARNING: SOMETHING WENT WRONG:\", title);  \n\nPages for label = electric vehicle : ['Electric vehicle', 'History of the electric vehicle', 'Battery electric vehicle', 'Electric vehicle battery', 'Hybrid electric vehicle', 'Electric car use by country', 'Plug-in electric vehicle', 'List of production battery electric vehicles', 'Neighborhood Electric Vehicle', 'Hybrid vehicle drivetrain', 'Aptera (solar electric vehicle)', 'Citroën Ami (electric vehicle)', 'Electric car', 'Electric vehicle conversion', 'Hybrid vehicle', 'Plug-in hybrid', 'Capacitor electric vehicle', 'Charging station', 'Grumman LLV', 'London Electric Vehicle Company', 'Electric vehicle industry in China', 'Fuel cell vehicle', 'Plug-in electric vehicles in China', 'Electric Vehicle Company', 'Electric vehicle warning sounds']\n     Electric vehicle\n     History of the electric vehicle\n     Battery electric vehicle\n     Electric vehicle battery\n     Hybrid electric vehicle\n     Electric car use by country\n     Plug-in electric vehicle\nWARNING: SOMETHING WENT WRONG: Plug-in electric vehicle\n     List of production battery electric vehicles\n     Neighborhood Electric Vehicle\n     Hybrid vehicle drivetrain\n     Aptera (solar electric vehicle)\n     Citroën Ami (electric vehicle)\n     Electric car\n     Electric vehicle conversion\n     Hybrid vehicle\n     Plug-in hybrid\n     Capacitor electric vehicle\n     Charging station\nWARNING: SOMETHING WENT WRONG: Charging station\n     Grumman LLV\n     London Electric Vehicle Company\n     Electric vehicle industry in China\n     Fuel cell vehicle\nWARNING: SOMETHING WENT WRONG: Fuel cell vehicle\n     Plug-in electric vehicles in China\n     Electric Vehicle Company\n     Electric vehicle warning sounds\nPages for label = gasoline vehicle : ['Petrol engine', 'Flexible-fuel vehicle', 'Gasoline', 'Alternative fuel vehicle', 'Electric vehicle', 'Miles per gallon gasoline equivalent', 'Hybrid electric vehicle', 'Natural gas vehicle', 'Bi-fuel vehicle', 'Gasoline pump', 'Common ethanol fuel mixtures', 'Hydrogen internal combustion engine vehicle', 'Filling station', 'Idle (engine)', 'Gasoline gallon equivalent', 'Low-speed pre-ignition', 'History of the automobile', 'Exhaust gas', 'Monroney sticker', 'Green vehicle', 'Hydrogen vehicle', 'Ford-Utilimaster FFV', 'Automotive engine', 'Fuel efficiency', 'United States vehicle emission standards']\n     Petrol engine\n     Flexible-fuel vehicle\n     Gasoline\n     Alternative fuel vehicle\n     Electric vehicle\n     Miles per gallon gasoline equivalent\n     Hybrid electric vehicle\n     Natural gas vehicle\n     Bi-fuel vehicle\n     Gasoline pump\n     Common ethanol fuel mixtures\n     Hydrogen internal combustion engine vehicle\n     Filling station\n     Idle (engine)\n     Gasoline gallon equivalent\n     Low-speed pre-ignition\n     History of the automobile\n     Exhaust gas\nWARNING: SOMETHING WENT WRONG: Exhaust gas\n     Monroney sticker\n     Green vehicle\n     Hydrogen vehicle\n     Ford-Utilimaster FFV\n     Automotive engine\n     Fuel efficiency\n     United States vehicle emission standards\nPages for label = hybrid vehicle : ['Hybrid vehicle', 'Hybrid electric vehicle', 'Hybrid vehicle drivetrain', 'Plug-in hybrid', 'List of hybrid vehicles', 'Hybrid vehicle (disambiguation)', 'Hydraulic hybrid vehicle', 'Electric vehicle', 'Audi hybrid vehicles', 'Mild hybrid', 'Toyota Prius Plug-in Hybrid', 'Ford Fusion Hybrid', 'Plug-in electric vehicle', 'Compressed-air vehicle', 'Hybrid Synergy Drive', 'Toyota Highlander', 'Green vehicle', 'Electric vehicle warning sounds', 'Vehicle classification by propulsion system', 'Toyota concept vehicles (2000–2009)', 'Hybrid', 'Battery electric vehicle', 'Alternative fuel vehicle', 'Ford Escape', 'Hybrid Air Vehicles']\n     Hybrid vehicle\n     Hybrid electric vehicle\n     Hybrid vehicle drivetrain\n     Plug-in hybrid\n     List of hybrid vehicles\n     Hybrid vehicle (disambiguation)\nWARNING: SOMETHING WENT WRONG: Hybrid vehicle (disambiguation)\n     Hydraulic hybrid vehicle\n     Electric vehicle\n     Audi hybrid vehicles\n     Mild hybrid\n     Toyota Prius Plug-in Hybrid\n     Ford Fusion Hybrid\n     Plug-in electric vehicle\nWARNING: SOMETHING WENT WRONG: Plug-in electric vehicle\n     Compressed-air vehicle\n     Hybrid Synergy Drive\n     Toyota Highlander\nWARNING: SOMETHING WENT WRONG: Toyota Highlander\n     Green vehicle\n     Electric vehicle warning sounds\n     Vehicle classification by propulsion system\n     Toyota concept vehicles (2000–2009)\nWARNING: SOMETHING WENT WRONG: Toyota concept vehicles (2000–2009)\n     Hybrid\nWARNING: SOMETHING WENT WRONG: Hybrid\n     Battery electric vehicle\n     Alternative fuel vehicle\n     Ford Escape\nWARNING: SOMETHING WENT WRONG: Ford Escape\n     Hybrid Air Vehicles\n\n\n\n\nSave results\n\n#SANITY CHECKS AND PRINT TO FILE \nprint(\"number of text chunks = \",len(corpus))\nprint(\"number of targets = \",len(targets))\n\ntmp=[]\nfor i in range(0,len(corpus)):\n    tmp.append([corpus[i],targets[i][0],targets[i][1]])\ndf=pd.DataFrame(tmp)\ndf=df.rename(columns={0: \"text\", 1: \"label\", 2: \"sentiment\"})\nprint(df)\ndf.to_csv('ev-wiki-crawl-results.csv',index=False)\n\nnumber of text chunks =  1173\nnumber of targets =  1173\n                                                   text             label  \\\n0     electric motive power started 1827 hungarian p...  electric vehicle   \n1     first mass produced appeared america early 190...  electric vehicle   \n2     20th century uk world largest user electric ro...  electric vehicle   \n3     1900 28 percent car road electric ev popular e...  electric vehicle   \n4     seldom marketed woman luxury car may stigma am...  electric vehicle   \n...                                                 ...               ...   \n1168  best known best selling steam powered car stan...    hybrid vehicle   \n1169  wind powered vehicle well known long time real...    hybrid vehicle   \n1170  wood gas used power car ordinary internal comb...    hybrid vehicle   \n1171  hybrid air vehicle formed 2007 roger munk jeff...    hybrid vehicle   \n1172  hav 304 developed military lemv project follow...    hybrid vehicle   \n\n      sentiment  \n0       -0.7506  \n1        0.9201  \n2        0.7096  \n3        0.9169  \n4        0.9231  \n...         ...  \n1168     0.9413  \n1169     0.4404  \n1170    -0.0347  \n1171     0.6808  \n1172    -0.4939  \n\n[1173 rows x 3 columns]"
  },
  {
    "objectID": "5000-website/decision-tree/reference-notebooks/r.html",
    "href": "5000-website/decision-tree/reference-notebooks/r.html",
    "title": "Isfar Baset",
    "section": "",
    "text": "import sklearn\nfrom sklearn import datasets\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\ndf = pd.read_csv('../eda/cars-data.csv')\n\ndf.head()\n\n\n\n\n\n\n\n\n\nUnnamed: 0\ncity_mpg\nclass\ncombination_mpg\ncylinders\ndisplacement\ndrive\nfuel_type\nhighway_mpg\nmake\nmodel\ntransmission\nyear\n\n\n\n\n0\n0\n18\nmidsize car\n21\n4.0\n2.2\nfwd\ngas\n26\ntoyota\nCamry\na\n1993\n\n\n1\n1\n19\nmidsize car\n22\n4.0\n2.2\nfwd\ngas\n27\ntoyota\nCamry\nm\n1993\n\n\n2\n2\n16\nmidsize car\n19\n6.0\n3.0\nfwd\ngas\n22\ntoyota\nCamry\na\n1993\n\n\n3\n3\n16\nmidsize car\n18\n6.0\n3.0\nfwd\ngas\n22\ntoyota\nCamry\nm\n1993\n\n\n4\n4\n18\nmidsize-large station wagon\n21\n4.0\n2.2\nfwd\ngas\n26\ntoyota\nCamry\na\n1993\n\n\n\n\n\n\n\n\n\nnan_count = df.isna().sum()\n\nprint(nan_count)\n\nUnnamed: 0           0\ncity_mpg             0\nclass                0\ncombination_mpg      0\ncylinders          124\ndisplacement       124\ndrive                8\nfuel_type            0\nhighway_mpg          0\nmake                 0\nmodel                0\ntransmission         0\nyear                 0\ndtype: int64\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 719 entries, 0 to 718\nData columns (total 13 columns):\n #   Column           Non-Null Count  Dtype  \n---  ------           --------------  -----  \n 0   Unnamed: 0       719 non-null    int64  \n 1   city_mpg         719 non-null    int64  \n 2   class            719 non-null    object \n 3   combination_mpg  719 non-null    int64  \n 4   cylinders        595 non-null    float64\n 5   displacement     595 non-null    float64\n 6   drive            711 non-null    object \n 7   fuel_type        719 non-null    object \n 8   highway_mpg      719 non-null    int64  \n 9   make             719 non-null    object \n 10  model            719 non-null    object \n 11  transmission     719 non-null    object \n 12  year             719 non-null    int64  \ndtypes: float64(2), int64(5), object(6)\nmemory usage: 73.2+ KB\n\n\n\n# Convert all 'object' type columns to 'string'\nfor col in df.select_dtypes(include=['object']).columns:\n    df[col] = df[col].astype('string')\n\n# Verify the changes\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 719 entries, 0 to 718\nData columns (total 13 columns):\n #   Column           Non-Null Count  Dtype  \n---  ------           --------------  -----  \n 0   Unnamed: 0       719 non-null    int64  \n 1   city_mpg         719 non-null    int64  \n 2   class            719 non-null    string \n 3   combination_mpg  719 non-null    int64  \n 4   cylinders        595 non-null    float64\n 5   displacement     595 non-null    float64\n 6   drive            711 non-null    string \n 7   fuel_type        719 non-null    string \n 8   highway_mpg      719 non-null    int64  \n 9   make             719 non-null    string \n 10  model            719 non-null    string \n 11  transmission     719 non-null    string \n 12  year             719 non-null    int64  \ndtypes: float64(2), int64(5), string(6)\nmemory usage: 73.2 KB\n\n\n\n# get y \n\ny = df['fuel_type'].unique().tolist()\n\n# Replace 'diesel' with 'gas' in the entire DataFrame\ndf.replace('diesel', 'gas', inplace=True)\n\ny = df['fuel_type'].unique().tolist()\n\ny\n\n['gas', 'electricity']\n\n\n\n\n# Dropping non-numerical and unnecessary columns\ndf = df.drop(columns=['Unnamed: 0'])\n\n\n# Replace continuous missing values with mean of the column. check for Nan values again.\n\ncols = ['displacement', 'cylinders']\ndf[cols] = df[cols].fillna(df[cols].mean())\n\nnan_count = df.isna().sum()\nprint(nan_count)\n\ncity_mpg           0\nclass              0\ncombination_mpg    0\ncylinders          0\ndisplacement       0\ndrive              8\nfuel_type          0\nhighway_mpg        0\nmake               0\nmodel              0\ntransmission       0\nyear               0\ndtype: int64\n\n\n\n# Replace categorical missing values with mode of the column. check for Nan values again.\n\ndf['drive'] = df['drive'].fillna(df['drive'].mode().iloc[0])\n\nnan_count = df.isna().sum()\nprint(nan_count)\n\ncity_mpg           0\nclass              0\ncombination_mpg    0\ncylinders          0\ndisplacement       0\ndrive              0\nfuel_type          0\nhighway_mpg        0\nmake               0\nmodel              0\ntransmission       0\nyear               0\ndtype: int64\n\n\n\n# Using a for loop to replace categorical values with cat codes\ncat_cols = ['class', 'drive', 'fuel_type', 'make', 'model', 'transmission']\nfor col in cat_cols:\n    df[col] = df[col].astype('category')\n    df[col] = df[col].cat.codes\n\n# Display the altered DataFrame\ndf.head()\n\n\n\n\n\n\n\n\n\ncity_mpg\nclass\ncombination_mpg\ncylinders\ndisplacement\ndrive\nfuel_type\nhighway_mpg\nmake\nmodel\ntransmission\nyear\n\n\n\n\n0\n18\n2\n21\n4.0\n2.2\n2\n1\n26\n24\n33\n0\n1993\n\n\n1\n19\n2\n22\n4.0\n2.2\n2\n1\n27\n24\n33\n1\n1993\n\n\n2\n16\n2\n19\n6.0\n3.0\n2\n1\n22\n24\n33\n0\n1993\n\n\n3\n16\n2\n18\n6.0\n3.0\n2\n1\n22\n24\n33\n1\n1993\n\n\n4\n18\n4\n21\n4.0\n2.2\n2\n1\n26\n24\n33\n0\n1993\n\n\n\n\n\n\n\n\n\n# LOOK AT FIRST ROW\nprint(df.iloc[0])\n\ncity_mpg             18.0\nclass                 2.0\ncombination_mpg      21.0\ncylinders             4.0\ndisplacement          2.2\ndrive                 2.0\nfuel_type             1.0\nhighway_mpg          26.0\nmake                 24.0\nmodel                33.0\ntransmission          0.0\nyear               1993.0\nName: 0, dtype: float64\n\n\n\n# INSERT CODE TO MAKE DATA-FRAMES (or numpy arrays) (X,Y) WHERE Y=\"target\" COLUMN and X=\"everything else\"\n# Resource used: https://medium.com/codex/how-to-set-x-and-y-in-pandas-3f38584e9bed\n\n\nX = df.drop('fuel_type', axis=1)  # X includes everything except the target column\ny = df['fuel_type']  # Y is just the target column\n\n\nprint(X.shape)\nprint(y.shape)\n\n(719, 11)\n(719,)\n\n\n\nprint(X.iloc[0:11])\n\n    city_mpg  class  combination_mpg  cylinders  displacement  drive  \\\n0         18      2               21        4.0           2.2      2   \n1         19      2               22        4.0           2.2      2   \n2         16      2               19        6.0           3.0      2   \n3         16      2               18        6.0           3.0      2   \n4         18      4               21        4.0           2.2      2   \n5         23      0               24        4.0           1.6      2   \n6         23      0               26        4.0           1.6      2   \n7         23      0               25        4.0           1.8      2   \n8         23      0               26        4.0           1.8      2   \n9         23      9               25        4.0           1.8      2   \n10        21     10               23        4.0           2.0      2   \n\n    highway_mpg  make  model  transmission  year  \n0            26    24     33             0  1993  \n1            27    24     33             1  1993  \n2            22    24     33             0  1993  \n3            22    24     33             1  1993  \n4            26    24     33             0  1993  \n5            26    24     37             0  1993  \n6            31    24     37             1  1993  \n7            30    24     37             0  1993  \n8            30    24     37             1  1993  \n9            30    24     37             0  1993  \n10           26    24    104             0  1996  \n\n\n\nprint(pd.DataFrame(y[0:10]))\n\n   fuel_type\n0          1\n1          1\n2          1\n3          1\n4          1\n5          1\n6          1\n7          1\n8          1\n9          1\n\n\n\n\n#NORMALIZE \nX=0.1+(X-np.min(X,axis=0))/(np.max(X,axis=0)-np.min(X,axis=0))\ny=0.1+(y-np.min(y,axis=0))/(np.max(y,axis=0)-np.min(y,axis=0))\n\n\nimport pandas as pd\n\n# If X is a NumPy array, convert it to a DataFrame\ndf_features = pd.DataFrame(X)\n\n# Add the target variable y to the DataFrame\ndf_features['fuel_type'] = y.values  # .values will convert the Series to a NumPy array if needed\n\n\nprint(df.describe())\n\n         city_mpg       class  combination_mpg   cylinders  displacement  \\\ncount  719.000000  719.000000       719.000000  719.000000    719.000000   \nmean    34.051460    7.634214        35.094576    5.154622      2.760336   \nstd     32.732968    5.233117        30.071909    1.276406      0.974911   \nmin      9.000000    0.000000        11.000000    3.000000      1.400000   \n25%     16.000000    2.000000        19.000000    4.000000      2.000000   \n50%     20.000000    9.000000        22.000000    5.154622      2.760336   \n75%     27.000000   12.000000        30.000000    6.000000      3.100000   \nmax    150.000000   15.000000       136.000000   10.000000      6.200000   \n\n            drive   fuel_type  highway_mpg        make       model  \\\ncount  719.000000  719.000000   719.000000  719.000000  719.000000   \nmean     1.815021    0.827538    37.329624   13.137691   73.645341   \nstd      0.811132    0.378044    26.892826    8.400770   43.532087   \nmin      0.000000    0.000000    12.000000    0.000000    0.000000   \n25%      1.000000    1.000000    22.000000    5.000000   36.000000   \n50%      2.000000    1.000000    26.000000   14.000000   72.000000   \n75%      2.000000    1.000000    34.000000   22.000000  111.500000   \nmax      3.000000    1.000000   123.000000   26.000000  150.000000   \n\n       transmission         year  \ncount    719.000000   719.000000  \nmean       0.219750  2006.634214  \nstd        0.414365    11.345719  \nmin        0.000000  1984.000000  \n25%        0.000000  1995.000000  \n50%        0.000000  2007.000000  \n75%        0.000000  2018.000000  \nmax        1.000000  2023.000000  \n\n\n\nprint(\"X CORRELATION:\") \ncorr = df.corr(); print(corr)                   #COMPUTE CORRELATION OF FEATER MATRIX\n\nX CORRELATION:\n                 city_mpg     class  combination_mpg     cylinders  \\\ncity_mpg         1.000000 -0.213484         0.998454 -1.213155e-01   \nclass           -0.213484  1.000000        -0.222664  1.415416e-01   \ncombination_mpg  0.998454 -0.222664         1.000000 -1.321393e-01   \ncylinders       -0.121316  0.141542        -0.132139  1.000000e+00   \ndisplacement    -0.128895  0.169821        -0.141069  9.217459e-01   \ndrive           -0.108960 -0.029415        -0.108910  2.397666e-02   \nfuel_type       -0.934239  0.172228        -0.938093 -1.723270e-16   \nhighway_mpg      0.991697 -0.236399         0.997063 -1.442941e-01   \nmake             0.168306 -0.146960         0.172437 -9.592731e-02   \nmodel            0.115270  0.058427         0.118600 -3.518819e-02   \ntransmission    -0.240810  0.118692        -0.238849 -1.011991e-01   \nyear             0.561037 -0.075022         0.566843 -1.142699e-01   \n\n                 displacement     drive     fuel_type  highway_mpg      make  \\\ncity_mpg        -1.288949e-01 -0.108960 -9.342390e-01     0.991697  0.168306   \nclass            1.698205e-01 -0.029415  1.722283e-01    -0.236399 -0.146960   \ncombination_mpg -1.410687e-01 -0.108910 -9.380932e-01     0.997063  0.172437   \ncylinders        9.217459e-01  0.023977 -1.723270e-16    -0.144294 -0.095927   \ndisplacement     1.000000e+00  0.037908 -1.868610e-16    -0.155732 -0.030382   \ndrive            3.790814e-02  1.000000  1.819618e-01    -0.105232 -0.016696   \nfuel_type       -1.868610e-16  0.181962  1.000000e+00    -0.938278 -0.164860   \nhighway_mpg     -1.557315e-01 -0.105232 -9.382783e-01     1.000000  0.175792   \nmake            -3.038238e-02 -0.016696 -1.648605e-01     0.175792  1.000000   \nmodel           -9.926496e-02 -0.095618 -1.414992e-01     0.119540  0.018262   \ntransmission    -1.190592e-01  0.187411  2.422699e-01    -0.233231 -0.047114   \nyear            -7.101525e-02 -0.284616 -5.358940e-01     0.569990  0.097615   \n\n                    model  transmission      year  \ncity_mpg         0.115270     -0.240810  0.561037  \nclass            0.058427      0.118692 -0.075022  \ncombination_mpg  0.118600     -0.238849  0.566843  \ncylinders       -0.035188     -0.101199 -0.114270  \ndisplacement    -0.099265     -0.119059 -0.071015  \ndrive           -0.095618      0.187411 -0.284616  \nfuel_type       -0.141499      0.242270 -0.535894  \nhighway_mpg      0.119540     -0.233231  0.569990  \nmake             0.018262     -0.047114  0.097615  \nmodel            1.000000     -0.043622  0.113015  \ntransmission    -0.043622      1.000000 -0.365931  \nyear             0.113015     -0.365931  1.000000  \n\n\n\n# INSERT CODE TO SHOW A HEAT MAP FOR THE X FEATURES\n\nsns.set_theme(style=\"white\")\nf, ax = plt.subplots(figsize=(11, 9))  # Set up the matplotlib figure\ncmap = sns.diverging_palette(230, 20, as_cmap=True)     # Generate a custom diverging colormap\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr,  cmap=cmap, vmin=-1, vmax=1, center=0,\n        square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\nplt.show();\n\n\n\n\n\n\n\n\n\n# # # # INSERT CODE TO GENERATE A PAIR-PLOT \nsns.pairplot(df)\nplt.show()\n\n/Users/isfarbaset/anaconda3/lib/python3.11/site-packages/seaborn/axisgrid.py:118: UserWarning: The figure layout has changed to tight\n  self._figure.tight_layout(*args, **kwargs)\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.model_selection import train_test_split\n\n# Assuming 'X' is a pandas DataFrame and you want to drop the first two columns\nX = X.iloc[:, 2:]  # Select all rows and all columns starting from the third\n\n# Now you can perform the train-test split\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n\n# If 'y' is a pandas Series and you want to convert it to a 1D numpy array\ny_train = y_train.values.flatten()\ny_test = y_test.values.flatten()\n\n# Printing shapes and sizes\nprint(\"x_train.shape :\", x_train.shape)\nprint(\"y_train.shape :\", y_train.shape)\nprint(\"x_test.shape  :\", x_test.shape)\nprint(\"y_test.shape  :\", y_test.shape)\n\nx_train.shape : (575, 9)\ny_train.shape : (575,)\nx_test.shape  : (144, 9)\ny_test.shape  : (144,)\n\n\n\nfrom sklearn.metrics import mean_absolute_percentage_error\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.tree import DecisionTreeRegressor\n\n\n# HYPER PARAMETER SEARCH FOR OPTIMAL NUMBER OF NEIGHBORS \nhyper_param=[]\ntrain_error=[]\ntest_error=[]\n\n# LOOP OVER HYPER-PARAM\nfor i in range(1,40):\n    # INITIALIZE MODEL \n    model = DecisionTreeRegressor(max_depth=i)\n\n    # TRAIN MODEL \n    model.fit(x_train,y_train)\n\n    # OUTPUT PREDICTIONS FOR TRAINING AND TEST SET \n    yp_train = model.predict(x_train)\n    yp_test = model.predict(x_test)\n\n    # shift=1+np.min(y_train) #add shift to remove division by zero \n    err1=mean_absolute_error(y_train, yp_train) \n    err2=mean_absolute_error(y_test, yp_test) \n    \n    # err1=100.0*np.mean(np.absolute((yp_train-y_train)/y_train))\n    # err2=100.0*np.mean(np.absolute((yp_test-y_test)/y_test))\n\n    hyper_param.append(i)\n    train_error.append(err1)\n    test_error.append(err2)\n\n    if(i==1 or i%10==0):\n        print(\"hyperparam =\",i)\n        print(\" train error:\",err1)\n        print(\" test error:\" ,err2)\n\nhyperparam = 1\n train error: 7.69261466121199e-15\n test error: 7.996207862515092e-15\nhyperparam = 10\n train error: 7.69261466121199e-15\n test error: 7.996207862515092e-15\nhyperparam = 20\n train error: 7.69261466121199e-15\n test error: 7.996207862515092e-15\nhyperparam = 30\n train error: 7.69261466121199e-15\n test error: 7.996207862515092e-15\n\n\n\nplt.plot(hyper_param,train_error ,linewidth=2, color='k')\nplt.plot(hyper_param,test_error ,linewidth=2, color='b')\n\nplt.xlabel(\"Depth of tree (max depth)\")\nplt.ylabel(\"Training (black) and test (blue) MAE (error)\")\n\ni=1\nprint(hyper_param[i],train_error[i],test_error[i])\n\n2 7.69261466121199e-15 7.996207862515092e-15\n\n\n\n\n\n\n\n\n\n\n# HYPER PARAMETER SEARCH FOR OPTIMAL NUMBER OF NEIGHBORS \nhyper_param=[]\ntrain_error=[]\ntest_error=[]\n\n# LOOP OVER HYPER-PARAM\nfor i in range(2,100):\n    # INITIALIZE MODEL \n    model = DecisionTreeRegressor(min_samples_split=i)\n\n    # TRAIN MODEL \n    model.fit(x_train,y_train)\n\n    # OUTPUT PREDICTIONS FOR TRAINING AND TEST SET \n    yp_train = model.predict(x_train)\n    yp_test = model.predict(x_test)\n\n    # shift=1+np.min(y_train) #add shift to remove division by zero \n    err1=mean_absolute_error(y_train, yp_train) \n    err2=mean_absolute_error(y_test, yp_test) \n    \n    # err1=100.0*np.mean(np.absolute((yp_train-y_train)/y_train))\n    # err2=100.0*np.mean(np.absolute((yp_test-y_test)/y_test))\n\n    hyper_param.append(i)\n    train_error.append(err1)\n    test_error.append(err2)\n\n    if(i%10==0):\n        print(\"hyperparam =\",i)\n        print(\" train error:\",err1)\n        print(\" test error:\" ,err2)\n\nhyperparam = 10\n train error: 7.69261466121199e-15\n test error: 7.996207862515092e-15\nhyperparam = 20\n train error: 7.69261466121199e-15\n test error: 7.996207862515092e-15\nhyperparam = 30\n train error: 7.69261466121199e-15\n test error: 7.996207862515092e-15\nhyperparam = 40\n train error: 7.69261466121199e-15\n test error: 7.996207862515092e-15\nhyperparam = 50\n train error: 7.69261466121199e-15\n test error: 7.996207862515092e-15\nhyperparam = 60\n train error: 7.69261466121199e-15\n test error: 7.996207862515092e-15\nhyperparam = 70\n train error: 7.69261466121199e-15\n test error: 7.996207862515092e-15\nhyperparam = 80\n train error: 7.69261466121199e-15\n test error: 7.996207862515092e-15\nhyperparam = 90\n train error: 7.69261466121199e-15\n test error: 7.996207862515092e-15\n\n\n\nplt.plot(hyper_param,train_error ,linewidth=2, color='k')\nplt.plot(hyper_param,test_error ,linewidth=2, color='b')\n\nplt.xlabel(\"Minimum number of points in split (min_samples_split)\")\nplt.ylabel(\"Training (black) and test (blue) MAE (error)\")\n\nText(0, 0.5, 'Training (black) and test (blue) MAE (error)')"
  },
  {
    "objectID": "5000-website/decision-tree/reference-notebooks/practice2.html",
    "href": "5000-website/decision-tree/reference-notebooks/practice2.html",
    "title": "Decision Trees",
    "section": "",
    "text": "About the data: - X strings from wikipedia articles about different topics (need to be vectorized first)\nShow the code\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport numpy as np\nfrom sklearn.metrics import accuracy_score"
  },
  {
    "objectID": "5000-website/decision-tree/reference-notebooks/practice2.html#utility-functions",
    "href": "5000-website/decision-tree/reference-notebooks/practice2.html#utility-functions",
    "title": "Decision Trees",
    "section": "Utility functions",
    "text": "Utility functions\n\nWrite a function to report accuracy\nNote this will act on object stored in pythons global scope. Therefore as long as everything is named the same you can recycle it for multiple models\nA function to plot the confusion plot\n\n\n\nShow the code\ndef report(y,ypred):\n      #ACCURACY COMPUTE \n      print(\"Accuracy:\",accuracy_score(y, ypred)*100)\n      print(\"Number of mislabeled points out of a total %d points = %d\"\n            % (y.shape[0], (y != ypred).sum()))\n\ndef print_model_summary():\n      # LABEL PREDICTIONS FOR TRAINING AND TEST SET \n      yp_train = model.predict(x_train)\n      yp_test = model.predict(x_test)\n\n      print(\"ACCURACY CALCULATION\\n\")\n\n      print(\"TRAINING SET:\")\n      report(y_train,yp_train)\n\n      print(\"\\nTEST SET (UNTRAINED DATA):\")\n      report(y_test,yp_test)\n\n      print(\"\\nCHECK FIRST 20 PREDICTIONS\")\n      print(\"TRAINING SET:\")\n      print(y_train[0:20])\n      print(yp_train[0:20])\n      print(\"ERRORS:\",yp_train[0:20]-y_train[0:20])\n\n      print(\"\\nTEST SET (UNTRAINED DATA):\")\n      print(y_test[0:20])\n      print(yp_test[0:20])\n      print(\"ERRORS:\",yp_test[0:20]-y_test[0:20])\n\n\n\n\nShow the code\n#INSERT CODE TO WRITE A FUNCTION def confusion_plot(y_data,y_pred) WHICH GENERATES A CONFUSION MATRIX PLOT AND PRINTS THE INFORMATION ABOVE (see link above for example)\n\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_classification\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nimport pandas as pd\nimport seaborn as sns \nimport matplotlib.pyplot as plt\nfrom sklearn import tree\nfrom IPython.display import Image\nimport numpy as np\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\n\ndef confusion_plot(y_data, y_pred):\n\n    cm = confusion_matrix(y_data, y_pred)\n\n    # Calculate metrics\n    accuracy = accuracy_score(y_data, y_pred)\n    negative_recall = recall_score(y_data, y_pred, average= None)\n    negative_precision = precision_score(y_data, y_pred, average= None)\n    positive_recall = recall_score(y_data, y_pred, average= None)\n    positive_precision = precision_score(y_data, y_pred, average= None)\n    \n    # Print metrics\n    print(f\"ACCURACY: {accuracy}\")\n    print(f\"NEGATIVE RECALL (Y=0): {negative_recall}\")\n    print(f\"NEGATIVE PRECISION (Y=0): {negative_precision}\")\n    print(f\"POSITIVE RECALL (Y=1): {positive_recall}\")\n    print(f\"POSITIVE PRECISION (Y=1): {positive_precision}\")\n    print(cm)\n    \n    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n    \n    disp.plot()\n    plt.show()\n\n\n\n\nShow the code\n#RELOAD FILE AND PRETEND THAT IS OUR STARTING POINT \ndf=pd.read_csv('../eda/ev-wiki-crawl-results.csv')  \nprint(df.shape)\n\n#CONVERT FROM STRING LABELS TO INTEGERS \nlabels=[]; #=[]; y2=[]\ny1=[]\nfor label in df[\"label\"]:\n    if label not in labels:\n        labels.append(label)\n        print(\"index =\",len(labels)-1,\": label =\",label)\n    for i in range(0,len(labels)):\n        if(label==labels[i]):\n            y1.append(i)\ny1=np.array(y1)\n\n# CONVERT DF TO LIST OF STRINGS \ncorpus=df[\"text\"].to_list()\ny2=df[\"sentiment\"].to_numpy()\n\nprint(\"number of text chunks = \",len(corpus))\nprint(corpus[0:3])\n\n\n(1225, 3)\nindex = 0 : label = electric vehicle\nindex = 1 : label = gasoline vehicle\nindex = 2 : label = hybrid vehicle\nnumber of text chunks =  1225\n['electric motive power started 1827 hungarian priest nyos jedlik built first crude viable electric motor used stator rotor commutator next year used power small car 1835 professor sibrandus stratingh university groningen netherlands built small scale electric car sometime 1832 1839 robert anderson scotland invented first crude electric carriage powered non rechargeable primary cell american blacksmith inventor thomas davenport built toy electric locomotive powered primitive electric motor 1835 1838 scotsman named robert davidson built electric locomotive attained speed four mile per hour km england patent granted 1840 use rail conductor electric current similar american patent issued lilley colten 1847', 'first mass produced appeared america early 1900s 1902 studebaker automobile company entered automotive business though also entered gasoline vehicle market 1904 however advent cheap assembly line car ford motor company popularity electric car declined significantly due lack electricity grid limitation storage battery time electric car gain much popularity however electric train gained immense popularity due economy achievable speed 20th century electric rail transport became commonplace due advance development electric locomotive time general purpose commercial use reduced specialist role platform truck forklift truck ambulance tow tractor urban delivery vehicle iconic british milk float', '20th century uk world largest user electric road vehicle electrified train used coal transport motor use valuable oxygen mine switzerland lack natural fossil resource forced rapid electrification rail network one earliest rechargeable battery nickel iron battery favored edison use electric car ev among earliest automobile preeminence light powerful internal combustion engine ice electric automobile held many vehicle land speed distance record early 1900s produced baker electric columbia electric detroit electric others one point history outsold gasoline powered vehicle']\n\n\n\n\nShow the code\ndf.head()\n\n\n\n\n\n\n\n\n\n\ntext\nlabel\nsentiment\n\n\n\n\n0\nelectric motive power started 1827 hungarian p...\nelectric vehicle\n-0.7506\n\n\n1\nfirst mass produced appeared america early 190...\nelectric vehicle\n0.9201\n\n\n2\n20th century uk world largest user electric ro...\nelectric vehicle\n0.7096\n\n\n3\n1900 28 percent car road electric ev popular e...\nelectric vehicle\n0.9169\n\n\n4\nseldom marketed woman luxury car may stigma am...\nelectric vehicle\n0.9231"
  },
  {
    "objectID": "5000-website/decision-tree/reference-notebooks/practice2.html#vectorize",
    "href": "5000-website/decision-tree/reference-notebooks/practice2.html#vectorize",
    "title": "Decision Trees",
    "section": "Vectorize",
    "text": "Vectorize\n\n\nShow the code\n# INITIALIZE COUNT VECTORIZER\n# minDF = 0.01 means \"ignore terms that appear in less than 1% of the documents\". \n# minDF = 5 means \"ignore terms that appear in less than 5 documents\".\nvectorizer=CountVectorizer(min_df=0.0001)   \n\n# RUN COUNT VECTORIZER ON OUR COURPUS \nXs  =  vectorizer.fit_transform(corpus)   \nX=np.array(Xs.todense())\n\n#CONVERT TO ONE-HOT VECTORS\nmaxs=np.max(X,axis=0)\nX=np.ceil(X/maxs)\n\n# DOUBLE CHECK \nprint(X.shape,y1.shape,y2.shape)\n\n\n(1225, 8233) (1225,) (1225,)"
  },
  {
    "objectID": "5000-website/decision-tree/reference-notebooks/practice2.html#partition-data",
    "href": "5000-website/decision-tree/reference-notebooks/practice2.html#partition-data",
    "title": "Decision Trees",
    "section": "Partition Data",
    "text": "Partition Data\n\n\nShow the code\n# BEFORE SPLIT\nprint(y1[1000:1200])\n\n\n[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n\n\n\n\nShow the code\n#INSERT CODE TO PARTITION DATASET INTO TRAINING-TEST\n\nfrom sklearn.model_selection import train_test_split\ntest_ratio=0.2\nx_train, x_test, y_train, y_test = train_test_split(X, y1, test_size=test_ratio, random_state=0)\ny_train=y_train.flatten()\ny_test=y_test.flatten()\n\nprint(\"x_train.shape        :\",x_train.shape)\nprint(\"y_train.shape        :\",y_train.shape)\n\nprint(\"X_test.shape     :\",x_test.shape)\nprint(\"y_test.shape     :\",y_test.shape)\n\n\nx_train.shape       : (980, 8233)\ny_train.shape       : (980,)\nX_test.shape        : (245, 8233)\ny_test.shape        : (245,)\n\n\n\n\nShow the code\n#CHECK TO MAKE SURE IT WAS RANDOMIZED \nprint(y_train[0:100])\n\n\n[0 1 1 2 1 2 2 1 0 0 1 1 2 0 2 0 1 0 0 2 0 1 1 0 1 1 2 2 1 1 1 1 0 0 2 0 1\n 0 1 1 0 1 2 2 1 2 1 2 1 2 1 2 0 1 1 2 0 2 0 2 0 1 2 2 2 0 1 0 0 0 0 1 2 2\n 1 0 1 1 1 1 0 0 1 0 2 1 0 0 1 2 0 1 1 1 2 1 1 0 0 0]"
  },
  {
    "objectID": "5000-website/decision-tree/reference-notebooks/practice2.html#search-1-remove-features-from-high-to-low",
    "href": "5000-website/decision-tree/reference-notebooks/practice2.html#search-1-remove-features-from-high-to-low",
    "title": "Decision Trees",
    "section": "Search-1: Remove features from high to low",
    "text": "Search-1: Remove features from high to low\n\n\nShow the code\n#UTILITY FUNCTION TO INITIALIZE RELEVANT ARRAYS\ndef initialize_arrays():\n    global num_features,train_accuracies\n    global test_accuracies,train_time,eval_time\n    num_features=[]\n    train_accuracies=[]\n    test_accuracies=[]\n    train_time=[]\n    eval_time=[]\n\n\n\n\nShow the code\n# INITIALIZE ARRAYS\ninitialize_arrays()\n\n# DEFINE SEARCH FUNCTION\ndef partial_grid_search(num_runs, min_index, max_index):\n    for i in range(1, num_runs+1):\n        # SUBSET FEATURES \n        upper_index=min_index+i*int((max_index-min_index)/num_runs)\n        xtmp=x[:,0:upper_index]\n\n        #TRAIN \n        (acc_train,acc_test,time_train,time_eval)=train_DTC_model(xtmp,y,i_print=False)\n\n        if(i%5==0):\n            print(i,upper_index,xtmp.shape[1],acc_train,acc_test)\n            \n        #RECORD \n        num_features.append(xtmp.shape[1])\n        train_accuracies.append(acc_train)\n        test_accuracies.append(acc_test)\n        train_time.append(time_train)\n        eval_time.append(time_eval)\n\n# DENSE SEARCH (SMALL NUMBER OF FEATURES (FAST))\npartial_grid_search(num_runs=100, min_index=0, max_index=1000)\n\n# SPARSE SEARCH (LARGE NUMBER OF FEATURES (SLOWER))\npartial_grid_search(num_runs=20, min_index=1000, max_index=10000)\n\n\n5 50 50 100.0 82.85714285714286\n10 100 100 100.0 80.40816326530611\n15 150 150 100.0 81.63265306122449\n20 200 200 100.0 82.85714285714286\n25 250 250 100.0 81.22448979591836\n30 300 300 100.0 84.48979591836735\n35 350 350 100.0 81.22448979591836\n40 400 400 100.0 80.0\n45 450 450 100.0 80.40816326530611\n50 500 500 100.0 81.22448979591836\n55 550 550 100.0 84.48979591836735\n60 600 600 100.0 83.6734693877551\n65 650 650 100.0 84.08163265306122\n70 700 700 100.0 86.12244897959184\n75 750 750 100.0 84.48979591836735\n80 800 800 100.0 84.89795918367346\n85 850 850 100.0 85.71428571428571\n90 900 900 100.0 86.53061224489797\n95 950 950 100.0 85.71428571428571\n100 1000 1000 100.0 85.3061224489796\n5 3250 3250 100.0 84.89795918367346\n10 5500 5500 100.0 86.12244897959184\n15 7750 7140 100.0 86.53061224489797\n20 10000 7140 100.0 85.3061224489796\n\n\n\n\nShow the code\n#UTILITY FUNCTION TO SAVE RESULTS\ndef save_results(path_root):\n    out=np.transpose(np.array([num_features,train_accuracies,test_accuracies,train_time,eval_time])) \n    out=pd.DataFrame(out)\n    out.to_csv(path_root+\".csv\")\n\n\n\n\nShow the code\n#UTILITY FUNCTION TO PLOT RESULTS\ndef plot_results(path_root):\n\n    #PLOT-1\n    plt.plot(num_features,train_accuracies,'-or')\n    plt.plot(num_features,test_accuracies,'-ob')\n    plt.xlabel('Number of features')\n    plt.ylabel('ACCURACY: Training (blue) and Test (red)')\n    plt.savefig(path_root+'-1.png')\n    plt.show()\n\n    # #PLOT-2\n    plt.plot(num_features,train_time,'-or')\n    plt.plot(num_features,eval_time,'-ob')\n    plt.xlabel('Number of features')\n    plt.ylabel('Runtime: training time (red) and evaluation time(blue)')\n    plt.savefig(path_root+'-2.png')\n    plt.show()\n\n    # #PLOT-3\n    plt.plot(np.array(test_accuracies),train_time,'-or')\n    plt.plot(np.array(test_accuracies),eval_time,'-ob')\n    plt.xlabel('test_accuracies')\n    plt.ylabel('Runtime: training time (red) and evaluation time (blue)')\n    plt.savefig(path_root+'-3.png')\n    plt.show()\n\n    # #PLOT-3\n    plt.plot(num_features,np.array(train_accuracies)-np.array(test_accuracies),'-or')\n    plt.xlabel('Number of features')\n    plt.ylabel('train_accuracies-test_accuracies')\n    plt.savefig(path_root+'-4.png')\n    plt.show()\n\n\n\n\nShow the code\nsave_results(output_dir+\"/partial_grid_search\")\nplot_results(output_dir+\"/partial_grid_search\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nx_var=np.var(x,axis=0)\nprint(np.min(x_var))\nprint(np.max(x_var))\n\n\n0.0008156601416076161\n0.24855910037484058"
  },
  {
    "objectID": "5000-website/decision-tree/reference-notebooks/practice2.html#variance-threshold",
    "href": "5000-website/decision-tree/reference-notebooks/practice2.html#variance-threshold",
    "title": "Decision Trees",
    "section": "Variance Threshold",
    "text": "Variance Threshold\n\n\nShow the code\nfrom sklearn.feature_selection import VarianceThreshold\n\n# DEFINE GRID OF THRESHOLDS \nnum_thresholds=30\nthresholds=np.linspace(np.min(x_var),np.max(x_var),num_thresholds)\n\n#DOESN\"T WORK WELL WITH EDGE VALUES \nthresholds=thresholds[1:-2]; #print(thresholds)\n\n# INITIALIZE ARRAYS\ninitialize_arrays()\n\n# SEARCH FOR OPTIMAL THRESHOLD\nfor THRESHOLD in thresholds:\n    feature_selector = VarianceThreshold(threshold=THRESHOLD)\n    xtmp=feature_selector.fit_transform(x)\n    print(\"THRESHOLD =\",THRESHOLD, xtmp.shape[1])\n\n    (acc_train,acc_test,time_train,time_eval)=train_DTC_model(xtmp,y,i_print=False)\n             \n    #RECORD \n    num_features.append(xtmp.shape[1])\n    train_accuracies.append(acc_train)\n    test_accuracies.append(acc_test)\n    train_time.append(time_train)\n    eval_time.append(time_eval)\n\n\nTHRESHOLD = 0.009358537391029442 1088\nTHRESHOLD = 0.01790141464045127 609\nTHRESHOLD = 0.026444291889873094 414\nTHRESHOLD = 0.03498716913929492 278\nTHRESHOLD = 0.04353004638871675 211\nTHRESHOLD = 0.05207292363813857 155\nTHRESHOLD = 0.0606158008875604 127\nTHRESHOLD = 0.06915867813698222 95\nTHRESHOLD = 0.07770155538640404 73\nTHRESHOLD = 0.08624443263582587 61\nTHRESHOLD = 0.0947873098852477 52\nTHRESHOLD = 0.10333018713466952 43\nTHRESHOLD = 0.11187306438409135 36\nTHRESHOLD = 0.12041594163351317 32\nTHRESHOLD = 0.128958818882935 28\nTHRESHOLD = 0.13750169613235683 24\nTHRESHOLD = 0.14604457338177865 21\nTHRESHOLD = 0.15458745063120047 17\nTHRESHOLD = 0.16313032788062232 13\nTHRESHOLD = 0.17167320513004414 13\nTHRESHOLD = 0.18021608237946596 11\nTHRESHOLD = 0.18875895962888778 11\nTHRESHOLD = 0.1973018368783096 8\nTHRESHOLD = 0.20584471412773142 8\nTHRESHOLD = 0.21438759137715327 7\nTHRESHOLD = 0.2229304686265751 6\nTHRESHOLD = 0.2314733458759969 6\n\n\n\n\nShow the code\n# CHECK RESULTS \nsave_results(output_dir+\"/variance_threshold\")\nplot_results(output_dir+\"/variance_threshold\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\n# # INSERT CODE TO USE THE MODEL TO MAKE PREDICTIONS FOR THE TRAINING AND TEST SET \n# yp_train = model.predict(x_train)\n# yp_test = model.predict(x_test)"
  },
  {
    "objectID": "5000-website/decision-tree/reference-notebooks/practice2.html#random-forest-classifier-final-results",
    "href": "5000-website/decision-tree/reference-notebooks/practice2.html#random-forest-classifier-final-results",
    "title": "Decision Trees",
    "section": "Random Forest Classifier Final results",
    "text": "Random Forest Classifier Final results\n\n\nShow the code\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Create the random forest model with a specific number of trees (n_estimators)\nmodel = RandomForestClassifier(n_estimators=100, max_depth=200)\n\n# Fit the model to your training data\nmodel = model.fit(x_train, y_train)\n\n# Predict on training and test data\nyp_train = model.predict(x_train)\nyp_test = model.predict(x_test)\n\n\n\n\nShow the code\n# RUN THE FOLLOWING CODE TO TEST YOUR FUNCTION \nprint(\"------TRAINING------\")\nconfusion_plot(y_train,yp_train)\nprint(\"------TEST------\")\nconfusion_plot(y_test,yp_test)\n\n\n------TRAINING------\nACCURACY: 0.7918367346938775\nNEGATIVE RECALL (Y=0): [0.8559322  0.8470255  0.63736264]\nNEGATIVE PRECISION (Y=0): [0.78092784 0.81917808 0.76651982]\nPOSITIVE RECALL (Y=1): [0.8559322  0.8470255  0.63736264]\nPOSITIVE PRECISION (Y=1): [0.78092784 0.81917808 0.76651982]\n[[303  26  25]\n [ 26 299  28]\n [ 59  40 174]]\n------TEST------\nACCURACY: 0.46122448979591835\nNEGATIVE RECALL (Y=0): [0.50666667 0.64516129 0.19480519]\nNEGATIVE PRECISION (Y=0): [0.38383838 0.68181818 0.25862069]\nPOSITIVE RECALL (Y=1): [0.50666667 0.64516129 0.19480519]\nPOSITIVE PRECISION (Y=1): [0.38383838 0.68181818 0.25862069]\n[[38  8 29]\n [19 60 14]\n [42 20 15]]"
  },
  {
    "objectID": "5000-website/decision-tree/reference-notebooks/practice.html",
    "href": "5000-website/decision-tree/reference-notebooks/practice.html",
    "title": "Decision Trees",
    "section": "",
    "text": "About the data: - X strings from wikipedia articles about different topics (need to be vectorized first)\nShow the code\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport numpy as np\nfrom sklearn.metrics import accuracy_score"
  },
  {
    "objectID": "5000-website/decision-tree/reference-notebooks/practice.html#utility-functions",
    "href": "5000-website/decision-tree/reference-notebooks/practice.html#utility-functions",
    "title": "Decision Trees",
    "section": "Utility functions",
    "text": "Utility functions\n\nWrite a function to report accuracy\nNote this will act on object stored in pythons global scope. Therefore as long as everything is named the same you can recycle it for multiple models\nA function to plot the confusion plot\n\n\n\nShow the code\ndef report(y,ypred):\n      #ACCURACY COMPUTE \n      print(\"Accuracy:\",accuracy_score(y, ypred)*100)\n      print(\"Number of mislabeled points out of a total %d points = %d\"\n            % (y.shape[0], (y != ypred).sum()))\n\ndef print_model_summary():\n      # LABEL PREDICTIONS FOR TRAINING AND TEST SET \n      yp_train = model.predict(x_train)\n      yp_test = model.predict(x_test)\n\n      print(\"ACCURACY CALCULATION\\n\")\n\n      print(\"TRAINING SET:\")\n      report(y_train,yp_train)\n\n      print(\"\\nTEST SET (UNTRAINED DATA):\")\n      report(y_test,yp_test)\n\n      print(\"\\nCHECK FIRST 20 PREDICTIONS\")\n      print(\"TRAINING SET:\")\n      print(y_train[0:20])\n      print(yp_train[0:20])\n      print(\"ERRORS:\",yp_train[0:20]-y_train[0:20])\n\n      print(\"\\nTEST SET (UNTRAINED DATA):\")\n      print(y_test[0:20])\n      print(yp_test[0:20])\n      print(\"ERRORS:\",yp_test[0:20]-y_test[0:20])\n\n\n\n\nShow the code\n#INSERT CODE TO WRITE A FUNCTION def confusion_plot(y_data,y_pred) WHICH GENERATES A CONFUSION MATRIX PLOT AND PRINTS THE INFORMATION ABOVE (see link above for example)\n\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_classification\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nimport pandas as pd\nimport seaborn as sns \nimport matplotlib.pyplot as plt\nfrom sklearn import tree\nfrom IPython.display import Image\nimport numpy as np\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\n\ndef confusion_plot(y_data, y_pred):\n\n    cm = confusion_matrix(y_data, y_pred)\n\n    # Calculate metrics\n    accuracy = accuracy_score(y_data, y_pred)\n    negative_recall = recall_score(y_data, y_pred, average= None)\n    negative_precision = precision_score(y_data, y_pred, average= None)\n    positive_recall = recall_score(y_data, y_pred, average= None)\n    positive_precision = precision_score(y_data, y_pred, average= None)\n    \n    # Print metrics\n    print(f\"ACCURACY: {accuracy}\")\n    print(f\"NEGATIVE RECALL (Y=0): {negative_recall}\")\n    print(f\"NEGATIVE PRECISION (Y=0): {negative_precision}\")\n    print(f\"POSITIVE RECALL (Y=1): {positive_recall}\")\n    print(f\"POSITIVE PRECISION (Y=1): {positive_precision}\")\n    print(cm)\n    \n    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n    \n    disp.plot()\n    plt.show()\n\n\n\n\nShow the code\n#RELOAD FILE AND PRETEND THAT IS OUR STARTING POINT \ndf=pd.read_csv('../eda/ev-wiki-crawl-results.csv')  \nprint(df.shape)\n\n#CONVERT FROM STRING LABELS TO INTEGERS \nlabels=[]; #=[]; y2=[]\ny1=[]\nfor label in df[\"label\"]:\n    if label not in labels:\n        labels.append(label)\n        print(\"index =\",len(labels)-1,\": label =\",label)\n    for i in range(0,len(labels)):\n        if(label==labels[i]):\n            y1.append(i)\ny1=np.array(y1)\n\n# CONVERT DF TO LIST OF STRINGS \ncorpus=df[\"text\"].to_list()\ny2=df[\"sentiment\"].to_numpy()\n\nprint(\"number of text chunks = \",len(corpus))\nprint(corpus[0:3])\n\n\n(1225, 3)\nindex = 0 : label = electric vehicle\nindex = 1 : label = gasoline vehicle\nindex = 2 : label = hybrid vehicle\nnumber of text chunks =  1225\n['electric motive power started 1827 hungarian priest nyos jedlik built first crude viable electric motor used stator rotor commutator next year used power small car 1835 professor sibrandus stratingh university groningen netherlands built small scale electric car sometime 1832 1839 robert anderson scotland invented first crude electric carriage powered non rechargeable primary cell american blacksmith inventor thomas davenport built toy electric locomotive powered primitive electric motor 1835 1838 scotsman named robert davidson built electric locomotive attained speed four mile per hour km england patent granted 1840 use rail conductor electric current similar american patent issued lilley colten 1847', 'first mass produced appeared america early 1900s 1902 studebaker automobile company entered automotive business though also entered gasoline vehicle market 1904 however advent cheap assembly line car ford motor company popularity electric car declined significantly due lack electricity grid limitation storage battery time electric car gain much popularity however electric train gained immense popularity due economy achievable speed 20th century electric rail transport became commonplace due advance development electric locomotive time general purpose commercial use reduced specialist role platform truck forklift truck ambulance tow tractor urban delivery vehicle iconic british milk float', '20th century uk world largest user electric road vehicle electrified train used coal transport motor use valuable oxygen mine switzerland lack natural fossil resource forced rapid electrification rail network one earliest rechargeable battery nickel iron battery favored edison use electric car ev among earliest automobile preeminence light powerful internal combustion engine ice electric automobile held many vehicle land speed distance record early 1900s produced baker electric columbia electric detroit electric others one point history outsold gasoline powered vehicle']\n\n\n\n\nShow the code\ndf.head()\n\n\n\n\n\n\n\n\n\n\ntext\nlabel\nsentiment\n\n\n\n\n0\nelectric motive power started 1827 hungarian p...\nelectric vehicle\n-0.7506\n\n\n1\nfirst mass produced appeared america early 190...\nelectric vehicle\n0.9201\n\n\n2\n20th century uk world largest user electric ro...\nelectric vehicle\n0.7096\n\n\n3\n1900 28 percent car road electric ev popular e...\nelectric vehicle\n0.9169\n\n\n4\nseldom marketed woman luxury car may stigma am...\nelectric vehicle\n0.9231"
  },
  {
    "objectID": "5000-website/decision-tree/reference-notebooks/practice.html#vectorize",
    "href": "5000-website/decision-tree/reference-notebooks/practice.html#vectorize",
    "title": "Decision Trees",
    "section": "Vectorize",
    "text": "Vectorize\n\n\nShow the code\n# INITIALIZE COUNT VECTORIZER\n# minDF = 0.01 means \"ignore terms that appear in less than 1% of the documents\". \n# minDF = 5 means \"ignore terms that appear in less than 5 documents\".\nvectorizer=CountVectorizer(min_df=0.0001)   \n\n# RUN COUNT VECTORIZER ON OUR COURPUS \nXs  =  vectorizer.fit_transform(corpus)   \nX=np.array(Xs.todense())\n\n#CONVERT TO ONE-HOT VECTORS\nmaxs=np.max(X,axis=0)\nX=np.ceil(X/maxs)\n\n# DOUBLE CHECK \nprint(X.shape,y1.shape,y2.shape)\n\n\n(1225, 8233) (1225,) (1225,)"
  },
  {
    "objectID": "5000-website/decision-tree/reference-notebooks/practice.html#partition-data",
    "href": "5000-website/decision-tree/reference-notebooks/practice.html#partition-data",
    "title": "Decision Trees",
    "section": "Partition Data",
    "text": "Partition Data\n\n\nShow the code\n# BEFORE SPLIT\nprint(y1[1000:1200])\n\n\n[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n\n\n\n\nShow the code\n#INSERT CODE TO PARTITION DATASET INTO TRAINING-TEST\n\nfrom sklearn.model_selection import train_test_split\ntest_ratio=0.2\nx_train, x_test, y_train, y_test = train_test_split(X, y1, test_size=test_ratio, random_state=0)\ny_train=y_train.flatten()\ny_test=y_test.flatten()\n\nprint(\"x_train.shape        :\",x_train.shape)\nprint(\"y_train.shape        :\",y_train.shape)\n\nprint(\"X_test.shape     :\",x_test.shape)\nprint(\"y_test.shape     :\",y_test.shape)\n\n\nx_train.shape       : (980, 8233)\ny_train.shape       : (980,)\nX_test.shape        : (245, 8233)\ny_test.shape        : (245,)\n\n\n\n\nShow the code\n#CHECK TO MAKE SURE IT WAS RANDOMIZED \nprint(y_train[0:100])\n\n\n[0 1 1 2 1 2 2 1 0 0 1 1 2 0 2 0 1 0 0 2 0 1 1 0 1 1 2 2 1 1 1 1 0 0 2 0 1\n 0 1 1 0 1 2 2 1 2 1 2 1 2 1 2 0 1 1 2 0 2 0 2 0 1 2 2 2 0 1 0 0 0 0 1 2 2\n 1 0 1 1 1 1 0 0 1 0 2 1 0 0 1 2 0 1 1 1 2 1 1 0 0 0]"
  },
  {
    "objectID": "5000-website/decision-tree/reference-notebooks/practice.html#search-1-remove-features-from-high-to-low",
    "href": "5000-website/decision-tree/reference-notebooks/practice.html#search-1-remove-features-from-high-to-low",
    "title": "Decision Trees",
    "section": "Search-1: Remove features from high to low",
    "text": "Search-1: Remove features from high to low\n\n\nShow the code\n#UTILITY FUNCTION TO INITIALIZE RELEVANT ARRAYS\ndef initialize_arrays():\n    global num_features,train_accuracies\n    global test_accuracies,train_time,eval_time\n    num_features=[]\n    train_accuracies=[]\n    test_accuracies=[]\n    train_time=[]\n    eval_time=[]\n\n\n\n\nShow the code\n# INITIALIZE ARRAYS\ninitialize_arrays()\n\n# DEFINE SEARCH FUNCTION\ndef partial_grid_search(num_runs, min_index, max_index):\n    for i in range(1, num_runs+1):\n        # SUBSET FEATURES \n        upper_index=min_index+i*int((max_index-min_index)/num_runs)\n        xtmp=x[:,0:upper_index]\n\n        #TRAIN \n        (acc_train,acc_test,time_train,time_eval)=train_DTC_model(xtmp,y,i_print=False)\n\n        if(i%5==0):\n            print(i,upper_index,xtmp.shape[1],acc_train,acc_test)\n            \n        #RECORD \n        num_features.append(xtmp.shape[1])\n        train_accuracies.append(acc_train)\n        test_accuracies.append(acc_test)\n        train_time.append(time_train)\n        eval_time.append(time_eval)\n\n# DENSE SEARCH (SMALL NUMBER OF FEATURES (FAST))\npartial_grid_search(num_runs=100, min_index=0, max_index=1000)\n\n# SPARSE SEARCH (LARGE NUMBER OF FEATURES (SLOWER))\npartial_grid_search(num_runs=20, min_index=1000, max_index=10000)\n\n\n5 50 50 100.0 82.85714285714286\n10 100 100 100.0 80.40816326530611\n15 150 150 100.0 81.63265306122449\n20 200 200 100.0 82.85714285714286\n25 250 250 100.0 81.22448979591836\n30 300 300 100.0 84.48979591836735\n35 350 350 100.0 81.22448979591836\n40 400 400 100.0 80.0\n45 450 450 100.0 80.40816326530611\n50 500 500 100.0 81.22448979591836\n55 550 550 100.0 84.48979591836735\n60 600 600 100.0 83.6734693877551\n65 650 650 100.0 84.08163265306122\n70 700 700 100.0 86.12244897959184\n75 750 750 100.0 84.48979591836735\n80 800 800 100.0 84.89795918367346\n85 850 850 100.0 85.71428571428571\n90 900 900 100.0 86.53061224489797\n95 950 950 100.0 85.71428571428571\n100 1000 1000 100.0 85.3061224489796\n5 3250 3250 100.0 84.89795918367346\n10 5500 5500 100.0 86.12244897959184\n15 7750 7140 100.0 86.53061224489797\n20 10000 7140 100.0 85.3061224489796\n\n\n\n\nShow the code\n#UTILITY FUNCTION TO SAVE RESULTS\ndef save_results(path_root):\n    out=np.transpose(np.array([num_features,train_accuracies,test_accuracies,train_time,eval_time])) \n    out=pd.DataFrame(out)\n    out.to_csv(path_root+\".csv\")\n\n\n\n\nShow the code\n#UTILITY FUNCTION TO PLOT RESULTS\ndef plot_results(path_root):\n\n    #PLOT-1\n    plt.plot(num_features,train_accuracies,'-or')\n    plt.plot(num_features,test_accuracies,'-ob')\n    plt.xlabel('Number of features')\n    plt.ylabel('ACCURACY: Training (blue) and Test (red)')\n    plt.savefig(path_root+'-1.png')\n    plt.show()\n\n    # #PLOT-2\n    plt.plot(num_features,train_time,'-or')\n    plt.plot(num_features,eval_time,'-ob')\n    plt.xlabel('Number of features')\n    plt.ylabel('Runtime: training time (red) and evaluation time(blue)')\n    plt.savefig(path_root+'-2.png')\n    plt.show()\n\n    # #PLOT-3\n    plt.plot(np.array(test_accuracies),train_time,'-or')\n    plt.plot(np.array(test_accuracies),eval_time,'-ob')\n    plt.xlabel('test_accuracies')\n    plt.ylabel('Runtime: training time (red) and evaluation time (blue)')\n    plt.savefig(path_root+'-3.png')\n    plt.show()\n\n    # #PLOT-3\n    plt.plot(num_features,np.array(train_accuracies)-np.array(test_accuracies),'-or')\n    plt.xlabel('Number of features')\n    plt.ylabel('train_accuracies-test_accuracies')\n    plt.savefig(path_root+'-4.png')\n    plt.show()\n\n\n\n\nShow the code\nsave_results(output_dir+\"/partial_grid_search\")\nplot_results(output_dir+\"/partial_grid_search\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nx_var=np.var(x,axis=0)\nprint(np.min(x_var))\nprint(np.max(x_var))\n\n\n0.0008156601416076161\n0.24855910037484058"
  },
  {
    "objectID": "5000-website/decision-tree/reference-notebooks/practice.html#variance-threshold",
    "href": "5000-website/decision-tree/reference-notebooks/practice.html#variance-threshold",
    "title": "Decision Trees",
    "section": "Variance Threshold",
    "text": "Variance Threshold\n\n\nShow the code\nfrom sklearn.feature_selection import VarianceThreshold\n\n# DEFINE GRID OF THRESHOLDS \nnum_thresholds=30\nthresholds=np.linspace(np.min(x_var),np.max(x_var),num_thresholds)\n\n#DOESN\"T WORK WELL WITH EDGE VALUES \nthresholds=thresholds[1:-2]; #print(thresholds)\n\n# INITIALIZE ARRAYS\ninitialize_arrays()\n\n# SEARCH FOR OPTIMAL THRESHOLD\nfor THRESHOLD in thresholds:\n    feature_selector = VarianceThreshold(threshold=THRESHOLD)\n    xtmp=feature_selector.fit_transform(x)\n    print(\"THRESHOLD =\",THRESHOLD, xtmp.shape[1])\n\n    (acc_train,acc_test,time_train,time_eval)=train_DTC_model(xtmp,y,i_print=False)\n             \n    #RECORD \n    num_features.append(xtmp.shape[1])\n    train_accuracies.append(acc_train)\n    test_accuracies.append(acc_test)\n    train_time.append(time_train)\n    eval_time.append(time_eval)\n\n\nTHRESHOLD = 0.009358537391029442 1088\nTHRESHOLD = 0.01790141464045127 609\nTHRESHOLD = 0.026444291889873094 414\nTHRESHOLD = 0.03498716913929492 278\nTHRESHOLD = 0.04353004638871675 211\nTHRESHOLD = 0.05207292363813857 155\nTHRESHOLD = 0.0606158008875604 127\nTHRESHOLD = 0.06915867813698222 95\nTHRESHOLD = 0.07770155538640404 73\nTHRESHOLD = 0.08624443263582587 61\nTHRESHOLD = 0.0947873098852477 52\nTHRESHOLD = 0.10333018713466952 43\nTHRESHOLD = 0.11187306438409135 36\nTHRESHOLD = 0.12041594163351317 32\nTHRESHOLD = 0.128958818882935 28\nTHRESHOLD = 0.13750169613235683 24\nTHRESHOLD = 0.14604457338177865 21\nTHRESHOLD = 0.15458745063120047 17\nTHRESHOLD = 0.16313032788062232 13\nTHRESHOLD = 0.17167320513004414 13\nTHRESHOLD = 0.18021608237946596 11\nTHRESHOLD = 0.18875895962888778 11\nTHRESHOLD = 0.1973018368783096 8\nTHRESHOLD = 0.20584471412773142 8\nTHRESHOLD = 0.21438759137715327 7\nTHRESHOLD = 0.2229304686265751 6\nTHRESHOLD = 0.2314733458759969 6\n\n\n\n\nShow the code\n# CHECK RESULTS \nsave_results(output_dir+\"/variance_threshold\")\nplot_results(output_dir+\"/variance_threshold\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\n# # INSERT CODE TO USE THE MODEL TO MAKE PREDICTIONS FOR THE TRAINING AND TEST SET \n# yp_train = model.predict(x_train)\n# yp_test = model.predict(x_test)"
  },
  {
    "objectID": "5000-website/decision-tree/reference-notebooks/practice.html#random-forest-classifier-final-results",
    "href": "5000-website/decision-tree/reference-notebooks/practice.html#random-forest-classifier-final-results",
    "title": "Decision Trees",
    "section": "Random Forest Classifier Final results",
    "text": "Random Forest Classifier Final results\n\n\nShow the code\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Create the random forest model with a specific number of trees (n_estimators)\nmodel = RandomForestClassifier(n_estimators=100, max_depth=200)\n\n# Fit the model to your training data\nmodel = model.fit(x_train, y_train)\n\n# Predict on training and test data\nyp_train = model.predict(x_train)\nyp_test = model.predict(x_test)\n\n\n\n\nShow the code\n# RUN THE FOLLOWING CODE TO TEST YOUR FUNCTION \nprint(\"------TRAINING------\")\nconfusion_plot(y_train,yp_train)\nprint(\"------TEST------\")\nconfusion_plot(y_test,yp_test)\n\n\n------TRAINING------\nACCURACY: 0.7918367346938775\nNEGATIVE RECALL (Y=0): [0.8559322  0.8470255  0.63736264]\nNEGATIVE PRECISION (Y=0): [0.78092784 0.81917808 0.76651982]\nPOSITIVE RECALL (Y=1): [0.8559322  0.8470255  0.63736264]\nPOSITIVE PRECISION (Y=1): [0.78092784 0.81917808 0.76651982]\n[[303  26  25]\n [ 26 299  28]\n [ 59  40 174]]\n------TEST------\nACCURACY: 0.46122448979591835\nNEGATIVE RECALL (Y=0): [0.50666667 0.64516129 0.19480519]\nNEGATIVE PRECISION (Y=0): [0.38383838 0.68181818 0.25862069]\nPOSITIVE RECALL (Y=1): [0.50666667 0.64516129 0.19480519]\nPOSITIVE PRECISION (Y=1): [0.38383838 0.68181818 0.25862069]\n[[38  8 29]\n [19 60 14]\n [42 20 15]]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nplt.plot(max_depth,train_accuracy ,linewidth=2, color='k')\nplt.plot(max_depth,train_accuracy ,linewidth=2, color='k')\n,test_accuracy ,linewidth=2, color='b')\n\nplt.xlabel(\"Number of neighbors in KNN\")\nplt.ylabel(\"Training (black) and test (blue) accuracy\")\n\n\nNameError: name 'max_depth' is not defined"
  },
  {
    "objectID": "5000-website/decision-tree/reference-notebooks/classification.html",
    "href": "5000-website/decision-tree/reference-notebooks/classification.html",
    "title": "Classification",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport numpy as np\nfrom sklearn.metrics import accuracy_score"
  },
  {
    "objectID": "5000-website/decision-tree/reference-notebooks/classification.html#import",
    "href": "5000-website/decision-tree/reference-notebooks/classification.html#import",
    "title": "Classification",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport numpy as np\nfrom sklearn.metrics import accuracy_score"
  },
  {
    "objectID": "5000-website/decision-tree/reference-notebooks/classification.html#read-and-re-format",
    "href": "5000-website/decision-tree/reference-notebooks/classification.html#read-and-re-format",
    "title": "Classification",
    "section": "Read and re-format",
    "text": "Read and re-format\n\n#RELOAD FILE AND PRETEND THAT IS OUR STARTING POINT \ndf=pd.read_csv('../eda/ev-wiki-crawl-results.csv')  \nprint(df.shape)\ndf.head()\n\n(1225, 3)\n\n\n\n\n\n\n\n\n\n\ntext\nlabel\nsentiment\n\n\n\n\n0\nelectric motive power started 1827 hungarian p...\nelectric vehicle\n-0.7506\n\n\n1\nfirst mass produced appeared america early 190...\nelectric vehicle\n0.9201\n\n\n2\n20th century uk world largest user electric ro...\nelectric vehicle\n0.7096\n\n\n3\n1900 28 percent car road electric ev popular e...\nelectric vehicle\n0.9169\n\n\n4\nseldom marketed woman luxury car may stigma am...\nelectric vehicle\n0.9231\n\n\n\n\n\n\n\n\n\n#CONVERT FROM STRING LABELS TO INTEGERS \nlabels=[]; #y1=[]; y2=[]\ny1=[]\nfor label in df[\"label\"]:\n    if label not in labels:\n        labels.append(label)\n        print(\"index =\",len(labels)-1,\": label =\",label)\n    for i in range(0,len(labels)):\n        if(label==labels[i]):\n            y1.append(i)\ny1=np.array(y1)\n\n# CONVERT DF TO LIST OF STRINGS \ncorpus=df[\"text\"].to_list()\ny2=df[\"sentiment\"].to_numpy()\n\nprint(\"number of text chunks = \",len(corpus))\nprint(corpus[0:3])\n\nindex = 0 : label = electric vehicle\nindex = 1 : label = gasoline vehicle\nindex = 2 : label = hybrid vehicle\nnumber of text chunks =  1225\n['electric motive power started 1827 hungarian priest nyos jedlik built first crude viable electric motor used stator rotor commutator next year used power small car 1835 professor sibrandus stratingh university groningen netherlands built small scale electric car sometime 1832 1839 robert anderson scotland invented first crude electric carriage powered non rechargeable primary cell american blacksmith inventor thomas davenport built toy electric locomotive powered primitive electric motor 1835 1838 scotsman named robert davidson built electric locomotive attained speed four mile per hour km england patent granted 1840 use rail conductor electric current similar american patent issued lilley colten 1847', 'first mass produced appeared america early 1900s 1902 studebaker automobile company entered automotive business though also entered gasoline vehicle market 1904 however advent cheap assembly line car ford motor company popularity electric car declined significantly due lack electricity grid limitation storage battery time electric car gain much popularity however electric train gained immense popularity due economy achievable speed 20th century electric rail transport became commonplace due advance development electric locomotive time general purpose commercial use reduced specialist role platform truck forklift truck ambulance tow tractor urban delivery vehicle iconic british milk float', '20th century uk world largest user electric road vehicle electrified train used coal transport motor use valuable oxygen mine switzerland lack natural fossil resource forced rapid electrification rail network one earliest rechargeable battery nickel iron battery favored edison use electric car ev among earliest automobile preeminence light powerful internal combustion engine ice electric automobile held many vehicle land speed distance record early 1900s produced baker electric columbia electric detroit electric others one point history outsold gasoline powered vehicle']"
  },
  {
    "objectID": "5000-website/decision-tree/reference-notebooks/classification.html#vectorize-the-text-data",
    "href": "5000-website/decision-tree/reference-notebooks/classification.html#vectorize-the-text-data",
    "title": "Classification",
    "section": "Vectorize the text data",
    "text": "Vectorize the text data\n\n# INITIALIZE COUNT VECTORIZER\n# minDF = 0.01 means \"ignore terms that appear in less than 1% of the documents\". \n# minDF = 5 means \"ignore terms that appear in less than 5 documents\".\nvectorizer=CountVectorizer(min_df=0.0001)   \n\n# RUN COUNT VECTORIZER ON OUR COURPUS \nXs  =  vectorizer.fit_transform(corpus)   \nX=np.array(Xs.todense())\n\n#CONVERT TO ONE-HOT VECTORS\nmaxs=np.max(X,axis=0)\nX=np.ceil(X/maxs)\n\n# DOUBLE CHECK \nprint(X.shape,y1.shape,y2.shape)\nprint(\"DATA POINT-0:\",X[0,0:10],\"y1 =\",y1[0],\"  y2 =\",y2[0])\n\n(1225, 8233) (1225,) (1225,)\nDATA POINT-0: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] y1 = 0   y2 = -0.7506"
  },
  {
    "objectID": "5000-website/decision-tree/reference-notebooks/classification.html#partition-data",
    "href": "5000-website/decision-tree/reference-notebooks/classification.html#partition-data",
    "title": "Classification",
    "section": "Partition Data",
    "text": "Partition Data\n\n# INSERT CODE, AS A CONSISTENCY CHECK, TO PRINT THE TYPE AND SHAPE OF x_train, x_test, y_train, y_test\n# Changed the output structure for better understanding\n\nprint(\"TRAINING SHAPES: \\n\", f\"x_train:\\n shape: {x_train.shape}\\n type: {type(x_train)}\\n\", f\"y_train:\\n shape: {y_train.shape}\\n type: {type(y_train)}\\n\")\nprint(\"TEST SHAPES: \\n\", f\"x_test:\\n shape: {x_test.shape}\\n type: {type(x_test)}\\n\", f\"y_test:\\n shape: {y_test.shape}\\n type: {type(y_test)}\\n\")\n\nTRAINING SHAPES: \n x_train:\n shape: (980, 8233)\n type: &lt;class 'numpy.ndarray'&gt;\n y_train:\n shape: (980,)\n type: &lt;class 'numpy.ndarray'&gt;\n\nTEST SHAPES: \n x_test:\n shape: (245, 8233)\n type: &lt;class 'numpy.ndarray'&gt;\n y_test:\n shape: (245,)\n type: &lt;class 'numpy.ndarray'&gt;"
  },
  {
    "objectID": "5000-website/decision-tree/reference-notebooks/classification.html#class-distribution",
    "href": "5000-website/decision-tree/reference-notebooks/classification.html#class-distribution",
    "title": "Classification",
    "section": "Class distribution",
    "text": "Class distribution\n\nimport numpy as np\n\n# Labels for the classes\nlabels = {0: 'electric vehicle', 1: 'gasoline vehicle', 2: 'hybrid vehicle'}\n\n# y_train is a numpy array\nunique_classes, counts = np.unique(y_train, return_counts=True)\n\n# Get the proportions\nproportions = counts / len(y_train)\n\n# Print the results with labels\nfor unique_class, count in zip(unique_classes, counts):\n    label = labels[unique_class]\n    print(f\"Number of points with target={label}: {count} ({count/len(y_train):.2f})\")\n\n# Retrieve the dominant class\ndominant_class_index = np.argmax(counts)\ndominant_class_label = labels[unique_classes[dominant_class_index]]\nprint(\"The dominant class is:\", dominant_class_label)\n\nNumber of points with target=electric vehicle: 354 (0.36)\nNumber of points with target=gasoline vehicle: 353 (0.36)\nNumber of points with target=hybrid vehicle: 273 (0.28)\nThe dominant class is: electric vehicle"
  },
  {
    "objectID": "5000-website/decision-tree/reference-notebooks/classification.html#baseline-model-for-comparison",
    "href": "5000-website/decision-tree/reference-notebooks/classification.html#baseline-model-for-comparison",
    "title": "Classification",
    "section": "Baseline model for comparison",
    "text": "Baseline model for comparison\n\nimport numpy as np\nimport random\nfrom collections import Counter\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_recall_fscore_support\n\n\nfrom collections import Counter\nimport numpy as np\nimport random\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\n\ndef generate_label_data(class_labels, weights, N=10000):\n    # Generate random labels based on the given class weights\n    y = random.choices(class_labels, weights=weights, k=N)\n    print(\"-----GENERATING DATA-----\")\n    print(\"unique entries:\", Counter(y).keys())\n    print(\"count of labels:\", Counter(y).values())\n    print(\"probability of labels:\", np.array(list(Counter(y).values())) / N)\n    return y\n\ndef random_classifier(y_data, class_labels, weights):\n    # Generate random predictions based on the distribution of class labels\n    ypred = random.choices(class_labels, weights=weights, k=len(y_data))\n    \n    print(\"-----RANDOM CLASSIFIER-----\")\n    print(\"count of prediction:\", Counter(ypred).values())\n    print(\"probability of prediction:\", np.array(list(Counter(ypred).values())) / len(y_data))\n    print(\"accuracy\", accuracy_score(y_data, ypred))\n    precision, recall, fscore, _ = precision_recall_fscore_support(y_data, ypred, average=None, labels=class_labels)\n    print(\"precision, recall, fscore:\", list(zip(precision, recall, fscore)))\n\n\n# Example usage:\nclass_labels = ['electric vehicle', 'gasoline vehicle', 'hybrid vehicle']\nweights = [0.36, 0.36, 0.28]\n\nprint(\"\\nMULTI-CLASS: NON-UNIFORM LOAD\")\ny = generate_label_data(class_labels, weights, 10000)\nrandom_classifier(y, class_labels, weights)\n\n\nMULTI-CLASS: NON-UNIFORM LOAD\n-----GENERATING DATA-----\nunique entries: dict_keys(['gasoline vehicle', 'electric vehicle', 'hybrid vehicle'])\ncount of labels: dict_values([3595, 3640, 2765])\nprobability of labels: [0.3595 0.364  0.2765]\n-----RANDOM CLASSIFIER-----\ncount of prediction: dict_values([2801, 3555, 3644])\nprobability of prediction: [0.2801 0.3555 0.3644]\naccuracy 0.3401\nprecision, recall, fscore: [(0.37019758507135014, 0.3706043956043956, 0.3704008786381109), (0.3592123769338959, 0.3552155771905424, 0.3572027972027972), (0.2766868975365941, 0.28028933092224234, 0.2784764642472153)]\n\n\n\n#INSERT CODE TO WRITE A FUNCTION def confusion_plot(y_data,y_pred) WHICH GENERATES A CONFUSION MATRIX PLOT AND PRINTS THE INFORMATION ABOVE (see link above for example)\n\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_classification\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\n\ndef confusion_plot(y_data, y_pred):\n\n    cm = confusion_matrix(y_data, y_pred)\n\n    # Calculate metrics\n    accuracy = accuracy_score(y_data, y_pred)\n    negative_recall = recall_score(y_data, y_pred, average=None)\n    negative_precision = precision_score(y_data, y_pred, average=None)\n    positive_recall = recall_score(y_data, y_pred, average=None)\n    positive_precision = precision_score(y_data, y_pred, average=None)\n    \n    # Print metrics\n    print(f\"ACCURACY: {accuracy}\")\n    print(f\"NEGATIVE RECALL (Y=0): {negative_recall}\")\n    print(f\"NEGATIVE PRECISION (Y=0): {negative_precision}\")\n    print(f\"POSITIVE RECALL (Y=1): {positive_recall}\")\n    print(f\"POSITIVE PRECISION (Y=1): {positive_precision}\")\n    print(cm)\n    \n    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n    \n    disp.plot()\n    plt.show()\n\n\nlabels = [\"electric vehicle\", \"gasoline vehicle\", \"hybrid vehicle\"]\n\n# RUN THE FOLLOWING CODE TO TEST YOUR FUNCTION \nprint(\"------TRAINING------\")\nconfusion_plot(y_train,yp_train)\nprint(\"------TEST------\")\nconfusion_plot(y_test,yp_test)\n\n------TRAINING------\nACCURACY: 0.5479591836734694\nNEGATIVE RECALL (Y=0): [0.64124294 0.67422096 0.26373626]\nNEGATIVE PRECISION (Y=0): [0.50782998 0.65927978 0.41860465]\nPOSITIVE RECALL (Y=1): [0.64124294 0.67422096 0.26373626]\nPOSITIVE PRECISION (Y=1): [0.50782998 0.65927978 0.41860465]\n[[227  70  57]\n [ 72 238  43]\n [148  53  72]]\n------TEST------\nACCURACY: 0.5142857142857142\nNEGATIVE RECALL (Y=0): [0.62666667 0.67741935 0.20779221]\nNEGATIVE PRECISION (Y=0): [0.40517241 0.68478261 0.43243243]\nPOSITIVE RECALL (Y=1): [0.62666667 0.67741935 0.20779221]\nPOSITIVE PRECISION (Y=1): [0.40517241 0.68478261 0.43243243]\n[[47 15 13]\n [22 63  8]\n [47 14 16]]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\nimport time\n\ndef train_DTC_model(X,Y,i_print=False):\n\n    if(i_print):\n        print(X.shape,Y.shape)\n\n    # SPLIT\n    x_train = X[train_index]\n    y_train = Y[train_index].flatten()\n\n    x_test = X[test_index]\n    y_test = Y[test_index].flatten()\n\n    # INITIALIZE MODEL \n    model = DecisionTreeClassifier()\n\n    # TRAIN MODEL \n    start = time.process_time()\n    model.fit(x_train, y_train)\n    time_train = time.process_time() - start\n\n    # LABEL PREDICTIONS FOR TRAINING AND TEST SET \n    start = time.process_time()\n    yp_train = model.predict(x_train)\n    yp_test = model.predict(x_test)\n    time_eval = time.process_time() - start\n\n    acc_train = accuracy_score(y_train, yp_train) * 100\n    acc_test = accuracy_score(y_test, yp_test) * 100\n\n    if(i_print):\n        print(acc_train, acc_test, time_train, time_eval)\n\n    return (acc_train, acc_test, time_train, time_eval)\n\n# TEST\nprint(type(x), type(y))\nprint(x.shape, y.shape)\n(acc_train, acc_test, time_train, time_eval) = train_DTC_model(x, y, i_print=True)\n\nNameError: name 'x' is not defined\n\n\n\n#UTILITY FUNCTION TO INITIALIZE RELEVANT ARRAYS\ndef initialize_arrays():\n    global num_features,train_accuracies\n    global test_accuracies,train_time,eval_time\n    num_features=[]\n    train_accuracies=[]\n    test_accuracies=[]\n    train_time=[]\n    eval_time=[]\n\n\n# INITIALIZE ARRAYS\ninitialize_arrays()\n\n# DEFINE SEARCH FUNCTION\ndef partial_grid_search(num_runs, min_index, max_index):\n    for i in range(1, num_runs+1):\n        # SUBSET FEATURES \n        upper_index=min_index+i*int((max_index-min_index)/num_runs)\n        xtmp=x[:,0:upper_index]\n\n        #TRAIN \n        (acc_train,acc_test,time_train,time_eval)=train_DTC_model(xtmp,y,i_print=False)\n\n        if(i%5==0):\n            print(i,upper_index,xtmp.shape[1],acc_train,acc_test)\n            \n        #RECORD \n        num_features.append(xtmp.shape[1])\n        train_accuracies.append(acc_train)\n        test_accuracies.append(acc_test)\n        train_time.append(time_train)\n        eval_time.append(time_eval)\n\n# DENSE SEARCH (SMALL NUMBER OF FEATURES (FAST))\npartial_grid_search(num_runs=100, min_index=0, max_index=1000)\n\n# SPARSE SEARCH (LARGE NUMBER OF FEATURES (SLOWER))\npartial_grid_search(num_runs=20, min_index=1000, max_index=10000)\n\n------TRAINING------\nACCURACY: 0.5479591836734694\nNEGATIVE RECALL (Y=0): [0.64124294 0.67422096 0.26373626]\nNEGATIVE PRECISION (Y=0): [0.50782998 0.65927978 0.41860465]\nPOSITIVE RECALL (Y=1): [0.64124294 0.67422096 0.26373626]\nPOSITIVE PRECISION (Y=1): [0.50782998 0.65927978 0.41860465]\n[[227  70  57]\n [ 72 238  43]\n [148  53  72]]\n------TEST------\nACCURACY: 0.5142857142857142\nNEGATIVE RECALL (Y=0): [0.62666667 0.67741935 0.20779221]\nNEGATIVE PRECISION (Y=0): [0.40517241 0.68478261 0.43243243]\nPOSITIVE RECALL (Y=1): [0.62666667 0.67741935 0.20779221]\nPOSITIVE PRECISION (Y=1): [0.40517241 0.68478261 0.43243243]\n[[47 15 13]\n [22 63  8]\n [47 14 16]]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAttributeError: 'numpy.ndarray' object has no attribute 'columns'\n\n\n&lt;Figure size 2000x1000 with 0 Axes&gt;\n\n\n\n#UTILITY FUNCTION TO SAVE RESULTS\ndef save_results(path_root):\n    out=np.transpose(np.array([num_features,train_accuracies,test_accuracies,train_time,eval_time])) \n    out=pd.DataFrame(out)\n    out.to_csv(path_root+\".csv\")\n\n(250, 250)\n[[0.         0.06661734 0.0877058  ... 0.05510388 0.07161149 0.14634235]\n [0.06661734 0.         0.         ... 0.04241898 0.10336228 0.02414023]\n [0.0877058  0.         0.         ... 0.10471348 0.06123724 0.09534626]\n ...\n [0.05510388 0.04241898 0.10471348 ... 0.         0.0854982  0.07987231]\n [0.07161149 0.10336228 0.06123724 ... 0.0854982  0.         0.09731237]\n [0.14634235 0.02414023 0.09534626 ... 0.07987231 0.09731237 0.        ]]\n\n\n\n\n\n\n\n\n\n\n#UTILITY FUNCTION TO PLOT RESULTS\ndef plot_results(path_root):\n\n    #PLOT-1\n    plt.plot(num_features,train_accuracies,'-or')\n    plt.plot(num_features,test_accuracies,'-ob')\n    plt.xlabel('Number of features')\n    plt.ylabel('ACCURACY: Training (blue) and Test (red)')\n    plt.savefig(path_root+'-1.png')\n    plt.show()\n\n    # #PLOT-2\n    plt.plot(num_features,train_time,'-or')\n    plt.plot(num_features,eval_time,'-ob')\n    plt.xlabel('Number of features')\n    plt.ylabel('Runtime: training time (red) and evaluation time(blue)')\n    plt.savefig(path_root+'-2.png')\n    plt.show()\n\n    # #PLOT-3\n    plt.plot(np.array(test_accuracies),train_time,'-or')\n    plt.plot(np.array(test_accuracies),eval_time,'-ob')\n    plt.xlabel('test_accuracies')\n    plt.ylabel('Runtime: training time (red) and evaluation time (blue)')\n    plt.savefig(path_root+'-3.png')\n    plt.show()\n\n    # #PLOT-3\n    plt.plot(num_features,np.array(train_accuracies)-np.array(test_accuracies),'-or')\n    plt.xlabel('Number of features')\n    plt.ylabel('train_accuracies-test_accuracies')\n    plt.savefig(path_root+'-4.png')\n    plt.show()\n\n[0.02668296 0.01480364 0.01259585 0.01007728 0.00935501 0.00881098\n 0.00829497 0.0077585  0.0067137  0.00627816]\n[43.67336893 32.5299975  30.00636873 26.83930993 25.85960328 25.09641805\n 24.35045766 23.54987992 21.90688661 21.184393  ]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n/Users/isfarbaset/anaconda3/lib/python3.11/site-packages/seaborn/axisgrid.py:118: UserWarning: The figure layout has changed to tight\n  self._figure.tight_layout(*args, **kwargs)\n\n\n\n\n\n\n\n\n\n\nsave_results(output_dir+\"/partial_grid_search\")\nplot_results(output_dir+\"/partial_grid_search\")\n\n\nx_var=np.var(x,axis=0)\nprint(np.min(x_var))\nprint(np.max(x_var))\n\nKeyError: \"['target_column'] not found in axis\"\n\n\n\nfrom sklearn.feature_selection import VarianceThreshold\n\n# DEFINE GRID OF THRESHOLDS \nnum_thresholds=30\nthresholds=np.linspace(np.min(x_var),np.max(x_var),num_thresholds)\n\n#DOESN\"T WORK WELL WITH EDGE VALUES \nthresholds=thresholds[1:-2]; #print(thresholds)\n\n# INITIALIZE ARRAYS\ninitialize_arrays()\n\n# SEARCH FOR OPTIMAL THRESHOLD\nfor THRESHOLD in thresholds:\n    feature_selector = VarianceThreshold(threshold=THRESHOLD)\n    xtmp=feature_selector.fit_transform(x)\n    print(\"THRESHOLD =\",THRESHOLD, xtmp.shape[1])\n\n    (acc_train,acc_test,time_train,time_eval)=train_DTC_model(xtmp,y,i_print=False)\n             \n    #RECORD \n    num_features.append(xtmp.shape[1])\n    train_accuracies.append(acc_train)\n    test_accuracies.append(acc_test)\n    train_time.append(time_train)\n    eval_time.append(time_eval)\n\n\n# CHECK RESULTS \nsave_results(output_dir+\"/variance_threshold\")\nplot_results(output_dir+\"/variance_threshold\")\n\n\n#### COMPLETE THE CODE BELOW TO TRAIN A SKLEARN DECISION TREE MODEL ON x_train,y_train \nfrom sklearn import tree\nmodel = tree.DecisionTreeClassifier(max_depth=200)\nmodel = model.fit(x_train, y_train)\n\nyp_train=model.predict(x_train)\nyp_test=model.predict(x_test)\n\n------TRAINING------\nACCURACY: 0.7918367346938775\nNEGATIVE RECALL (Y=0): [1.         0.82152975 0.48351648]\nNEGATIVE PRECISION (Y=0): [0.67946257 0.88685015 1.        ]\nPOSITIVE RECALL (Y=1): [1.         0.82152975 0.48351648]\nPOSITIVE PRECISION (Y=1): [0.67946257 0.88685015 1.        ]\n[[354   0   0]\n [ 63 290   0]\n [104  37 132]]\n------TEST------\nACCURACY: 0.40816326530612246\nNEGATIVE RECALL (Y=0): [0.52       0.56989247 0.1038961 ]\nNEGATIVE PRECISION (Y=0): [0.33913043 0.59550562 0.19512195]\nPOSITIVE RECALL (Y=1): [0.52       0.56989247 0.1038961 ]\nPOSITIVE PRECISION (Y=1): [0.33913043 0.59550562 0.19512195]\n[[39 19 17]\n [24 53 16]\n [52 17  8]]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# RUN THE FOLLOWING CODE TO TEST YOUR FUNCTION \nprint(\"------TRAINING------\")\nconfusion_plot(y_train,yp_train)\nprint(\"------TEST------\")\nconfusion_plot(y_test,yp_test)"
  },
  {
    "objectID": "5000-website/decision-tree/reference-notebooks/regression.html",
    "href": "5000-website/decision-tree/reference-notebooks/regression.html",
    "title": "Regression",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport numpy as np\nfrom sklearn.metrics import accuracy_score"
  },
  {
    "objectID": "5000-website/decision-tree/reference-notebooks/regression.html#import",
    "href": "5000-website/decision-tree/reference-notebooks/regression.html#import",
    "title": "Regression",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport numpy as np\nfrom sklearn.metrics import accuracy_score"
  },
  {
    "objectID": "5000-website/decision-tree/reference-notebooks/regression.html#read-and-re-format",
    "href": "5000-website/decision-tree/reference-notebooks/regression.html#read-and-re-format",
    "title": "Regression",
    "section": "Read and re-format",
    "text": "Read and re-format\n\n#RELOAD FILE AND PRETEND THAT IS OUR STARTING POINT \ndf=pd.read_csv('../eda/ev-wiki-crawl-results.csv')  \nprint(df.shape)\n\n#CONVERT FROM STRING LABELS TO INTEGERS \nlabels=[]; #y1=[]; y2=[]\ny1=[]\nfor label in df[\"label\"]:\n    if label not in labels:\n        labels.append(label)\n        print(\"index =\",len(labels)-1,\": label =\",label)\n    for i in range(0,len(labels)):\n        if(label==labels[i]):\n            y1.append(i)\ny1=np.array(y1)\n\n# CONVERT DF TO LIST OF STRINGS \ncorpus=df[\"text\"].to_list()\ny2=df[\"sentiment\"].to_numpy()\n\nprint(\"number of text chunks = \",len(corpus))\nprint(corpus[0:3])\n\n(1225, 3)\nindex = 0 : label = electric vehicle\nindex = 1 : label = gasoline vehicle\nindex = 2 : label = hybrid vehicle\nnumber of text chunks =  1225\n['electric motive power started 1827 hungarian priest nyos jedlik built first crude viable electric motor used stator rotor commutator next year used power small car 1835 professor sibrandus stratingh university groningen netherlands built small scale electric car sometime 1832 1839 robert anderson scotland invented first crude electric carriage powered non rechargeable primary cell american blacksmith inventor thomas davenport built toy electric locomotive powered primitive electric motor 1835 1838 scotsman named robert davidson built electric locomotive attained speed four mile per hour km england patent granted 1840 use rail conductor electric current similar american patent issued lilley colten 1847', 'first mass produced appeared america early 1900s 1902 studebaker automobile company entered automotive business though also entered gasoline vehicle market 1904 however advent cheap assembly line car ford motor company popularity electric car declined significantly due lack electricity grid limitation storage battery time electric car gain much popularity however electric train gained immense popularity due economy achievable speed 20th century electric rail transport became commonplace due advance development electric locomotive time general purpose commercial use reduced specialist role platform truck forklift truck ambulance tow tractor urban delivery vehicle iconic british milk float', '20th century uk world largest user electric road vehicle electrified train used coal transport motor use valuable oxygen mine switzerland lack natural fossil resource forced rapid electrification rail network one earliest rechargeable battery nickel iron battery favored edison use electric car ev among earliest automobile preeminence light powerful internal combustion engine ice electric automobile held many vehicle land speed distance record early 1900s produced baker electric columbia electric detroit electric others one point history outsold gasoline powered vehicle']"
  },
  {
    "objectID": "5000-website/decision-tree/reference-notebooks/regression.html#vectorize-the-text-data",
    "href": "5000-website/decision-tree/reference-notebooks/regression.html#vectorize-the-text-data",
    "title": "Regression",
    "section": "Vectorize the text data",
    "text": "Vectorize the text data\n\n# INITIALIZE COUNT VECTORIZER\n# minDF = 0.01 means \"ignore terms that appear in less than 1% of the documents\". \n# minDF = 5 means \"ignore terms that appear in less than 5 documents\".\nvectorizer=CountVectorizer(min_df=0.0001)   \n\n# RUN COUNT VECTORIZER ON OUR COURPUS \nXs  =  vectorizer.fit_transform(corpus)   \nX=np.array(Xs.todense())\n\n#CONVERT TO ONE-HOT VECTORS\nmaxs=np.max(X,axis=0)\nX=np.ceil(X/maxs)\n\n# DOUBLE CHECK \nprint(X.shape,y1.shape,y2.shape)\nprint(\"DATA POINT-0:\",X[0,0:10],\"y1 =\",y1[0],\"  y2 =\",y2[0])\n\n(1225, 8233) (1225,) (1225,)\nDATA POINT-0: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] y1 = 0   y2 = -0.7506"
  },
  {
    "objectID": "5000-website/decision-tree/reference-notebooks/regression.html#partition-data",
    "href": "5000-website/decision-tree/reference-notebooks/regression.html#partition-data",
    "title": "Regression",
    "section": "Partition Data",
    "text": "Partition Data\nAssignment 3.2.4: Break data into an 80-20 training/test set\n\n#INSERT CODE TO PARTITION DATASET INTO TRAINING-TEST\n\nfrom sklearn.model_selection import train_test_split\ntest_ratio=0.2\nx_train, x_test, y_train, y_test = train_test_split(X, y1, test_size=test_ratio, random_state=0)\ny_train=y_train.flatten()\ny_test=y_test.flatten()\n\nprint(\"x_train.shape        :\",x_train.shape)\nprint(\"y_train.shape        :\",y_train.shape)\n\nprint(\"X_test.shape     :\",x_test.shape)\nprint(\"y_test.shape     :\",y_test.shape)\n\nx_train.shape       : (980, 8233)\ny_train.shape       : (980,)\nX_test.shape        : (245, 8233)\ny_test.shape        : (245,)"
  },
  {
    "objectID": "5000-website/decision-tree/reference-notebooks/regression.html#utility-function",
    "href": "5000-website/decision-tree/reference-notebooks/regression.html#utility-function",
    "title": "Regression",
    "section": "Utility function",
    "text": "Utility function\n\nWrite a function to report accuracy\nNote this will act on object stored in pythons global scope. Therefore as long as everything is named the same you can recycle it for multiple models\n\n\ndef report(y,ypred):\n      #ACCURACY COMPUTE \n      print(\"Accuracy:\",accuracy_score(y, ypred)*100)\n      print(\"Number of mislabeled points out of a total %d points = %d\"\n            % (y.shape[0], (y != ypred).sum()))\n\ndef print_model_summary():\n      # LABEL PREDICTIONS FOR TRAINING AND TEST SET \n      yp_train = model.predict(x_train)\n      yp_test = model.predict(x_test)\n\n      print(\"ACCURACY CALCULATION\\n\")\n\n      print(\"TRAINING SET:\")\n      report(y_train,yp_train)\n\n      print(\"\\nTEST SET (UNTRAINED DATA):\")\n      report(y_test,yp_test)\n\n      print(\"\\nCHECK FIRST 20 PREDICTIONS\")\n      print(\"TRAINING SET:\")\n      print(y_train[0:20])\n      print(yp_train[0:20])\n      print(\"ERRORS:\",yp_train[0:20]-y_train[0:20])\n\n      print(\"\\nTEST SET (UNTRAINED DATA):\")\n      print(y_test[0:20])\n      print(yp_test[0:20])\n      print(\"ERRORS:\",yp_test[0:20]-y_test[0:20])"
  },
  {
    "objectID": "5000-website/decision-tree/reference-notebooks/regression.html#regression-model-knn",
    "href": "5000-website/decision-tree/reference-notebooks/regression.html#regression-model-knn",
    "title": "Regression",
    "section": "Regression model: KNN",
    "text": "Regression model: KNN\n\nfit y2 (sentiment score from -1 to 1) instead of y1 (classes)"
  },
  {
    "objectID": "5000-website/decision-tree/reference-notebooks/regression.html#hyper-parameter-tuning",
    "href": "5000-website/decision-tree/reference-notebooks/regression.html#hyper-parameter-tuning",
    "title": "Regression",
    "section": "Hyper-Parameter tuning",
    "text": "Hyper-Parameter tuning\n\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.metrics import mean_absolute_percentage_error\nfrom sklearn.metrics import mean_absolute_error\n\n# HYPER PARAMETER SEARCH FOR OPTIMAL NUMBER OF NEIGHBORS \nnum_neighbors_2=[]\ntrain_error=[]\ntest_error=[]\n\n# LOOP OVER HYPER-PARAM\nfor i in range(1,33):\n    # INITIALIZE MODEL \n    model = KNeighborsRegressor(n_neighbors=i)\n\n    # TRAIN MODEL \n    model.fit(x_train,y_train)\n\n    # OUTPUT PREDICTIONS FOR TRAINING AND TEST SET \n    yp_train = model.predict(x_train)\n    yp_test = model.predict(x_test)\n\n    print(\"n_neighbors =\",i)\n    # shift=1+np.min(y_train) #add shift to remove division by zero \n    err1=mean_absolute_error(y_train, yp_train) \n    err2=mean_absolute_error(y_test, yp_test) \n\n    num_neighbors_2.append(i)\n    train_error.append(err1)\n    test_error.append(err2)\n\n    print(\" train MAE:\",err1)\n    print(\" test MAE:\" ,err2)\n\nn_neighbors = 1\n train MAE: 0.3040816326530612\n test MAE: 0.926530612244898\nn_neighbors = 2\n train MAE: 0.4627551020408163\n test MAE: 0.8428571428571429\nn_neighbors = 3\n train MAE: 0.5479591836734694\n test MAE: 0.7945578231292517\nn_neighbors = 4\n train MAE: 0.5816326530612245\n test MAE: 0.7520408163265306\nn_neighbors = 5\n train MAE: 0.5932653061224491\n test MAE: 0.7240816326530614\nn_neighbors = 6\n train MAE: 0.585374149659864\n test MAE: 0.6965986394557823\nn_neighbors = 7\n train MAE: 0.5906705539358601\n test MAE: 0.6857142857142857\nn_neighbors = 8\n train MAE: 0.5923469387755103\n test MAE: 0.6586734693877551\nn_neighbors = 9\n train MAE: 0.591156462585034\n test MAE: 0.6476190476190476\nn_neighbors = 10\n train MAE: 0.5948979591836735\n test MAE: 0.6412244897959185\nn_neighbors = 11\n train MAE: 0.5950834879406307\n test MAE: 0.6430426716141002\nn_neighbors = 12\n train MAE: 0.5951530612244897\n test MAE: 0.6414965986394559\nn_neighbors = 13\n train MAE: 0.5978021978021978\n test MAE: 0.6417582417582417\nn_neighbors = 14\n train MAE: 0.5991253644314869\n test MAE: 0.6396501457725947\nn_neighbors = 15\n train MAE: 0.6009523809523809\n test MAE: 0.6345578231292516\nn_neighbors = 16\n train MAE: 0.6007015306122448\n test MAE: 0.6364795918367347\nn_neighbors = 17\n train MAE: 0.6027611044417768\n test MAE: 0.6357743097238895\nn_neighbors = 18\n train MAE: 0.6044784580498866\n test MAE: 0.6333333333333334\nn_neighbors = 19\n train MAE: 0.604296455424275\n test MAE: 0.6326530612244898\nn_neighbors = 20\n train MAE: 0.605561224489796\n test MAE: 0.6308163265306123\nn_neighbors = 21\n train MAE: 0.6053449951409134\n test MAE: 0.6359572400388727\nn_neighbors = 22\n train MAE: 0.6055658627087199\n test MAE: 0.6363636363636364\nn_neighbors = 23\n train MAE: 0.6061668145519077\n test MAE: 0.6346051464063887\nn_neighbors = 24\n train MAE: 0.6059098639455782\n test MAE: 0.6335034013605442\nn_neighbors = 25\n train MAE: 0.6081632653061224\n test MAE: 0.6349387755102041\nn_neighbors = 26\n train MAE: 0.6085557299843014\n test MAE: 0.6346938775510202\nn_neighbors = 27\n train MAE: 0.6096371882086168\n test MAE: 0.6374905517762662\nn_neighbors = 28\n train MAE: 0.6099854227405247\n test MAE: 0.6357142857142857\nn_neighbors = 29\n train MAE: 0.6112244897959184\n test MAE: 0.6378606615059818\nn_neighbors = 30\n train MAE: 0.6113945578231292\n test MAE: 0.6371428571428571\nn_neighbors = 31\n train MAE: 0.6120803159973666\n test MAE: 0.6359447004608295\nn_neighbors = 32\n train MAE: 0.6123405612244898\n test MAE: 0.6357142857142857"
  },
  {
    "objectID": "5000-website/decision-tree/reference-notebooks/regression.html#convergence-plot",
    "href": "5000-website/decision-tree/reference-notebooks/regression.html#convergence-plot",
    "title": "Regression",
    "section": "Convergence plot",
    "text": "Convergence plot\n\nplt.plot(num_neighbors_2,train_error ,linewidth=2, color='k')\nplt.plot(num_neighbors_2,test_error ,linewidth=2, color='b')\n\nplt.xlabel(\"Number of neighbors in KNN\")\nplt.ylabel(\"Training (black) and test (blue) MAE (error)\")\n\nText(0, 0.5, 'Training (black) and test (blue) MAE (error)')"
  },
  {
    "objectID": "5000-website/decision-tree/reference-notebooks/regression.html#re-train-with-optimal-parameters",
    "href": "5000-website/decision-tree/reference-notebooks/regression.html#re-train-with-optimal-parameters",
    "title": "Regression",
    "section": "Re-train with optimal parameters",
    "text": "Re-train with optimal parameters\n\n# INITIALIZE MODEL \nmodel = KNeighborsRegressor(n_neighbors=35)\n\n# TRAIN MODEL \nmodel.fit(x_train,y_train)\n\n# OUTPUT PREDICTIONS FOR TRAINING AND TEST SET \nyp_train = model.predict(x_train)\nyp_test = model.predict(x_test)"
  },
  {
    "objectID": "5000-website/decision-tree/reference-notebooks/regression.html#parity-plot",
    "href": "5000-website/decision-tree/reference-notebooks/regression.html#parity-plot",
    "title": "Regression",
    "section": "Parity Plot",
    "text": "Parity Plot\n\nPlotting y_pred vs y_data lets you see how good the fit is\nThe closer to the line y=x the better the fit (ypred=ydata –&gt; prefect fit)\n\n\nplt.plot(y_train,yp_train ,\"o\", color='k')\nplt.plot(y_test,yp_test ,\"o\", color='b')\nplt.plot(y_test,y_test ,\"-\", color='r')\n\nplt.xlabel(\"y_data\")\nplt.ylabel(\"y_pred (blue=test)(black=Train)\")\n\nText(0, 0.5, 'y_pred (blue=test)(black=Train)')\n\n\n\n\n\n\n\n\n\nPlot tree\n\nfrom sklearn import tree\ndef plot_tree(model):\n    fig = plt.figure(figsize=(25,20))\n    _ = tree.plot_tree(model, \n                    filled=True)\n    plt.show()\n\n\nplot_tree(model)\n\n\n\n\n\n\n\n\n\n#loop over columns\nfor i in range(0, x_train.shape[1]):\n    print(i)\n    plt.plot(x_train.iloc[:, i], y_train, \"o\", color='b')\n    plt.show()\n\n0\n1\n2\n3\n4\n5\n6\n7\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# LINEAR REGRESSION \nfrom sklearn.linear_model import LinearRegression\n\nmodel = LinearRegression().fit(X, y)\n\n# OUTPUT PREDICTIONS FOR TRAINING AND TEST SET \nyp_train = model.predict(x_train)\nyp_test = model.predict(x_test)\n\nplt.plot(y_train,yp_train ,\"o\", color='k')\nplt.plot(y_test,yp_test ,\"o\", color='b')\nplt.plot(y_train,y_train,\"-\", color='r')\n\nplt.xlabel(\"y_data\")\nplt.ylabel(\"y_pred (blue=test)(black=Train)\")\n\n    \nerr1=100.0*np.mean(np.absolute((yp_train-y_train)/y_train))\nerr2=100.0*np.mean(np.absolute((yp_test-y_test)/y_test))\n\nprint(\" train error:\",err1)\nprint(\" test error:\" ,err2)\n\n train error: 45.71642474234835\n test error: 27.859375656835915\n\n\n\n\n\n\n\n\n\n\nmodel = DecisionTreeRegressor(min_samples_split=50)\nmodel.fit(x_train,y_train)\nplot_tree(model)\n\n\n\n\n\n\n\n\n\nfrom sklearn.neural_network import MLPRegressor\n\nmodel = MLPRegressor(hidden_layer_sizes=(32,32,32),random_state=1, max_iter=10000).fit(x_train, y_train)\n\n# OUTPUT PREDICTIONS FOR TRAINING AND TEST SET \nyp_train = model.predict(x_train)\nyp_test = model.predict(x_test)\n\nplt.plot(y_train,yp_train ,\"o\", color='k')\nplt.plot(y_test,yp_test ,\"o\", color='b')\nplt.plot(y_train,y_train,\"-\", color='r')\n\nplt.xlabel(\"y_data\")\nplt.ylabel(\"y_pred (blue=test)(black=Train)\")\n\n    \nerr1=100.0*np.mean(np.absolute((yp_train-y_train)/y_train))\nerr2=100.0*np.mean(np.absolute((yp_test-y_test)/y_test))\n\nprint(\" train error:\",err1)\nprint(\" test error:\" ,err2)\n\n train error: 40.00431316772487\n test error: 27.20196900065661"
  },
  {
    "objectID": "5000-website/decision-tree/classification-and-regression.html",
    "href": "5000-website/decision-tree/classification-and-regression.html",
    "title": "Classification and Regression Decision Trees",
    "section": "",
    "text": "Decision trees are supervised machine learning algorithms. Supervised learning involves building models that learn from a labeled dataset, where the model is trained on pairs of inputs and their corresponding outputs. The algorithm learns a functional mapping between these inputs and outputs. For instance, if we consider a dataset as a group of neighbors, when a new data point (resident) is introduced into the neighborhood, a decision tree can be used to infer its characteristics based on the features of its settling ‘location’.\nThere are two main types of tasks that decision trees can be used for:\nClassification: This is a qualitative and categorical approach where the model is trained to identify known categories within the data. When a new data point is introduced, the decision tree assigns it to one of the predefined categories.\nRegression: This is a quantitative approach where the model performs curve fitting based on continuous numerical variables. In regression, the decision tree predicts a continuous value for the new data point.\nThe image below summarizes the concept of decision trees in a very pragmatic manner which makes the higher level concept easier to undertsand:\n\nimage source: https://www.datacamp.com/tutorial/decision-tree-classification-python",
    "crumbs": [
      "EV Insights",
      "Decision Trees"
    ]
  },
  {
    "objectID": "5000-website/decision-tree/classification-and-regression.html#class-distribution",
    "href": "5000-website/decision-tree/classification-and-regression.html#class-distribution",
    "title": "Classification and Regression Decision Trees",
    "section": "Class Distribution",
    "text": "Class Distribution\nHere we are interested to see the state of the distribution of the labels before any type of training is performed. We are seeking to understand how the data we are working with is spread out. This helps us understand which labels exist more than others and level sets an expectation for the output of the learning.\n\n\nShow the code\n# code ref: https://stackoverflow.com/questions/61612198/how-can-i-use-a-probability-distribution-in-python-class\nimport numpy as np\n\n# Labels for the classes\nlabels = {0: 'electric vehicle', 1: 'gasoline vehicle', 2: 'hybrid vehicle'}\n\n# y_train is a numpy array\nunique_classes, counts = np.unique(y_train, return_counts=True)\n\n# Get the proportions\nproportions = counts / len(y_train)\n\n# Print the results with labels\nfor unique_class, count in zip(unique_classes, counts):\n    label = labels[unique_class]\n    print(f\"Number of points with target={label}: {count} ({count/len(y_train):.2f})\")\n\n# Retrieve the dominant class\ndominant_class_index = np.argmax(counts)\ndominant_class_label = labels[unique_classes[dominant_class_index]]\nprint(\"The dominant class is:\", dominant_class_label)\n\nweight = count/len(y_train)\n\n\nNumber of points with target=electric vehicle: 354 (0.36)\nNumber of points with target=gasoline vehicle: 353 (0.36)\nNumber of points with target=hybrid vehicle: 273 (0.28)\nThe dominant class is: electric vehicle\n\n\nBased on the label distribution, we can infer that the dataset is fairly well-balanced, which should benefit the classification algorithm’s performance. The counts of electric and gasoline vehicles are almost equal, with electric vehicles marginally more prevalent. However, the hybrid vehicle class being less represented might affect the algorithm’s overall performance, particularly in accurately classifying this category.",
    "crumbs": [
      "EV Insights",
      "Decision Trees"
    ]
  },
  {
    "objectID": "5000-website/decision-tree/classification-and-regression.html#classification-model-2-random-forest",
    "href": "5000-website/decision-tree/classification-and-regression.html#classification-model-2-random-forest",
    "title": "Classification and Regression Decision Trees",
    "section": "Classification model-2: Random Forest",
    "text": "Classification model-2: Random Forest\nA more advanced decision tree algorithm (Random Forest) is implemented to observe any performance enhancements from the previous step.\n\n\nShow the code\nfrom sklearn.ensemble import RandomForestClassifier\n\n# INITIALIZE MODEL\nmodel = RandomForestClassifier()\n\n# TRAIN MODEL \nmodel.fit(x_train, y_train)\n\n# PRINT REPORT USING UTILITY FUNCTION ABOVE\nprint_model_summary()\n\n\nACCURACY CALCULATION\n\nTRAINING SET:\nAccuracy: 79.18367346938776\nNumber of mislabeled points out of a total 980 points = 204\n\nTEST SET (UNTRAINED DATA):\nAccuracy: 46.53061224489796\nNumber of mislabeled points out of a total 245 points = 131\n\nCHECK FIRST 20 PREDICTIONS\nTRAINING SET:\n[0 1 1 2 1 2 2 1 0 0 1 1 2 0 2 0 1 0 0 2]\n[0 1 2 0 1 2 2 1 0 0 1 1 2 0 2 0 0 0 0 1]\nERRORS: [ 0  0  1 -2  0  0  0  0  0  0  0  0  0  0  0  0 -1  0  0 -1]\n\nTEST SET (UNTRAINED DATA):\n[2 0 2 0 1 2 2 2 0 1 2 1 0 0 2 0 0 1 2 2]\n[0 1 0 1 1 0 0 1 0 1 0 1 2 0 1 0 0 1 2 0]\nERRORS: [-2  1 -2  1  0 -2 -2 -1  0  0 -2  0  2  0 -1  0  0  0  0 -2]\n\n\n\n\nShow the code\nyp_train = model.predict(x_train)\nyp_test = model.predict(x_test)\n\n\n\nFinal results of Random Forest\n\n\nShow the code\nprint(\"-----RANDOM FOREST CLASSIFIER: TRAINING SET-----\")\nprint_metrics(y_train, yp_train, class_labels)\n\nprint(\"-----RANDOM FOREST: TRAINING SET-----\")\nplot_confusion_matrix(y_train, yp_train, labels.values())\n\n\n-----RANDOM FOREST CLASSIFIER: TRAINING SET-----\nAccuracy: 0.7918367346938775\nPrecision: [0.75245098 0.83426966 0.7962963 ]\nRecall: [0.86723164 0.84135977 0.63003663]\nF-score: [0.80577428 0.83779972 0.70347648]\n-----RANDOM FOREST: TRAINING SET-----\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nprint(\"-----RANDOM FOREST CLASSIFIER: TEST SET-----\")\nprint_metrics(y_test, yp_test, class_labels)\n\nprint(\"-----CONFUSION MATRIX: TEST SET-----\")\nplot_confusion_matrix(y_test, yp_test, labels.values())\n\n\n-----RANDOM FOREST CLASSIFIER: TEST SET-----\nAccuracy: 0.46530612244897956\nPrecision: [0.39795918 0.66292135 0.27586207]\nRecall: [0.52       0.6344086  0.20779221]\nF-score: [0.45086705 0.64835165 0.23703704]\n-----CONFUSION MATRIX: TEST SET-----",
    "crumbs": [
      "EV Insights",
      "Decision Trees"
    ]
  },
  {
    "objectID": "5000-website/decision-tree/classification-and-regression.html#regression-model-decision-tree-regression",
    "href": "5000-website/decision-tree/classification-and-regression.html#regression-model-decision-tree-regression",
    "title": "Classification and Regression Decision Trees",
    "section": "Regression model: Decision Tree Regression",
    "text": "Regression model: Decision Tree Regression\n\nfit y2 (sentiment score from -1 to 1) instead of y1 (classes)\n\n\n\nShow the code\nfrom sklearn.model_selection import train_test_split\ntest_ratio=0.2\nx_train, x_test, y_train, y_test = train_test_split(X, y2, test_size=test_ratio, random_state=0)\ny_train=y_train.flatten()\ny_test=y_test.flatten()\n\nprint(\"x_train.shape        :\",x_train.shape)\nprint(\"y_train.shape        :\",y_train.shape)\n\nprint(\"X_test.shape     :\",x_test.shape)\nprint(\"y_test.shape     :\",y_test.shape)\n\n\nx_train.shape       : (980, 8233)\ny_train.shape       : (980,)\nX_test.shape        : (245, 8233)\ny_test.shape        : (245,)\n\n\n\nModel tuning\n\nWe can find the optimal max_depth paramter from this step\n\n\n\nShow the code\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.metrics import mean_absolute_error\nimport matplotlib.pyplot as plt\n\n# Prepare lists to store the results of MAE for different depths\nmax_depths = []\ntrain_error = []\ntest_error = []\n\n# Loop over max_depth values\nfor depth in range(1, 33):  \n    # Initialize the DecisionTreeRegressor with the current depth\n    model = DecisionTreeRegressor(max_depth=depth)\n\n    # Train the model on the training data\n    model.fit(x_train, y_train)\n\n    # Make predictions on both the training and test sets\n    yp_train = model.predict(x_train)\n    yp_test = model.predict(x_test)\n\n    # Calculate the MAE for both the training and test sets\n    err_train = mean_absolute_error(y_train, yp_train)\n    err_test = mean_absolute_error(y_test, yp_test)\n\n    # Store the depth and corresponding MAE values in lists\n    max_depths.append(depth)\n    train_error.append(err_train)\n    test_error.append(err_test)\n\n    # Print the results for this depth\n    print(f\"max_depth = {depth}: train MAE = {err_train}, test MAE = {err_test}\")\n\n# Plotting the MAE against tree depth\nplt.figure(figsize=(12, 6))\nplt.plot(max_depths, train_error, label='Training Error', marker='o')\nplt.plot(max_depths, test_error, label='Test Error', marker='s')\nplt.title('Decision Tree Regressor MAE vs Tree Depth')\nplt.xlabel('Maximum Tree Depth')\nplt.ylabel('Mean Absolute Error')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\nmax_depth = 1: train MAE = 0.37802789659372404, test MAE = 0.37667956658126905\nmax_depth = 2: train MAE = 0.36483065270701065, test MAE = 0.3778158341734557\nmax_depth = 3: train MAE = 0.34791160696663853, test MAE = 0.3747152427786673\nmax_depth = 4: train MAE = 0.33299089908973295, test MAE = 0.35809115136540054\nmax_depth = 5: train MAE = 0.3165762999024118, test MAE = 0.3507836345744812\nmax_depth = 6: train MAE = 0.30307608835465766, test MAE = 0.35604688067725576\nmax_depth = 7: train MAE = 0.2870110952413117, test MAE = 0.35906959232241364\nmax_depth = 8: train MAE = 0.27164520108073037, test MAE = 0.35243928181934286\nmax_depth = 9: train MAE = 0.2557884599294701, test MAE = 0.3427716615089755\nmax_depth = 10: train MAE = 0.24016683732093946, test MAE = 0.3387680314774147\nmax_depth = 11: train MAE = 0.2280788458865884, test MAE = 0.33039492978113805\nmax_depth = 12: train MAE = 0.2173305357787406, test MAE = 0.31998115627524676\nmax_depth = 13: train MAE = 0.2058571547677174, test MAE = 0.31368188490835297\nmax_depth = 14: train MAE = 0.19844277297366467, test MAE = 0.3095764841990457\nmax_depth = 15: train MAE = 0.1909754766599104, test MAE = 0.30601829486066523\nmax_depth = 16: train MAE = 0.17732874462377016, test MAE = 0.30926151591990514\nmax_depth = 17: train MAE = 0.1661241131266359, test MAE = 0.297249822476754\nmax_depth = 18: train MAE = 0.15496339434222706, test MAE = 0.2896524477304179\nmax_depth = 19: train MAE = 0.1461612081760722, test MAE = 0.286733428974952\nmax_depth = 20: train MAE = 0.13675809059878025, test MAE = 0.2752070757396265\nmax_depth = 21: train MAE = 0.12749758005837994, test MAE = 0.27860259491457146\nmax_depth = 22: train MAE = 0.11704547590704215, test MAE = 0.2745737868067137\nmax_depth = 23: train MAE = 0.11001769561277626, test MAE = 0.26479823225248156\nmax_depth = 24: train MAE = 0.10137320468608818, test MAE = 0.26703124458556327\nmax_depth = 25: train MAE = 0.09480656404331814, test MAE = 0.26314708047504015\nmax_depth = 26: train MAE = 0.0902363251109938, test MAE = 0.25808292363983515\nmax_depth = 27: train MAE = 0.08512440125868737, test MAE = 0.2612204381679343\nmax_depth = 28: train MAE = 0.08122117404398292, test MAE = 0.2603488695589683\nmax_depth = 29: train MAE = 0.07634287382100458, test MAE = 0.2633075857859296\nmax_depth = 30: train MAE = 0.07306051925293142, test MAE = 0.2549292422798067\nmax_depth = 31: train MAE = 0.07005340133112985, test MAE = 0.26701824763292964\nmax_depth = 32: train MAE = 0.06515929044289452, test MAE = 0.26898387674265084",
    "crumbs": [
      "EV Insights",
      "Decision Trees"
    ]
  },
  {
    "objectID": "conclusion.html",
    "href": "conclusion.html",
    "title": "Conclusion",
    "section": "",
    "text": "Conclusion"
  },
  {
    "objectID": "eda/eda.html",
    "href": "eda/eda.html",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "",
    "text": "Exploratory Data Analysis (EDA)"
  },
  {
    "objectID": "5000-website/eda/eda.html",
    "href": "5000-website/eda/eda.html",
    "title": "Data Exploration",
    "section": "",
    "text": "Show the code\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np",
    "crumbs": [
      "EV Insights",
      "Data Exploration"
    ]
  },
  {
    "objectID": "5000-website/eda/eda.html#quick-look-at-the-data",
    "href": "5000-website/eda/eda.html#quick-look-at-the-data",
    "title": "Data Exploration",
    "section": "",
    "text": "Show the code\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np",
    "crumbs": [
      "EV Insights",
      "Data Exploration"
    ]
  },
  {
    "objectID": "5000-website/eda/eda.html#data-understanding",
    "href": "5000-website/eda/eda.html#data-understanding",
    "title": "Data Exploration",
    "section": "Data Understanding",
    "text": "Data Understanding\n\n\nShow the code\n# Apply the default theme\nsns.set_theme(style=\"whitegrid\", palette=\"pastel\")\n\n# Load a local file\nstocks = pd.read_csv(\"stock.csv\")\nprint(stocks)\n\n\n           STLA         GM          TM      NSANY      MBGYY      BMWYY  \\\n0     11.472710  37.606449  128.369995  20.040001  15.427056  25.132998   \n1     11.945811  38.524120  130.130005  20.250000  15.544848  25.270374   \n2     12.854664  39.711693  132.160004  20.230000  15.751438  25.443907   \n3     13.551867  39.594738  133.860001  20.389999  15.934468  25.740351   \n4     13.433591  39.783680  134.770004  20.440001  16.003330  25.913887   \n...         ...        ...         ...        ...        ...        ...   \n1464  18.480000  28.549999  171.610001   7.780000  15.140000  31.129999   \n1465  18.040001  27.219999  172.809998   7.800000  14.710000  31.059999   \n1466  18.000000  27.360001  171.399994   7.590000  14.780000  31.219999   \n1467  18.680000  28.200001  175.179993   7.690000  14.600000  30.940001   \n1468  18.940001  28.000000  185.779999   7.930000  14.790000  31.530001   \n\n         POAHY        TSLA       Dates  \n0     6.742718   21.368668  2018-01-02  \n1     6.887550   21.150000  2018-01-03  \n2     7.072612   20.974667  2018-01-04  \n3     7.145028   21.105333  2018-01-05  \n4     7.161119   22.427334  2018-01-08  \n...        ...         ...         ...  \n1464  4.440000  205.759995  2023-10-26  \n1465  4.390000  207.300003  2023-10-27  \n1466  4.400000  197.360001  2023-10-30  \n1467  4.410000  200.839996  2023-10-31  \n1468  4.460000  205.660004  2023-11-01  \n\n[1469 rows x 9 columns]\n\n\n\n\nShow the code\nstocks.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1469 entries, 0 to 1468\nData columns (total 9 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   STLA    1469 non-null   float64\n 1   GM      1469 non-null   float64\n 2   TM      1469 non-null   float64\n 3   NSANY   1469 non-null   float64\n 4   MBGYY   1469 non-null   float64\n 5   BMWYY   1469 non-null   float64\n 6   POAHY   1469 non-null   float64\n 7   TSLA    1469 non-null   float64\n 8   Dates   1469 non-null   object \ndtypes: float64(8), object(1)\nmemory usage: 103.4+ KB\n\n\n\n\nShow the code\nstocks['Dates'] = pd.to_datetime(stocks['Dates'])  # Convert the 'Dates' column to datetime format\n\n# Filter the dataframe for only 2023 dates\nstocks_2023 = stocks[stocks['Dates'].dt.year == 2023]\n\nprint(stocks_2023)\n\n\n           STLA         GM          TM  NSANY      MBGYY      BMWYY     POAHY  \\\n1259  13.374442  33.556999  138.279999   6.33  15.643178  27.898378  5.281156   \n1260  13.805285  34.420227  137.190002   6.32  16.162148  28.553400  5.405082   \n1261  13.906119  34.727821  135.520004   6.31  16.180681  28.793264  5.433681   \n1262  14.162791  35.630741  138.970001   6.49  16.421631  29.374483  5.548074   \n1263  14.318627  35.640663  139.940002   6.56  16.588442  29.439062  5.633869   \n...         ...        ...         ...    ...        ...        ...       ...   \n1464  18.480000  28.549999  171.610001   7.78  15.140000  31.129999  4.440000   \n1465  18.040001  27.219999  172.809998   7.80  14.710000  31.059999  4.390000   \n1466  18.000000  27.360001  171.399994   7.59  14.780000  31.219999  4.400000   \n1467  18.680000  28.200001  175.179993   7.69  14.600000  30.940001  4.410000   \n1468  18.940001  28.000000  185.779999   7.93  14.790000  31.530001  4.460000   \n\n            TSLA      Dates  \n1259  108.099998 2023-01-03  \n1260  113.639999 2023-01-04  \n1261  110.339996 2023-01-05  \n1262  113.059998 2023-01-06  \n1263  119.769997 2023-01-09  \n...          ...        ...  \n1464  205.759995 2023-10-26  \n1465  207.300003 2023-10-27  \n1466  197.360001 2023-10-30  \n1467  200.839996 2023-10-31  \n1468  205.660004 2023-11-01  \n\n[210 rows x 9 columns]",
    "crumbs": [
      "EV Insights",
      "Data Exploration"
    ]
  },
  {
    "objectID": "5000-website/eda/eda.html#descriptive-statistics",
    "href": "5000-website/eda/eda.html#descriptive-statistics",
    "title": "Data Exploration",
    "section": "Descriptive Statistics",
    "text": "Descriptive Statistics\n\nThe number of rows and columns\n\n\nShow the code\n# get the number of rows and columns\n\nlen(stocks_2023.index), len(stocks_2023.columns)\n\n\n(210, 9)\n\n\n\n\nNaN values in all columns\n\n\nShow the code\n# Counting NaN values in all columns\nnan_count = stocks_2023.isna().sum()\n\nprint(nan_count)\n\n\nSTLA     0\nGM       0\nTM       0\nNSANY    0\nMBGYY    0\nBMWYY    0\nPOAHY    0\nTSLA     0\nDates    0\ndtype: int64\n\n\n\n\nBasic Statistics\n\n\nShow the code\nprint(\"----------------------\")\nprint(\"BASIC STATISTICS: \")\nprint(\"----------------------\")\nstocks_2023.describe()\n\n\n----------------------\nBASIC STATISTICS: \n----------------------\n\n\n\n\n\n\n\n\n\n\nSTLA\nGM\nTM\nNSANY\nMBGYY\nBMWYY\nPOAHY\nTSLA\nDates\n\n\n\n\ncount\n210.000000\n210.000000\n210.000000\n210.000000\n210.000000\n210.000000\n210.000000\n210.000000\n210\n\n\nmean\n17.076711\n35.148255\n154.424142\n7.846190\n18.125585\n34.915196\n5.415873\n213.400285\n2023-06-03 13:42:51.428571392\n\n\nmin\n13.374442\n27.219999\n133.479996\n6.310000\n14.600000\n27.898378\n4.390000\n108.099998\n2023-01-03 00:00:00\n\n\n25%\n15.901123\n32.907166\n139.217495\n7.372500\n17.360720\n32.543494\n5.219193\n183.252499\n2023-03-20 06:00:00\n\n\n50%\n16.820590\n34.420546\n146.985001\n7.740000\n17.959999\n34.445070\n5.447980\n207.575005\n2023-06-03 12:00:00\n\n\n75%\n18.632500\n37.777060\n167.952503\n8.335000\n19.015000\n37.187501\n5.671500\n255.487499\n2023-08-17 18:00:00\n\n\nmax\n20.549999\n42.834282\n195.039993\n9.640000\n20.670000\n41.189999\n6.000000\n293.339996\n2023-11-01 00:00:00\n\n\nstd\n1.692184\n3.327676\n16.635200\n0.698586\n1.194182\n3.125648\n0.356021\n44.268249\nNaN",
    "crumbs": [
      "EV Insights",
      "Data Exploration"
    ]
  },
  {
    "objectID": "5000-website/eda/eda.html#data-visualization",
    "href": "5000-website/eda/eda.html#data-visualization",
    "title": "Data Exploration",
    "section": "Data visualization",
    "text": "Data visualization\n\n\nShow the code\n\n# Create a visualization\nsns.set_theme(style=\"whitegrid\", palette=\"pastel\")\n\n# Create a histogram for TSLA stock prices\nsns.lineplot(data=stocks_2023, x='Dates', y='TSLA')\nplt.title('TSLA Stock Price Fluctuation for 2023')\nplt.xlabel('Dates')\nplt.ylabel('Stock Price')\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\n\n\n\n\n\nIn 2023, TSLA’s stock price experienced a sharp rise until mid-year, followed by a decline towards the end of the year.\n\n\nShow the code\nsns.lineplot(data=stocks_2023, x='Dates', y='TSLA', label='TSLA')\nsns.lineplot(data=stocks_2023, x='Dates', y='GM', label='GM')\nplt.title('TSLA vs GM Stock Price Comparison for 2023')\nplt.xlabel('Dates')\nplt.ylabel('Stock Price')\nplt.legend()\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\n\n\n\n\n\nThroughout 2023, TSLA’s stock price exhibited significant volatility, peaking around mid-year before declining, while GM’s stock remained relatively stable, hovering at a much lower price range. The comparison shows a stark contrast in the stock price trajectories of the two companies during this period.",
    "crumbs": [
      "EV Insights",
      "Data Exploration"
    ]
  },
  {
    "objectID": "5000-website/eda/eda.html#correlation-analysis",
    "href": "5000-website/eda/eda.html#correlation-analysis",
    "title": "Data Exploration",
    "section": "Correlation Analysis",
    "text": "Correlation Analysis\n\n\nShow the code\n# INSERT CODE TO RE-CREATE THE FOLLOWING PLOTS \n# SEE IF THERE IS ANY CORRELATION BETWEEN THE CONTINOUS VARIABLES\n\n# INSERT CODE\n\n# LINEAR MODEL PLOT (lmplot --&gt; Plot data and regression model fits across a FacetGrid)\nsns.lmplot(data=stocks_2023, x='TSLA', y='STLA')\n\n\n/Users/isfarbaset/anaconda3/lib/python3.11/site-packages/seaborn/axisgrid.py:118: UserWarning: The figure layout has changed to tight\n  self._figure.tight_layout(*args, **kwargs)\n\n\n\n\n\n\n\n\n\nThe scatter plot displays a positive correlation between “TSLA” and “STLA” stock values. As the “TSLA” value increases on the x-axis, the “STLA” value typically rises on the y-axis, with some variability.\n\n\nShow the code\n# # Convert the dataframe from wide to long format\n# # df_melted = stocks_2023.melt(id_vars=['Dates'], value_vars=['STLA', 'GM', 'TM', 'NSANY', 'MBGYY', 'BMWYY', 'POAHY', 'TSLA'], \n# #                     var_name='Brand', value_name='Stock Price')\n\n# print(df_melted)\n\n\n\n\nShow the code\n# INSERT CODE TO RE-CREATE THE FOLLOWING PLOTS \n\n# INSERT CODE\n\nsns.set_theme(style=\"white\")\ncorr = stocks_2023.corr(numeric_only=True)  #Compute the correlation matrix\n\n# # Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=bool)) \nf, ax = plt.subplots(figsize=(7, 5)) #initialize figure\n\ncmap = sns.diverging_palette(230, 20, as_cmap=True) #custom diverging colormap\n\n# # Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\nplt.show()\n\n\n\n\n\n\n\n\n\nBased on the correlation heatmap, it can be said that the stock prices of the different care brands are mostly negatively correlated.",
    "crumbs": [
      "EV Insights",
      "Data Exploration"
    ]
  },
  {
    "objectID": "5000-website/eda/eda.html#hypothesis-generation",
    "href": "5000-website/eda/eda.html#hypothesis-generation",
    "title": "Data Exploration",
    "section": "Hypothesis Generation",
    "text": "Hypothesis Generation\nBased on the different visualizations above, an apt hypothesis to test out could be “TSLA’s significant price surge in 2023 suggests a dominant market influence, potentially driving similar trends in associated automotive stocks”",
    "crumbs": [
      "EV Insights",
      "Data Exploration"
    ]
  },
  {
    "objectID": "5000-website/eda/eda.html#data-grouping-and-segmentation",
    "href": "5000-website/eda/eda.html#data-grouping-and-segmentation",
    "title": "Data Exploration",
    "section": "Data Grouping and Segmentation",
    "text": "Data Grouping and Segmentation\n\n\nShow the code\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nstocks_2023 = stocks_2023.drop('Dates', axis=1)\n\n# Standardizing the data (important for K-Means)\nscaler = StandardScaler()\nscaled_df = scaler.fit_transform(stocks_2023.dropna())\n\n# Using K-Means clustering\nkmeans = KMeans(n_clusters=3)  # assuming 3 clusters\nstocks_2023['Cluster'] = kmeans.fit_predict(scaled_df)\n\n# Visualize using TSLA and another stock, e.g., 'GM'\nsns.lmplot(x='TSLA', y='GM', data=stocks_2023, hue='Cluster', fit_reg=False)\nplt.show()\n\n\n/Users/isfarbaset/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/isfarbaset/anaconda3/lib/python3.11/site-packages/seaborn/axisgrid.py:118: UserWarning: The figure layout has changed to tight\n  self._figure.tight_layout(*args, **kwargs)\n\n\n\n\n\n\n\n\n\nThe visualization displays three distinct clusters of data points based on TSLA and GM stock values. Cluster 0 primarily represents data points where both TSLA and GM have lower stock values, Cluster 1 captures instances with mid-range values, and Cluster 2 comprises of higher TSLA stock values with varying GM values. The spread of the clusters suggests a potential relationship or pattern between the stock values of TSLA and GM over the period of 2023",
    "crumbs": [
      "EV Insights",
      "Data Exploration"
    ]
  },
  {
    "objectID": "5000-website/eda/eda.html#identifying-outliers",
    "href": "5000-website/eda/eda.html#identifying-outliers",
    "title": "Data Exploration",
    "section": "Identifying Outliers",
    "text": "Identifying Outliers\n\n\nShow the code\n# Source: https://www.datacamp.com/tutorial/naive-bayes-scikit-learn\n\ndef show_boxplot(df):\n    plt.rcParams['figure.figsize'] = [14,6]\n    sns.boxplot(data = df, orient=\"v\")\n    plt.title(\"Outliers Distribution\", fontsize = 16)\n    plt.ylabel(\"Range\", fontweight = 'bold')\n    plt.xlabel(\"Attributes\", fontweight = 'bold')\n   \nshow_boxplot(stocks_2023)\n\n\n\n\n\n\n\n\n\nHere we can see the distribution of outliers for various automotive stock attributes. Most stocks, like GM, TM, NSANY, MBGYY, BMWYY, and POAHY, have a narrow range of outlier values, typically below 100. Notably, TM displays a compact outlier range between 100 and 150, while two instances of TSLA stock demonstrate a broader range of outliers, with one reaching up to 300 and another clustering around 150 to 200.",
    "crumbs": [
      "EV Insights",
      "Data Exploration"
    ]
  },
  {
    "objectID": "5000-website/eda/eda.html#report-and-discussion-of-methods-and-findings",
    "href": "5000-website/eda/eda.html#report-and-discussion-of-methods-and-findings",
    "title": "Data Exploration",
    "section": "Report and discussion of methods and findings",
    "text": "Report and discussion of methods and findings\n\n\n\n\n\n\n\n\nTopic\nDescription\nMethods\n\n\n\n\nCorrelation Matrix\nThe matrix depicts correlations between various stock tickers. Blue indicates positive correlation, red indicates negative correlation, and white means no correlation. For instance, “STLA” and “GM” have a strong positive correlation while “TSLA” has low to no correlation with most of the other stocks.\nThis was done using seaborn’s correlation analysis tool\n\n\nTSLA Stock Price Fluctuation for 2023\nThe chart shows Tesla’s stock price performance throughout 2023. Tesla’s stock price has seen a significant rise from January to around March/April and then a fluctuation with a general downtrend.\nMatplotlib and Seaborn was used\n\n\nTSLA vs GM Stock Price Comparison for 2023\nThis line graph compares the stock prices of Tesla (TSLA) and General Motors (GM) for 2023. While TSLA experienced significant fluctuations in its stock price, GM’s stock price remained relatively stable and lower in comparison.\nMatplotlib and seaborn was used\n\n\nStock Correlation Heatmap\nThe heatmap visually represents the correlation between various stock tickers. The colors range from blue (positive correlation) to red (negative correlation) with white indicating no correlation. The vertical and horizontal labels denote the stock tickers being compared.\nSeaborn was used\n\n\nOutliers Distribution\nThe box plot demonstrates the distribution of outliers for different stock tickers. It helps in identifying stocks with higher price volatility and potential risks. TSLA and TM seem to have a more extensive range compared to others like STLA and GM.\nBox Plot using Seaborn and Matplotlib\n\n\nStock Price Cluster Analysis (TSLA vs GM)\nThis scatter plot showcases a cluster analysis between TSLA and GM stock prices. It suggests possible correlation patterns or trading strategies by observing how the stocks cluster together in distinct groups. The colors differentiate the clusters.\nScatter Plot using Matplotlib, Seaborn and SKlearn",
    "crumbs": [
      "EV Insights",
      "Data Exploration"
    ]
  },
  {
    "objectID": "5000-website/eda/eda.html#tools-and-software",
    "href": "5000-website/eda/eda.html#tools-and-software",
    "title": "Data Exploration",
    "section": "Tools and Software",
    "text": "Tools and Software\nPython with libraries like Pandas, Matplotlib, Seaborn, Numpy and SKlearn",
    "crumbs": [
      "EV Insights",
      "Data Exploration"
    ]
  },
  {
    "objectID": "5000-website/decision-tree/reference-notebooks/car-classification.html",
    "href": "5000-website/decision-tree/reference-notebooks/car-classification.html",
    "title": "Isfar Baset",
    "section": "",
    "text": "import pandas as pd\nimport seaborn as sns \nimport matplotlib.pyplot as plt\nfrom sklearn import tree\nfrom IPython.display import Image\nimport numpy as np\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\n\n\ndf=pd.read_csv('cardf_cleaned.csv')  \nprint(df.shape)\ndf.head()\n\n(719, 13)\n\n\n\n\n\n\n\n\n\n\nUnnamed: 0\ncity_mpg\nclass\ncombination_mpg\ncylinders\ndisplacement\ndrive\nfuel_type\nhighway_mpg\nmake\nmodel\ntransmission\nyear\n\n\n\n\n0\n0\n18\n2\n21\n4.0\n2.2\n2\n2\n26\n24\n33\n0\n1993\n\n\n1\n1\n19\n2\n22\n4.0\n2.2\n2\n2\n27\n24\n33\n1\n1993\n\n\n2\n2\n16\n2\n19\n6.0\n3.0\n2\n2\n22\n24\n33\n0\n1993\n\n\n3\n3\n16\n2\n18\n6.0\n3.0\n2\n2\n22\n24\n33\n1\n1993\n\n\n4\n4\n18\n4\n21\n4.0\n2.2\n2\n2\n26\n24\n33\n0\n1993\n\n\n\n\n\n\n\n\n\n# Dropping rows from the DataFrame where 'fuel_type' is 'Diesel'\ndf = df[df['fuel_type'] != 0]\n\n# Display the resulting DataFrame\ndf\n\n\n\n\n\n\n\n\n\nUnnamed: 0\ncity_mpg\nclass\ncombination_mpg\ncylinders\ndisplacement\ndrive\nfuel_type\nhighway_mpg\nmake\nmodel\ntransmission\nyear\n\n\n\n\n0\n0\n18\n2\n21\n4.000000\n2.200000\n2\n2\n26\n24\n33\n0\n1993\n\n\n1\n1\n19\n2\n22\n4.000000\n2.200000\n2\n2\n27\n24\n33\n1\n1993\n\n\n2\n2\n16\n2\n19\n6.000000\n3.000000\n2\n2\n22\n24\n33\n0\n1993\n\n\n3\n3\n16\n2\n18\n6.000000\n3.000000\n2\n2\n22\n24\n33\n1\n1993\n\n\n4\n4\n18\n4\n21\n4.000000\n2.200000\n2\n2\n26\n24\n33\n0\n1993\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n714\n714\n80\n8\n76\n5.154622\n2.760336\n0\n1\n72\n9\n60\n0\n2021\n\n\n715\n715\n89\n8\n85\n5.154622\n2.760336\n0\n1\n82\n9\n60\n0\n2023\n\n\n716\n716\n79\n8\n76\n5.154622\n2.760336\n0\n1\n72\n9\n60\n0\n2023\n\n\n717\n717\n74\n12\n70\n5.154622\n2.760336\n0\n1\n66\n20\n102\n0\n2022\n\n\n718\n718\n73\n13\n69\n5.154622\n2.760336\n0\n1\n65\n20\n101\n0\n2022\n\n\n\n\n714 rows × 13 columns\n\n\n\n\n\n# LOOK AT FIRST ROW\nprint(df.iloc[0])\n\nUnnamed: 0            0.0\ncity_mpg             18.0\nclass                 2.0\ncombination_mpg      21.0\ncylinders             4.0\ndisplacement          2.2\ndrive                 2.0\nfuel_type             2.0\nhighway_mpg          26.0\nmake                 24.0\nmodel                33.0\ntransmission          0.0\nyear               1993.0\nName: 0, dtype: float64\n\n\n\n# INSERT CODE TO PRINT ITS SHAPE AND COLUMN NAMES\n\nprint(df.shape)\ndf.columns\n\n(714, 13)\n\n\nIndex(['Unnamed: 0', 'city_mpg', 'class', 'combination_mpg', 'cylinders',\n       'displacement', 'drive', 'fuel_type', 'highway_mpg', 'make', 'model',\n       'transmission', 'year'],\n      dtype='object')\n\n\n\n#INSERT CODE TO PRINT THE FOLLOWING DATA-FRAME WHICH SUMMARIZES EACH COLUMN \n\nsummary = df.describe().T  \nsummary['dtypes'] = df.dtypes \nsummary = summary[['dtypes', 'min', 'mean', 'max']]  \nprint(summary)\n\n                  dtypes     min         mean     max\nUnnamed: 0         int64     0.0   358.282913   718.0\ncity_mpg           int64     9.0    34.099440   150.0\nclass              int64     0.0     7.614846    15.0\ncombination_mpg    int64    11.0    35.127451   136.0\ncylinders        float64     3.0     5.159906    10.0\ndisplacement     float64     1.4     2.765100     6.2\ndrive              int64     0.0     1.813725     3.0\nfuel_type          int64     1.0     1.833333     2.0\nhighway_mpg        int64    12.0    37.345938   123.0\nmake               int64     0.0    13.113445    26.0\nmodel              int64     0.0    74.015406   150.0\ntransmission       int64     0.0     0.218487     1.0\nyear               int64  1984.0  2006.728291  2023.0\n\n\n\n# Count the number of samples for each target value\n# resource: https://pandas.pydata.org/docs/reference/api/pandas.Series.value_counts.html\n\n\ntarget_counts = df['fuel_type'].value_counts(normalize=True)  # normalize=True to get the proportions\n\n# Print the results\nfor target, count in target_counts.items():\n    print(f\"Number of points with target={target}: {count * len(df)} {count}\")\n\nNumber of points with target=2: 595.0 0.8333333333333334\nNumber of points with target=1: 119.0 0.16666666666666666\n\n\n\n# RUN THE FOLLOWING CODE TO SHOW THE HEAT-MAP FOR THE CORRELATION MATRIX\ncorr = df.corr();  #print(corr)                 #COMPUTE CORRELATION OF FEATER MATRIX\nprint(corr.shape)\nsns.set_theme(style=\"white\")\nf, ax = plt.subplots(figsize=(20, 20))  # Set up the matplotlib figure\ncmap = sns.diverging_palette(230, 20, as_cmap=True)     # Generate a custom diverging colormap\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr,  cmap=cmap, vmin=-1, vmax=1, center=0,\n        square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\nplt.show();\n\n(13, 13)\n\n\n\n\n\n\n\n\n\n\n# INSERT CODE TO MAKE DATA-FRAMES (or numpy arrays) (X,Y) WHERE Y=\"target\" COLUMN and X=\"everything else\"\n# Resource used: https://medium.com/codex/how-to-set-x-and-y-in-pandas-3f38584e9bed\n\n\nX = df.drop('fuel_type', axis=1)  # X includes everything except the target column\ny = df['fuel_type']  # Y is just the target column\n\n\nfrom collections import Counter\nimport numpy as np\nimport random\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\n\ndef generate_label_data(class_labels, weights, N=10000):\n    # Generate random labels based on the given class weights\n    y = random.choices(class_labels, weights=weights, k=N)\n    print(\"-----GENERATING DATA-----\")\n    print(\"unique entries:\", Counter(y).keys())\n    print(\"count of labels:\", Counter(y).values())\n    print(\"probability of labels:\", np.array(list(Counter(y).values())) / N)\n    return y\n\ndef random_classifier(y_data, class_labels, weights):\n    # Generate random predictions based on the distribution of class labels\n    ypred = random.choices(class_labels, weights=weights, k=len(y_data))\n    \n    print(\"-----RANDOM CLASSIFIER-----\")\n    print(\"count of prediction:\", Counter(ypred).values())\n    print(\"probability of prediction:\", np.array(list(Counter(ypred).values())) / len(y_data))\n    print(\"accuracy\", accuracy_score(y_data, ypred))\n    precision, recall, fscore, _ = precision_recall_fscore_support(y_data, ypred, average=None, labels=class_labels)\n    print(\"precision, recall, fscore:\", list(zip(precision, recall, fscore)))\n\n\n# INSERT CODE TO PARTITION THE DATASET INTO TRAINING AND TEST SETS\n\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n\n\n# INSERT CODE, AS A CONSISTENCY CHECK, TO PRINT THE TYPE AND SHAPE OF x_train, x_test, y_train, y_test\n# Changed the output structure for better understanding\n\nprint(\"TRAINING SHAPES: \\n\", f\"x_train:\\n shape: {x_train.shape}\\n type: {type(x_train)}\\n\", f\"y_train:\\n shape: {y_train.shape}\\n type: {type(y_train)}\\n\")\nprint(\"TEST SHAPES: \\n\", f\"x_test:\\n shape: {x_test.shape}\\n type: {type(x_test)}\\n\", f\"y_test:\\n shape: {y_test.shape}\\n type: {type(y_test)}\\n\")\n\nTRAINING SHAPES: \n x_train:\n shape: (571, 12)\n type: &lt;class 'pandas.core.frame.DataFrame'&gt;\n y_train:\n shape: (571,)\n type: &lt;class 'pandas.core.series.Series'&gt;\n\nTEST SHAPES: \n x_test:\n shape: (143, 12)\n type: &lt;class 'pandas.core.frame.DataFrame'&gt;\n y_test:\n shape: (143,)\n type: &lt;class 'pandas.core.series.Series'&gt;\n\n\n\n\n# # INSERT CODE TO USE THE MODEL TO MAKE PREDICTIONS FOR THE TRAINING AND TEST SET \n# yp_train = model.predict(x_train)\n# yp_test = model.predict(x_test)\n\n\n#INSERT CODE TO WRITE A FUNCTION def confusion_plot(y_data,y_pred) WHICH GENERATES A CONFUSION MATRIX PLOT AND PRINTS THE INFORMATION ABOVE (see link above for example)\n\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_classification\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\n\ndef confusion_plot(y_data, y_pred):\n\n    cm = confusion_matrix(y_data, y_pred)\n\n    # Calculate metrics\n    accuracy = accuracy_score(y_data, y_pred)\n    negative_recall = recall_score(y_data, y_pred)\n    negative_precision = precision_score(y_data, y_pred)\n    positive_recall = recall_score(y_data, y_pred)\n    positive_precision = precision_score(y_data, y_pred)\n    \n    # Print metrics\n    print(f\"ACCURACY: {accuracy}\")\n    print(f\"NEGATIVE RECALL (Y=0): {negative_recall}\")\n    print(f\"NEGATIVE PRECISION (Y=0): {negative_precision}\")\n    print(f\"POSITIVE RECALL (Y=1): {positive_recall}\")\n    print(f\"POSITIVE PRECISION (Y=1): {positive_precision}\")\n    print(cm)\n    \n    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n    \n    disp.plot()\n    plt.show()\n\n\n# RUN THE FOLLOWING CODE TO TEST YOUR FUNCTION \nprint(\"------TRAINING------\")\nconfusion_plot(y_train,yp_train)\nprint(\"------TEST------\")\nconfusion_plot(y_test,yp_test)\n\n------TRAINING------\nACCURACY: 1.0\nNEGATIVE RECALL (Y=0): 1.0\nNEGATIVE PRECISION (Y=0): 1.0\nPOSITIVE RECALL (Y=1): 1.0\nPOSITIVE PRECISION (Y=1): 1.0\n[[ 97   0]\n [  0 474]]\n------TEST------\nACCURACY: 1.0\nNEGATIVE RECALL (Y=0): 1.0\nNEGATIVE PRECISION (Y=0): 1.0\nPOSITIVE RECALL (Y=1): 1.0\nPOSITIVE PRECISION (Y=1): 1.0\n[[ 22   0]\n [  0 121]]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom sklearn import tree\nimport matplotlib.pyplot as plt\n\ndef plot_tree(model, X, Y):\n    # Determine class names if Y is categorical and has more than one unique value\n    class_names = np.unique(Y).astype(str).tolist() if len(np.unique(Y)) &gt; 1 else None\n\n    plt.figure(figsize=(20, 10))\n    tree.plot_tree(model,\n                   feature_names=X.columns.tolist(),\n                   class_names=class_names,\n                   filled=True)\n    plt.show()\n\n\nplot_tree(model,X,y)\n\n\n\n\n\n\n\n\n\n# COMPLETE THE FOLLOWING CODE TO LOOP OVER POSSIBLE HYPER-PARAMETERS VALUES\ntest_results=[]\ntrain_results=[]\n\nfor num_layer in range(1,20):\n    model = tree.DecisionTreeClassifier(max_depth=num_layer)\n    model = model.fit(x_train, y_train)\n\n    yp_train=model.predict(x_train)\n    yp_test=model.predict(x_test)\n\n    # print(y_pred.shape)\n    test_results.append([num_layer,accuracy_score(y_test, yp_test),recall_score(y_test, yp_test,pos_label=1),recall_score(y_test, yp_test,pos_label=2)])\n    train_results.append([num_layer,accuracy_score(y_train, yp_train),recall_score(y_train, yp_train,pos_label=1),recall_score(y_train, yp_train,pos_label=2)])\n\ntest_results=np.array(test_results)\ntrain_results=np.array(train_results)\ncol=1\nplt.plot (test_results[:,0],test_results[:,col],'-or')\nplt.plot(train_results[:,0],train_results[: ,col],'-ob')\nplt.xlabel('Number of layers in decision tree (max_depth)') \nplt.ylabel ('ACCURACY (Y=0): Training (blue) and Test (red)') \nplt. show()\n\ncol=2\nplt.plot (test_results[:,0],test_results[:,col],'-or')\nplt.plot(train_results[:,0],train_results[: ,col],'-ob')\nplt.xlabel('Number of layers in decision tree (max_depth)') \nplt.ylabel ('RECALL (Y=0): Training (blue) and Test (red)') \nplt. show()\n\ncol=3\nplt.plot (test_results[:,0],test_results[:,col],'-or')\nplt.plot(train_results[:,0],train_results[: ,col],'-ob')\nplt.xlabel('Number of layers in decision tree (max_depth)') \nplt.ylabel ('RECALL (Y=1): Training (blue) and Test (red)') \nplt. show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#### COMPLETE THE CODE BELOW TO TRAIN A SKLEARN DECISION TREE MODEL ON x_train,y_train \nfrom sklearn import tree\nmodel = tree.DecisionTreeClassifier(max_depth=3)\nmodel = model.fit(x_train, y_train)\n\nyp_train=model.predict(x_train)\nyp_test=model.predict(x_test)\n\n\n# RUN THE FOLLOWING CODE TO EVALUATE YOUR MODEL\nprint(\"------TRAINING------\")\nconfusion_plot(y_train,yp_train)\nprint(\"------TEST------\")\nconfusion_plot(y_test,yp_test)\n\nplot_tree(model,X,y)\n\n------TRAINING------\nACCURACY: 1.0\nNEGATIVE RECALL (Y=0): 1.0\nNEGATIVE PRECISION (Y=0): 1.0\nPOSITIVE RECALL (Y=1): 1.0\nPOSITIVE PRECISION (Y=1): 1.0\n[[ 97   0]\n [  0 474]]\n------TEST------\nACCURACY: 1.0\nNEGATIVE RECALL (Y=0): 1.0\nNEGATIVE PRECISION (Y=0): 1.0\nPOSITIVE RECALL (Y=1): 1.0\nPOSITIVE PRECISION (Y=1): 1.0\n[[ 22   0]\n [  0 121]]"
  },
  {
    "objectID": "5000-website/decision-tree/reference-notebooks/classification copy.html",
    "href": "5000-website/decision-tree/reference-notebooks/classification copy.html",
    "title": "Classification",
    "section": "",
    "text": "import pandas as pd\nimport seaborn as sns \nimport matplotlib.pyplot as plt\nfrom sklearn import tree\nfrom IPython.display import Image\nimport numpy as np\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\n\n\ndf = pd.read_csv('../eda/cars-data.csv')\n\ndf.head()\n\n\n\n\n\n\n\n\n\nUnnamed: 0\ncity_mpg\nclass\ncombination_mpg\ncylinders\ndisplacement\ndrive\nfuel_type\nhighway_mpg\nmake\nmodel\ntransmission\nyear\n\n\n\n\n0\n0\n18\nmidsize car\n21\n4.0\n2.2\nfwd\ngas\n26\ntoyota\nCamry\na\n1993\n\n\n1\n1\n19\nmidsize car\n22\n4.0\n2.2\nfwd\ngas\n27\ntoyota\nCamry\nm\n1993\n\n\n2\n2\n16\nmidsize car\n19\n6.0\n3.0\nfwd\ngas\n22\ntoyota\nCamry\na\n1993\n\n\n3\n3\n16\nmidsize car\n18\n6.0\n3.0\nfwd\ngas\n22\ntoyota\nCamry\nm\n1993\n\n\n4\n4\n18\nmidsize-large station wagon\n21\n4.0\n2.2\nfwd\ngas\n26\ntoyota\nCamry\na\n1993\n\n\n\n\n\n\n\n\n\nnan_count = df.isna().sum()\n\nprint(nan_count)\n\nUnnamed: 0           0\ncity_mpg             0\nclass                0\ncombination_mpg      0\ncylinders          124\ndisplacement       124\ndrive                8\nfuel_type            0\nhighway_mpg          0\nmake                 0\nmodel                0\ntransmission         0\nyear                 0\ndtype: int64\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 719 entries, 0 to 718\nData columns (total 13 columns):\n #   Column           Non-Null Count  Dtype  \n---  ------           --------------  -----  \n 0   Unnamed: 0       719 non-null    int64  \n 1   city_mpg         719 non-null    int64  \n 2   class            719 non-null    object \n 3   combination_mpg  719 non-null    int64  \n 4   cylinders        595 non-null    float64\n 5   displacement     595 non-null    float64\n 6   drive            711 non-null    object \n 7   fuel_type        719 non-null    object \n 8   highway_mpg      719 non-null    int64  \n 9   make             719 non-null    object \n 10  model            719 non-null    object \n 11  transmission     719 non-null    object \n 12  year             719 non-null    int64  \ndtypes: float64(2), int64(5), object(6)\nmemory usage: 73.2+ KB\n\n\n\n# Convert all 'object' type columns to 'string'\nfor col in df.select_dtypes(include=['object']).columns:\n    df[col] = df[col].astype('string')\n\n# Verify the changes\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 719 entries, 0 to 718\nData columns (total 13 columns):\n #   Column           Non-Null Count  Dtype  \n---  ------           --------------  -----  \n 0   Unnamed: 0       719 non-null    int64  \n 1   city_mpg         719 non-null    int64  \n 2   class            719 non-null    string \n 3   combination_mpg  719 non-null    int64  \n 4   cylinders        595 non-null    float64\n 5   displacement     595 non-null    float64\n 6   drive            711 non-null    string \n 7   fuel_type        719 non-null    string \n 8   highway_mpg      719 non-null    int64  \n 9   make             719 non-null    string \n 10  model            719 non-null    string \n 11  transmission     719 non-null    string \n 12  year             719 non-null    int64  \ndtypes: float64(2), int64(5), string(6)\nmemory usage: 73.2 KB\n\n\n\n# get y \n\ny = df['fuel_type'].unique().tolist()\n\n# Replace 'diesel' with 'gas' in the entire DataFrame\ndf.replace('diesel', 'gas', inplace=True)\n\ny = df['fuel_type'].unique().tolist()\n\ny\n\n['gas', 'electricity']\n\n\n\n\n# Dropping non-numerical and unnecessary columns\ndf = df.drop(columns=['Unnamed: 0'])\n\n\n# Replace continuous missing values with mean of the column. check for Nan values again.\n\ncols = ['displacement', 'cylinders']\ndf[cols] = df[cols].fillna(df[cols].mean())\n\nnan_count = df.isna().sum()\nprint(nan_count)\n\ncity_mpg           0\nclass              0\ncombination_mpg    0\ncylinders          0\ndisplacement       0\ndrive              8\nfuel_type          0\nhighway_mpg        0\nmake               0\nmodel              0\ntransmission       0\nyear               0\ndtype: int64\n\n\n\n# Replace categorical missing values with mode of the column. check for Nan values again.\n\ndf['drive'] = df['drive'].fillna(df['drive'].mode().iloc[0])\n\nnan_count = df.isna().sum()\nprint(nan_count)\n\ncity_mpg           0\nclass              0\ncombination_mpg    0\ncylinders          0\ndisplacement       0\ndrive              0\nfuel_type          0\nhighway_mpg        0\nmake               0\nmodel              0\ntransmission       0\nyear               0\ndtype: int64\n\n\n\n# Using a for loop to replace categorical values with cat codes\ncat_cols = ['class', 'drive', 'fuel_type', 'make', 'model', 'transmission']\nfor col in cat_cols:\n    df[col] = df[col].astype('category')\n    df[col] = df[col].cat.codes\n\n# Display the altered DataFrame\ndf.head()\n\n\n\n\n\n\n\n\n\ncity_mpg\nclass\ncombination_mpg\ncylinders\ndisplacement\ndrive\nfuel_type\nhighway_mpg\nmake\nmodel\ntransmission\nyear\n\n\n\n\n0\n18\n2\n21\n4.0\n2.2\n2\n1\n26\n24\n33\n0\n1993\n\n\n1\n19\n2\n22\n4.0\n2.2\n2\n1\n27\n24\n33\n1\n1993\n\n\n2\n16\n2\n19\n6.0\n3.0\n2\n1\n22\n24\n33\n0\n1993\n\n\n3\n16\n2\n18\n6.0\n3.0\n2\n1\n22\n24\n33\n1\n1993\n\n\n4\n18\n4\n21\n4.0\n2.2\n2\n1\n26\n24\n33\n0\n1993\n\n\n\n\n\n\n\n\n\n# LOOK AT FIRST ROW\nprint(df.iloc[0])\n\ncity_mpg             18.0\nclass                 2.0\ncombination_mpg      21.0\ncylinders             4.0\ndisplacement          2.2\ndrive                 2.0\nfuel_type             1.0\nhighway_mpg          26.0\nmake                 24.0\nmodel                33.0\ntransmission          0.0\nyear               1993.0\nName: 0, dtype: float64\n\n\n\n# INSERT CODE TO PRINT COLUMN NAMES\ndf.columns\n\nIndex(['city_mpg', 'class', 'combination_mpg', 'cylinders', 'displacement',\n       'drive', 'fuel_type', 'highway_mpg', 'make', 'model', 'transmission',\n       'year'],\n      dtype='object')\n\n\n\n#INSERT CODE TO PRINT THE FOLLOWING DATA-FRAME WHICH SUMMARIZES EACH COLUMN \n\nsummary = df.describe().T  \nsummary['dtypes'] = df.dtypes \nsummary = summary[['dtypes', 'min', 'mean', 'max']]  \nprint(summary)\n\n                  dtypes     min         mean     max\ncity_mpg           int64     9.0    34.051460   150.0\nclass               int8     0.0     7.634214    15.0\ncombination_mpg    int64    11.0    35.094576   136.0\ncylinders        float64     3.0     5.154622    10.0\ndisplacement     float64     1.4     2.760336     6.2\ndrive               int8     0.0     1.815021     3.0\nfuel_type           int8     0.0     0.827538     1.0\nhighway_mpg        int64    12.0    37.329624   123.0\nmake                int8     0.0    13.137691    26.0\nmodel              int16     0.0    73.645341   150.0\ntransmission        int8     0.0     0.219750     1.0\nyear               int64  1984.0  2006.634214  2023.0\n\n\n\n# Count the number of samples for each target value\n# resource: https://pandas.pydata.org/docs/reference/api/pandas.Series.value_counts.html\n\n\ntarget_counts = df['fuel_type'].value_counts(normalize=True)  # normalize=True to get the proportions\n\n# Print the results\nfor target, count in target_counts.items():\n    print(f\"Number of points with target={target}: {count * len(df)} {count}\")\n\nNumber of points with target=1: 595.0 0.827538247566064\nNumber of points with target=0: 124.0 0.17246175243393602\n\n\n\n# RUN THE FOLLOWING CODE TO SHOW THE HEAT-MAP FOR THE CORRELATION MATRIX\ncorr = df.corr();  #print(corr)                 #COMPUTE CORRELATION OF FEATER MATRIX\nprint(corr.shape)\nsns.set_theme(style=\"white\")\nf, ax = plt.subplots(figsize=(20, 20))  # Set up the matplotlib figure\ncmap = sns.diverging_palette(230, 20, as_cmap=True)     # Generate a custom diverging colormap\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr,  cmap=cmap, vmin=-1, vmax=1, center=0,\n        square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\nplt.show();\n\n(12, 12)\n\n\n\n\n\n\n\n\n\n\n# # # RUN THE FOLLOWING CODE TO GENERATE A SEABORN PAIRPLOT \n# tmp=pd.concat([df.sample(n=10,axis=1),y],axis=1)\n# print(tmp.shape)\n# sns.pairplot(tmp,hue=\"fuel_type\", diag_kind='kde')\n# plt.show()\n\n\n# INSERT CODE TO MAKE DATA-FRAMES (or numpy arrays) (X,Y) WHERE Y=\"target\" COLUMN and X=\"everything else\"\n# Resource used: https://medium.com/codex/how-to-set-x-and-y-in-pandas-3f38584e9bed\n\n\nX = df.drop('fuel_type', axis=1)  # X includes everything except the target column\ny = df['fuel_type']  # Y is just the target column\n\n\n# INSERT CODE TO PARTITION THE DATASET INTO TRAINING AND TEST SETS\n\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n\n\n# INSERT CODE, AS A CONSISTENCY CHECK, TO PRINT THE TYPE AND SHAPE OF x_train, x_test, y_train, y_test\n# Changed the output structure for better understanding\n\nprint(\"TRAINING SHAPES: \\n\", f\"x_train:\\n shape: {x_train.shape}\\n type: {type(x_train)}\\n\", f\"y_train:\\n shape: {y_train.shape}\\n type: {type(y_train)}\\n\")\nprint(\"TEST SHAPES: \\n\", f\"x_test:\\n shape: {x_test.shape}\\n type: {type(x_test)}\\n\", f\"y_test:\\n shape: {y_test.shape}\\n type: {type(y_test)}\\n\")\n\nTRAINING SHAPES: \n x_train:\n shape: (575, 11)\n type: &lt;class 'pandas.core.frame.DataFrame'&gt;\n y_train:\n shape: (575,)\n type: &lt;class 'pandas.core.series.Series'&gt;\n\nTEST SHAPES: \n x_test:\n shape: (144, 11)\n type: &lt;class 'pandas.core.frame.DataFrame'&gt;\n y_test:\n shape: (144,)\n type: &lt;class 'pandas.core.series.Series'&gt;\n\n\n\n\n#### INSERT CODE BELOW TO TRAIN A SKLEARN DECISION TREE MODEL ON x_train,y_train \nfrom sklearn import tree\n\nmodel = tree.DecisionTreeClassifier()\nmodel = model.fit(x_train, y_train)\n\n\n# INSERT CODE TO USE THE MODEL TO MAKE PREDICTIONS FOR THE TRAINING AND TEST SET \nyp_train = model.predict(x_train)\nyp_test = model.predict(x_test)\n\n\n#INSERT CODE TO WRITE A FUNCTION def confusion_plot(y_data,y_pred) WHICH GENERATES A CONFUSION MATRIX PLOT AND PRINTS THE INFORMATION ABOVE (see link above for example)\n\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_classification\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\n\ndef confusion_plot(y_data, y_pred):\n\n    cm = confusion_matrix(y_data, y_pred)\n\n    # Calculate metrics\n    accuracy = accuracy_score(y_data, y_pred)\n    negative_recall = recall_score(y_data, y_pred)\n    negative_precision = precision_score(y_data, y_pred)\n    positive_recall = recall_score(y_data, y_pred)\n    positive_precision = precision_score(y_data, y_pred)\n    \n    # Print metrics\n    print(f\"ACCURACY: {accuracy}\")\n    print(f\"NEGATIVE RECALL (Y=0): {negative_recall}\")\n    print(f\"NEGATIVE PRECISION (Y=0): {negative_precision}\")\n    print(f\"POSITIVE RECALL (Y=1): {positive_recall}\")\n    print(f\"POSITIVE PRECISION (Y=1): {positive_precision}\")\n    print(cm)\n    \n    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n    \n    disp.plot()\n    plt.show()\n\n\n# RUN THE FOLLOWING CODE TO TEST YOUR FUNCTION \nprint(\"------TRAINING------\")\nconfusion_plot(y_train,yp_train)\nprint(\"------TEST------\")\nconfusion_plot(y_test,yp_test)\n\n------TRAINING------\nACCURACY: 1.0\nNEGATIVE RECALL (Y=0): 1.0\nNEGATIVE PRECISION (Y=0): 1.0\nPOSITIVE RECALL (Y=1): 1.0\nPOSITIVE PRECISION (Y=1): 1.0\n[[103   0]\n [  0 472]]\n------TEST------\nACCURACY: 1.0\nNEGATIVE RECALL (Y=0): 1.0\nNEGATIVE PRECISION (Y=0): 1.0\nPOSITIVE RECALL (Y=1): 1.0\nPOSITIVE PRECISION (Y=1): 1.0\n[[ 21   0]\n [  0 123]]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom sklearn import tree\nimport matplotlib.pyplot as plt\n\ndef plot_tree(model, X, Y):\n    # Determine class names if Y is categorical and has more than one unique value\n    class_names = np.unique(Y).astype(str).tolist() if len(np.unique(Y)) &gt; 1 else None\n\n    plt.figure(figsize=(20, 10))\n    tree.plot_tree(model,\n                   feature_names=X.columns.tolist(),\n                   class_names=class_names,\n                   filled=True)\n    plt.show()\n\n\nplot_tree(model,X,y)\n\n\n\n\n\n\n\n\n\n# COMPLETE THE FOLLOWING CODE TO LOOP OVER POSSIBLE HYPER-PARAMETERS VALUES\ntest_results=[]\ntrain_results=[]\n\nfor num_layer in range(1,20):\n    model = tree.DecisionTreeClassifier(max_depth=num_layer)\n    model = model.fit(x_train, y_train)\n\n    yp_train=model.predict(x_train)\n    yp_test=model.predict(x_test)\n\n    # print(y_pred.shape)\n    test_results.append([num_layer,accuracy_score(y_test, yp_test),recall_score(y_test, yp_test,pos_label=0),recall_score(y_test, yp_test,pos_label=1)])\n    train_results.append([num_layer,accuracy_score(y_train, yp_train),recall_score(y_train, yp_train,pos_label=0),recall_score(y_train, yp_train,pos_label=1)])\n\ntest_results=np.array(test_results)\ntrain_results=np.array(train_results)\ncol=1\nplt.plot (test_results[:,0],test_results[:,col],'-or')\nplt.plot(train_results[:,0],train_results[: ,col],'-ob')\nplt.xlabel('Number of layers in decision tree (max_depth)') \nplt.ylabel ('ACCURACY (Y=0): Training (blue) and Test (red)') \nplt. show()\n\ncol=2\nplt.plot (test_results[:,0],test_results[:,col],'-or')\nplt.plot(train_results[:,0],train_results[: ,col],'-ob')\nplt.xlabel('Number of layers in decision tree (max_depth)') \nplt.ylabel ('RECALL (Y=0): Training (blue) and Test (red)') \nplt. show()\n\ncol=3\nplt.plot (test_results[:,0],test_results[:,col],'-or')\nplt.plot(train_results[:,0],train_results[: ,col],'-ob')\nplt.xlabel('Number of layers in decision tree (max_depth)') \nplt.ylabel ('RECALL (Y=1): Training (blue) and Test (red)') \nplt. show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#### COMPLETE THE CODE BELOW TO TRAIN A SKLEARN DECISION TREE MODEL ON x_train,y_train \nfrom sklearn import tree\nmodel = tree.DecisionTreeClassifier(max_depth=2)\nmodel = model.fit(x_train, y_train)\n\nyp_train=model.predict(x_train)\nyp_test=model.predict(x_test)\n\n\n# RUN THE FOLLOWING CODE TO EVALUATE YOUR MODEL\nprint(\"------TRAINING------\")\nconfusion_plot(y_train,yp_train)\nprint(\"------TEST------\")\nconfusion_plot(y_test,yp_test)\n\nplot_tree(model,X,y)\n\n------TRAINING------\nACCURACY: 1.0\nNEGATIVE RECALL (Y=0): 1.0\nNEGATIVE PRECISION (Y=0): 1.0\nPOSITIVE RECALL (Y=1): 1.0\nPOSITIVE PRECISION (Y=1): 1.0\n[[103   0]\n [  0 472]]\n------TEST------\nACCURACY: 1.0\nNEGATIVE RECALL (Y=0): 1.0\nNEGATIVE PRECISION (Y=0): 1.0\nPOSITIVE RECALL (Y=1): 1.0\nPOSITIVE PRECISION (Y=1): 1.0\n[[ 21   0]\n [  0 123]]"
  },
  {
    "objectID": "5000-website/decision-tree/reference-notebooks/stock_regression.html",
    "href": "5000-website/decision-tree/reference-notebooks/stock_regression.html",
    "title": "Import",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport numpy as np\nfrom sklearn.metrics import accuracy_score"
  },
  {
    "objectID": "5000-website/decision-tree/reference-notebooks/stock_regression.html#read-and-re-format",
    "href": "5000-website/decision-tree/reference-notebooks/stock_regression.html#read-and-re-format",
    "title": "Import",
    "section": "Read and re-format",
    "text": "Read and re-format\n\n#RELOAD FILE AND PRETEND THAT IS OUR STARTING POINT \ndf = pd.read_csv('../eda/stock.csv')  \n\n\nprint(df.head())\nprint(df.shape)\n\n        STLA         GM          TM      NSANY      MBGYY      BMWYY  \\\n0  11.472709  37.606457  128.369995  20.040001  15.427057  25.132998   \n1  11.945812  38.524128  130.130005  20.250000  15.544848  25.270378   \n2  12.854664  39.711697  132.160004  20.230000  15.751436  25.443905   \n3  13.551867  39.594738  133.860001  20.389999  15.934467  25.740355   \n4  13.433592  39.783672  134.770004  20.440001  16.003330  25.913887   \n\n      POAHY       TSLA       Dates  \n0  6.742718  21.368668  2018-01-02  \n1  6.887550  21.150000  2018-01-03  \n2  7.072613  20.974667  2018-01-04  \n3  7.145028  21.105333  2018-01-05  \n4  7.161120  22.427334  2018-01-08  \n(1478, 9)\n\n\n\ndf = df[['POAHY', 'TSLA']]\n\n\ndf.head()\n\n\n\n\n\n\n\n\n\nPOAHY\nTSLA\n\n\n\n\n0\n6.742718\n21.368668\n\n\n1\n6.887550\n21.150000\n\n\n2\n7.072613\n20.974667\n\n\n3\n7.145028\n21.105333\n\n\n4\n7.161120\n22.427334\n\n\n\n\n\n\n\n\n\n# Converting X and y to 2D arrays by using a list of lists\nX = df.iloc[:, [0]].values  # Selects all rows and only the first column and keeps as 2D\ny = df.iloc[:, [1]].values  # Selects all rows and only the second column and keeps as 2D\n\nX, y\n\n(array([[6.74271774],\n        [6.88754988],\n        [7.07261276],\n        ...,\n        [4.6500001 ],\n        [4.67000008],\n        [4.90999985]]),\n array([[ 21.3686676 ],\n        [ 21.14999962],\n        [ 20.9746666 ],\n        ...,\n        [214.6499939 ],\n        [223.71000671],\n        [237.41000366]]))\n\n\n\nprint(X.shape,y.shape)\n\n(1478, 1) (1478, 1)\n\n\n\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n\nprint(\"TRAINING SHAPES:\",x_train.shape,y_train.shape)\nprint(\"TEST SHAPES:\",x_test.shape,y_test.shape)\n\nTRAINING SHAPES: (1034, 1) (1034, 1)\nTEST SHAPES: (444, 1) (444, 1)\n\n\n\n# # Plot the initial data\nplt.figure()\nplt.scatter(x_train, y_train, s=20, edgecolor=\"black\", c=\"darkorange\", label=\"training\")\nplt.scatter(x_test, y_test, s=20, edgecolor=\"black\", c=\"b\", label=\"test\")\nplt.xlabel(\"data\")\nplt.ylabel(\"target\")\nplt.title(\"Decision Tree Regression\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom sklearn.tree import DecisionTreeRegressor\n\nregr_1 = DecisionTreeRegressor(max_depth=2)\nregr_2 = DecisionTreeRegressor(max_depth=4)\nregr_3 = DecisionTreeRegressor(max_depth=50)\n\nregr_1.fit(x_train, y_train)\nregr_2.fit(x_train, y_train)\nregr_3.fit(x_train, y_train)\n\n\ny_1 = regr_1.predict(sorted(x_test))\ny_2 = regr_2.predict(sorted(x_test))\ny_3 = regr_3.predict(sorted(x_test))\n\n\nfrom sklearn.metrics import mean_absolute_error\nprint(\"MODEL-1: Training, test MAE:\",mean_absolute_error(y_train, regr_1.predict(x_train)),mean_absolute_error(y_test, regr_1.predict(x_test)))\nprint(\"MODEL-2: Training, test MAE:\",mean_absolute_error(y_train, regr_2.predict(x_train)),mean_absolute_error(y_test, regr_2.predict(x_test)))\nprint(\"MODEL-3: Training, test MAE:\",mean_absolute_error(y_train, regr_3.predict(x_train)),mean_absolute_error(y_test, regr_3.predict(x_test)))\n\n\n# from sklearn.metrics import mean_absolute_percentage_error\n# print(\"MODEL-1: Training, test MAPE:\",mean_absolute_percentage_error(y_train, regr_1.predict(x_train)),mean_absolute_percentage_error(y_test, regr_1.predict(x_test)))\n# print(\"MODEL-2: Training, test MAPE:\",mean_absolute_percentage_error(y_train, regr_2.predict(x_train)),mean_absolute_percentage_error(y_test, regr_2.predict(x_test)))\n# print(\"MODEL-3: Training, test MAPE:\",mean_absolute_percentage_error(y_train, regr_3.predict(x_train)),mean_absolute_percentage_error(y_test, regr_3.predict(x_test)))\n\n# from sklearn.metrics import mean_squared_error\n# print(\"MODEL-1: Training, test MSE:\",mean_squared_error(y_train, regr_1.predict(x_train)),mean_squared_error(y_test, regr_1.predict(x_test)))\n# print(\"MODEL-2: Training, test MSE:\",mean_squared_error(y_train, regr_2.predict(x_train)),mean_squared_error(y_test, regr_2.predict(x_test)))\n# print(\"MODEL-3: Training, test MSE:\",mean_squared_error(y_train, regr_3.predict(x_train)),mean_squared_error(y_test, regr_3.predict(x_test)))\n\nMODEL-1: Training, test MAE: 80.82229323529546 83.2379992411983\nMODEL-2: Training, test MAE: 77.02593400309358 80.0420082334354\nMODEL-3: Training, test MAE: 7.291053594189737 60.199355760115694\n\n\n\nplt.figure()\nplt.scatter(x_train, y_train, s=20, edgecolor=\"black\", c=\"darkorange\", label=\"training\")\nplt.plot(sorted(x_train), regr_1.predict(sorted(x_train)), color=\"b\", label=\"max_depth=2\", linewidth=2)\nplt.plot(sorted(x_train), regr_2.predict(sorted(x_train)), color=\"yellowgreen\", label=\"max_depth=4\", linewidth=2)\nplt.plot(sorted(x_train), regr_3.predict(sorted(x_train)), color=\"cornflowerblue\", label=\"no-pruning\", linewidth=2)\n\nplt.xlabel(\"data\")\nplt.ylabel(\"target\")\nplt.title(\"Decision Tree Regression\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nplt.figure()\nplt.scatter(x_train, y_train, s=20, edgecolor=\"black\", c=\"darkorange\", label=\"training\")\nplt.scatter(x_test, y_test, s=20, edgecolor=\"black\", c=\"b\", label=\"test\")\nplt.plot(sorted(x_test), y_1, color=\"b\", label=\"max_depth=2\", linewidth=2)\nplt.plot(sorted(x_test), y_2, color=\"yellowgreen\", label=\"max_depth=4\", linewidth=2)\nplt.plot(sorted(x_test), y_3, color=\"cornflowerblue\", label=\"no-pruning\", linewidth=2)\nplt.xlabel(\"data\")\nplt.ylabel(\"target\")\nplt.title(\"Decision Tree Regression\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom sklearn import tree\ndef plot_tree(model):\n    fig = plt.figure(figsize=(25,20))\n    _ = tree.plot_tree(model, \n                    filled=True)\n    plt.show()\n\nplot_tree(regr_1)\nplot_tree(regr_2)\nplot_tree(regr_3)"
  },
  {
    "objectID": "5000-website/decision-tree/reference-notebooks/classification1.html",
    "href": "5000-website/decision-tree/reference-notebooks/classification1.html",
    "title": "Isfar Baset",
    "section": "",
    "text": "import pandas as pd\n\n# Load the dataset\nfile_path = '../dim-reduction/cardf_cleaned.csv'\ndf = pd.read_csv(file_path)\n\n# Display the first few rows of the dataset to understand its structure\ndf.head()\n\n\n\n\n\n\n\n\n\nUnnamed: 0\ncity_mpg\nclass\ncombination_mpg\ncylinders\ndisplacement\ndrive\nfuel_type\nhighway_mpg\nmake\nmodel\ntransmission\nyear\n\n\n\n\n0\n0\n18\n2\n21\n4.0\n2.2\n1\n1\n26\n6\n3\n0\n1993\n\n\n1\n1\n19\n2\n22\n4.0\n2.2\n1\n1\n27\n6\n3\n1\n1993\n\n\n2\n2\n16\n2\n19\n6.0\n3.0\n1\n1\n22\n6\n3\n0\n1993\n\n\n3\n3\n16\n2\n18\n6.0\n3.0\n1\n1\n22\n6\n3\n1\n1993\n\n\n4\n4\n18\n3\n21\n4.0\n2.2\n1\n1\n26\n6\n3\n0\n1993\n\n\n\n\n\n\n\n\n\n# Compute the distribution of class labels in the dataset\nclass_distribution_cleaned = df['fuel_type'].value_counts(normalize=True)\n\nclass_distribution_cleaned\n\nfuel_type\n1    0.6875\n0    0.3125\nName: proportion, dtype: float64\n\n\n\n# Splitting the dataset into features (X) and target variable (y)\nX_cleaned = df.drop('fuel_type', axis=1)\ny_cleaned = df['fuel_type']\n\n# Splitting the data into train and test sets\nX_train_cleaned, X_test_cleaned, y_train_cleaned, y_test_cleaned = train_test_split(\n    X_cleaned, y_cleaned, test_size=0.2, random_state=42)\n\n# Generating random predictions based on class distribution\nrandom_predictions_cleaned = np.random.choice(class_distribution_cleaned.index, \n                                              size=len(y_test_cleaned), \n                                              p=class_distribution_cleaned.values)\n\n# Evaluating the baseline model\naccuracy_baseline = accuracy_score(y_test_cleaned, random_predictions_cleaned)\nprecision_baseline = precision_score(y_test_cleaned, random_predictions_cleaned, pos_label=0)\nrecall_baseline = recall_score(y_test_cleaned, random_predictions_cleaned, pos_label=0)\n\naccuracy_baseline, precision_baseline, recall_baseline\n\n(0.5, 0.2, 0.2)\n\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Correlation analysis\ncorrelation_matrix = df.corr()\n\n# Plotting the correlation matrix\nplt.figure(figsize=(12, 8))\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\nplt.title(\"Correlation Matrix of Features\")\nplt.show()\n\n\n\n\n\n\n\n\n\n# Feature importance from an initial Decision Tree model\nfrom sklearn import tree\n\ninitial_dt = tree.DecisionTreeClassifier(random_state=42)\ninitial_dt.fit(X_train_cleaned, y_train_cleaned)\nfeature_importance = initial_dt.feature_importances_\n\n# Displaying feature importance\nfeature_importance_df = pd.DataFrame({'Feature': X_train_cleaned.columns, 'Importance': feature_importance})\nfeature_importance_df.sort_values(by='Importance', ascending=False)\n\n\n\n\n\n\n\n\n\nFeature\nImportance\n\n\n\n\n7\nhighway_mpg\n1.0\n\n\n0\nUnnamed: 0\n0.0\n\n\n1\ncity_mpg\n0.0\n\n\n2\nclass\n0.0\n\n\n3\ncombination_mpg\n0.0\n\n\n4\ncylinders\n0.0\n\n\n5\ndisplacement\n0.0\n\n\n6\ndrive\n0.0\n\n\n8\nmake\n0.0\n\n\n9\nmodel\n0.0\n\n\n10\ntransmission\n0.0\n\n\n11\nyear\n0.0\n\n\n\n\n\n\n\n\n\n# Training the Decision Tree Classifier with the selected features\nselected_features = ['highway_mpg', 'city_mpg', 'cylinders', 'displacement', 'year'] # Example feature selection\nX_train_selected = X_train_cleaned[selected_features]\nX_test_selected = X_test_cleaned[selected_features]\n\n# Training the model\ndt_classifier_selected = tree.DecisionTreeClassifier(random_state=42)\ndt_classifier_selected.fit(X_train_selected, y_train_cleaned)\n\n# Making predictions on the test set\ny_pred_selected = dt_classifier_selected.predict(X_test_selected)\n\n# Evaluating the model\naccuracy_selected = accuracy_score(y_test_cleaned, y_pred_selected)\nprecision_selected = precision_score(y_test_cleaned, y_pred_selected, pos_label=0)\nrecall_selected = recall_score(y_test_cleaned, y_pred_selected, pos_label=0)\nclassification_report_selected = classification_report(y_test_cleaned, y_pred_selected)\n\naccuracy_selected, precision_selected, recall_selected, classification_report_selected\n\nNameError: name 'classification_report' is not defined\n\n\n\n# Correlation analysis\ncorrelation_matrix = df.corr()\n\n# Plotting the correlation matrix\nplt.figure(figsize=(12, 8))\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\nplt.title(\"Correlation Matrix\")\nplt.show()\n\n# Identifying features with the highest correlation to the target variable ('fuel_type')\ncorrelation_with_target = correlation_matrix['fuel_type'].sort_values(ascending=False)\n\ncorrelation_with_target\n\n\n\n\n\n\n\n\nfuel_type          1.000000e+00\ntransmission       3.762708e-01\nclass              3.035188e-01\ndrive              5.381653e-02\nUnnamed: 0         4.379399e-02\ncylinders          4.957269e-16\ndisplacement      -1.046619e-16\nmodel             -1.608799e-01\nmake              -2.917321e-01\nyear              -8.662596e-01\ncity_mpg          -9.503344e-01\ncombination_mpg   -9.603270e-01\nhighway_mpg       -9.674366e-01\nName: fuel_type, dtype: float64\n\n\n\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Preparing the features and target variable\nX = df.drop('fuel_type', axis=1)\ny = df['fuel_type']\n\n# Training a preliminary Decision Tree Classifier\ndt_preliminary = DecisionTreeClassifier(random_state=42)\ndt_preliminary.fit(X, y)\n\n# Extracting feature importance\nfeature_importance = pd.DataFrame({\n    'Feature': X.columns,\n    'Importance': dt_preliminary.feature_importances_\n}).sort_values(by='Importance', ascending=False)\n\nfeature_importance\n\n\n\n\n\n\n\n\n\nFeature\nImportance\n\n\n\n\n7\nhighway_mpg\n1.0\n\n\n0\nUnnamed: 0\n0.0\n\n\n1\ncity_mpg\n0.0\n\n\n2\nclass\n0.0\n\n\n3\ncombination_mpg\n0.0\n\n\n4\ncylinders\n0.0\n\n\n5\ndisplacement\n0.0\n\n\n6\ndrive\n0.0\n\n\n8\nmake\n0.0\n\n\n9\nmodel\n0.0\n\n\n10\ntransmission\n0.0\n\n\n11\nyear\n0.0\n\n\n\n\n\n\n\n\n\nfrom sklearn.model_selection import GridSearchCV\n\n# Selecting a subset of features based on the analysis\nselected_features = ['highway_mpg', 'city_mpg', 'combination_mpg', 'year', 'make', 'model', 'class', 'transmission']\n\n# Updating the feature set\nX_selected = df[selected_features]\n\n# Splitting the data into train and test sets\nX_train_selected, X_test_selected, y_train_selected, y_test_selected = train_test_split(\n    X_selected, y, test_size=0.2, random_state=42)\n\n# Parameters for hyper-parameter tuning\nparam_grid = {\n    'criterion': ['gini', 'entropy'],\n    'max_depth': [None, 10, 20, 30, 40, 50],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4]\n}\n\n# Grid search with cross-validation\ndt_grid_search = GridSearchCV(DecisionTreeClassifier(random_state=42), param_grid, cv=5, scoring='accuracy', n_jobs=-1)\ndt_grid_search.fit(X_train_selected, y_train_selected)\n\n# Best parameters and best score\nbest_params = dt_grid_search.best_params_\nbest_score = dt_grid_search.best_score_\n\nbest_params, best_score\n\n({'criterion': 'gini',\n  'max_depth': None,\n  'min_samples_leaf': 1,\n  'min_samples_split': 2},\n 1.0)"
  },
  {
    "objectID": "5000-website/decision-tree/reference-notebooks/assignment.html",
    "href": "5000-website/decision-tree/reference-notebooks/assignment.html",
    "title": "Lab: Binary classification with decision trees",
    "section": "",
    "text": "Author: J. Hickman\nThe breast cancer dataset is a well studied binary classification dataset.\n\nClasses: 2\nSamples per class: 212(M),357(B)\nSamples total:569\nDimensionality: 30\nFeatures: real, positive\n\nThe copy of UCI ML Breast Cancer Wisconsin (Diagnostic) dataset is downloaded from: https://goo.gl/U2Uwz2\nIn this lab we will use the dataset to train a decision tree model.\nhttps://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html#sklearn.datasets.load_breast_cancer\nInstructions * Read and work through all tutorial content and do all exercises below\nSubmission: * You need to upload ONE document to Canvas when you are done * (1) A PDF (or HTML) of the completed form of this notebook * The final uploaded version should NOT have any code-errors present * All outputs must be visible in the uploaded version, including code-cell outputs, images, graphs, etc\nFor reference recall the following definitions * Accuracy classification score. In multilabel classification, this function computes subset accuracy: the set of labels predicted for a sample must exactly match the corresponding set of labels in y_true.\n\nThe precision is the ratio tp / (tp + fp) where tp is the number of true positives and fp the number of false positives.\n\nThe precision is intuitively the ability of the classifier not to label as positive a sample that is negative. The best value is 1 and the worst value is 0.\n\nThe recall is the ratio tp / (tp + fn) where tp is the number of true positives and fn the number of false negatives.\n\nThe recall is intuitively the ability of the classifier to find all the positive samples. The best value is 1 and the worst value is 0.\n\nhttps://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html\n\n\n4.1.0 Student information\nPlease provide the following information\n\n# ## Name: Isfar Baset\n# ## Date: 11/11/23\n# ## Class Section: 01\n# ## Lab Section: 01\n\n\n\nImport\n\nimport pandas as pd\nimport seaborn as sns \nimport matplotlib.pyplot as plt\nfrom sklearn import tree\nfrom IPython.display import Image\nimport numpy as np\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\n\n\n\n4.1.1: Import\nThe following code will import the data file into a pandas data-frame\n\ndf=pd.read_csv('../eda/cars-data.csv')\nprint(df.iloc[0])\n\nUnnamed: 0                   0\ncity_mpg                    18\nclass              midsize car\ncombination_mpg             21\ncylinders                  4.0\ndisplacement               2.2\ndrive                      fwd\nfuel_type                  gas\nhighway_mpg                 26\nmake                    toyota\nmodel                    Camry\ntransmission                 a\nyear                      1993\nName: 0, dtype: object\n\n\n\n# get y \n\ny = df['fuel_type'].unique().tolist()\n\n# Replace 'diesel' with 'gas' in the entire DataFrame\ndf.replace('diesel', 'gas', inplace=True)\n\ny = df['fuel_type'].unique().tolist()\n\ny\n\n['gas', 'electricity']\n\n\n\n# Using a for loop to replace categorical values with cat codes\n\ndf['fuel_type'] = df['fuel_type'].astype('category')\ndf['fuel_type'] = df['fuel_type'].cat.codes\n\n# Display the altered DataFrame\ndf.head()\n\n\n\n\n\n\n\n\n\nUnnamed: 0\ncity_mpg\nclass\ncombination_mpg\ncylinders\ndisplacement\ndrive\nfuel_type\nhighway_mpg\nmake\nmodel\ntransmission\nyear\n\n\n\n\n0\n0\n18\nmidsize car\n21\n4.0\n2.2\nfwd\n1\n26\ntoyota\nCamry\na\n1993\n\n\n1\n1\n19\nmidsize car\n22\n4.0\n2.2\nfwd\n1\n27\ntoyota\nCamry\nm\n1993\n\n\n2\n2\n16\nmidsize car\n19\n6.0\n3.0\nfwd\n1\n22\ntoyota\nCamry\na\n1993\n\n\n3\n3\n16\nmidsize car\n18\n6.0\n3.0\nfwd\n1\n22\ntoyota\nCamry\nm\n1993\n\n\n4\n4\n18\nmidsize-large station wagon\n21\n4.0\n2.2\nfwd\n1\n26\ntoyota\nCamry\na\n1993\n\n\n\n\n\n\n\n\n\n# Drop non-numerical columns\ndf = df.select_dtypes(include=['number'])\n\ndf\n\n\n\n\n\n\n\n\n\nUnnamed: 0\ncity_mpg\ncombination_mpg\ncylinders\ndisplacement\nfuel_type\nhighway_mpg\nyear\n\n\n\n\n0\n0\n18\n21\n4.0\n2.2\n1\n26\n1993\n\n\n1\n1\n19\n22\n4.0\n2.2\n1\n27\n1993\n\n\n2\n2\n16\n19\n6.0\n3.0\n1\n22\n1993\n\n\n3\n3\n16\n18\n6.0\n3.0\n1\n22\n1993\n\n\n4\n4\n18\n21\n4.0\n2.2\n1\n26\n1993\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n714\n714\n80\n76\nNaN\nNaN\n0\n72\n2021\n\n\n715\n715\n89\n85\nNaN\nNaN\n0\n82\n2023\n\n\n716\n716\n79\n76\nNaN\nNaN\n0\n72\n2023\n\n\n717\n717\n74\n70\nNaN\nNaN\n0\n66\n2022\n\n\n718\n718\n73\n69\nNaN\nNaN\n0\n65\n2022\n\n\n\n\n719 rows × 8 columns\n\n\n\n\n\n# Dropping non-numerical and unnecessary columns\ndf = df.drop(columns=['Unnamed: 0'])\n\n\n\n# Dropping non-numerical and unnecessary columns\ndf = df.drop(columns=['year'])\n\n\n# LOAD THE DATAFRAME\nfrom sklearn.datasets import load_breast_cancer\n\n# (x,y) = load_breast_cancer(return_X_y=True,as_frame=True)\n# df=pd.concat([x,y],axis=1)\n# df=pd.read_csv('../eda/ev-wiki-crawl-results.csv')\n# LOOK AT FIRST ROW\nprint(df.iloc[0])\n\ncity_mpg           18.0\ncombination_mpg    21.0\ncylinders           4.0\ndisplacement        2.2\nfuel_type           1.0\nhighway_mpg        26.0\nName: 0, dtype: float64\n\n\n\n# INSERT CODE TO PRINT ITS SHAPE AND COLUMN NAMES\n\nprint(df.shape)\ndf.columns\n\n(719, 6)\n\n\nIndex(['city_mpg', 'combination_mpg', 'cylinders', 'displacement', 'fuel_type',\n       'highway_mpg'],\n      dtype='object')\n\n\n\n\n4.1.2: Basic data exploration\nWe will be using y=“target” (output target) and all other remaining columns as our X (input feature) matrix.\nBefore doing analysis it is always good to “get inside” the data and see what we are working with\n\n#INSERT CODE TO PRINT THE FOLLOWING DATA-FRAME WHICH SUMMARIZES EACH COLUMN \n\nsummary = df.describe().T  \nsummary['dtypes'] = df.dtypes \nsummary = summary[['dtypes', 'min', 'mean', 'max']]  \nprint(summary)\n\n                  dtypes   min       mean    max\ncity_mpg           int64   9.0  34.051460  150.0\ncombination_mpg    int64  11.0  35.094576  136.0\ncylinders        float64   3.0   5.154622   10.0\ndisplacement     float64   1.4   2.760336    6.2\nfuel_type           int8   0.0   0.834492    1.0\nhighway_mpg        int64  12.0  37.329624  123.0\n\n\n\n# Count the number of samples for each target value\n# resource: https://pandas.pydata.org/docs/reference/api/pandas.Series.value_counts.html\n\n\ntarget_counts = df['fuel_type'].value_counts(normalize=True)  # normalize=True to get the proportions\n\n# Print the results\nfor target, count in target_counts.items():\n    print(f\"Number of points with target={target}: {count * len(df)} {count}\")\n\nNumber of points with target=1: 600.0 0.8344923504867872\nNumber of points with target=0: 118.99999999999999 0.16550764951321278\n\n\n\n# RUN THE FOLLOWING CODE TO SHOW THE HEAT-MAP FOR THE CORRELATION MATRIX\ncorr = df.corr();  #print(corr)                 #COMPUTE CORRELATION OF FEATER MATRIX\nprint(corr.shape)\nsns.set_theme(style=\"white\")\nf, ax = plt.subplots(figsize=(20, 20))  # Set up the matplotlib figure\ncmap = sns.diverging_palette(230, 20, as_cmap=True)     # Generate a custom diverging colormap\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr,  cmap=cmap, vmin=-1, vmax=1, center=0,\n        square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\nplt.show();\n\n(6, 6)\n\n\n\n\n\n\n\n\n\nWhen the dataset is very large then the seaborn pairplot is often very slow.\nHowever, in this case it can still be useful to look at a subset of the features\n\nimport pandas as pd\n\n# Assuming df is your DataFrame and y is a Series or DataFrame to be concatenated\nn_samples = 10  # or any number you desire\n\n# Check if df has enough rows to sample without replacement\nif df.shape[0] &gt;= n_samples:  # Make sure to use shape[0] for rows\n    # Sample without replacement\n    df_sampled = df.sample(n=n_samples, axis=0)  # Sample rows with axis=0\nelse:\n    # Sample with replacement if not enough rows, or adjust n_samples as needed\n    df_sampled = df.sample(n=n_samples, replace=True, axis=0)  # Sample rows with axis=0\n\n# Now concatenate the sampled DataFrame with y along the columns\ntmp = pd.concat([df_sampled, df], axis=1)  # Concatenate along columns\n\n#### 4.1.3 Isolate inputs/output & Split data\n\n# INSERT CODE TO MAKE DATA-FRAMES (or numpy arrays) (X,Y) WHERE Y=\"target\" COLUMN and X=\"everything else\"\n# Resource used: https://medium.com/codex/how-to-set-x-and-y-in-pandas-3f38584e9bed\n\n\nX = df.drop('fuel_type', axis=1)  # X includes everything except the target column\ny = df['fuel_type']  # Y is just the target column\n\n\n# INSERT CODE TO PARTITION THE DATASET INTO TRAINING AND TEST SETS\n\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n\n\n# INSERT CODE, AS A CONSISTENCY CHECK, TO PRINT THE TYPE AND SHAPE OF x_train, x_test, y_train, y_test\n# Changed the output structure for better understanding\n\nprint(\"TRAINING SHAPES: \\n\", f\"x_train:\\n shape: {x_train.shape}\\n type: {type(x_train)}\\n\", f\"y_train:\\n shape: {y_train.shape}\\n type: {type(y_train)}\\n\")\nprint(\"TEST SHAPES: \\n\", f\"x_test:\\n shape: {x_test.shape}\\n type: {type(x_test)}\\n\", f\"y_test:\\n shape: {y_test.shape}\\n type: {type(y_test)}\\n\")\n\nTRAINING SHAPES: \n x_train:\n shape: (575, 5)\n type: &lt;class 'pandas.core.frame.DataFrame'&gt;\n y_train:\n shape: (575,)\n type: &lt;class 'pandas.core.series.Series'&gt;\n\nTEST SHAPES: \n x_test:\n shape: (144, 5)\n type: &lt;class 'pandas.core.frame.DataFrame'&gt;\n y_test:\n shape: (144,)\n type: &lt;class 'pandas.core.series.Series'&gt;\n\n\n\n#### 4.1.4 Training the model\n\n#### INSERT CODE BELOW TO TRAIN A SKLEARN DECISION TREE MODEL ON x_train,y_train \nfrom sklearn import tree\n\nmodel = tree.DecisionTreeClassifier()\nmodel = model.fit(x_train, y_train)\n\n#### 4.1.5 Check the results\nEvaluate the performance of the decision tree model by using the test data.\n\n# INSERT CODE TO USE THE MODEL TO MAKE PREDICTIONS FOR THE TRAINING AND TEST SET \nyp_train = model.predict(x_train)\nyp_test = model.predict(x_test)\n\nUse the following reference to display the confusion matrix. SKlearn Confusion Matrix will give you the code you need.\nIn the function below, also print the following as part of the function output ACCURACY: 0.9035087719298246 NEGATIVE RECALL (Y=0): 0.9574468085106383 NEGATIVE PRECISION (Y=0): 0.8333333333333334 POSITIVE RECALL (Y=1): 0.8656716417910447 POSITIVE PRECISION (Y=1): 0.9666666666666667 [[45  2]  [ 9 58]]\n\n#INSERT CODE TO WRITE A FUNCTION def confusion_plot(y_data,y_pred) WHICH GENERATES A CONFUSION MATRIX PLOT AND PRINTS THE INFORMATION ABOVE (see link above for example)\n\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_classification\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\n\ndef confusion_plot(y_data, y_pred):\n\n    cm = confusion_matrix(y_data, y_pred)\n\n    # Calculate metrics\n    accuracy = accuracy_score(y_data, y_pred)\n    negative_recall = recall_score(y_data, y_pred, average=None)\n    negative_precision = precision_score(y_data, y_pred, average=None)\n    positive_recall = recall_score(y_data, y_pred, average=None)\n    positive_precision = precision_score(y_data, y_pred, average=None)\n    \n    # Print metrics\n    print(f\"ACCURACY: {accuracy}\")\n    print(f\"NEGATIVE RECALL (Y=0): {negative_recall}\")\n    print(f\"NEGATIVE PRECISION (Y=0): {negative_precision}\")\n    print(f\"POSITIVE RECALL (Y=1): {positive_recall}\")\n    print(f\"POSITIVE PRECISION (Y=1): {positive_precision}\")\n    print(cm)\n    \n    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n    \n    disp.plot()\n    plt.show()\n\n\n# RUN THE FOLLOWING CODE TO TEST YOUR FUNCTION \nprint(\"------TRAINING------\")\nconfusion_plot(y_train,yp_train)\nprint(\"------TEST------\")\nconfusion_plot(y_test,yp_test)\n\n------TRAINING------\nACCURACY: 0.991304347826087\nNEGATIVE RECALL (Y=0): [1.         0.98951782]\nNEGATIVE PRECISION (Y=0): [0.95145631 1.        ]\nPOSITIVE RECALL (Y=1): [1.         0.98951782]\nPOSITIVE PRECISION (Y=1): [0.95145631 1.        ]\n[[ 98   0]\n [  5 472]]\n------TEST------\nACCURACY: 1.0\nNEGATIVE RECALL (Y=0): [1. 1.]\nNEGATIVE PRECISION (Y=0): [1. 1.]\nPOSITIVE RECALL (Y=1): [1. 1.]\nPOSITIVE PRECISION (Y=1): [1. 1.]\n[[ 21   0]\n [  0 123]]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#### 4.1.6 Visualize the tree\n\n# INSERT CODE TO WRITE A FUNCTION \"def plot_tree(model,X,Y)\" VISUALIZE THE DECISION TREE (see https://mljar.com/blog/visualize-decision-tree/ for an example)\ndef plot_tree(model,X,Y):\n    \n    plt.figure(figsize=(20, 10))\n    plot = tree.plot_tree(model, \n                          feature_names=X.columns, \n                          class_names=True, \n                          filled=True)\n    plt.show()\n\n\nplot_tree(model,X,y)\n\nInvalidParameterError: The 'feature_names' parameter of plot_tree must be an instance of 'list' or None. Got Index(['city_mpg', 'combination_mpg', 'cylinders', 'displacement',\n       'highway_mpg'],\n      dtype='object') instead.\n\n\n&lt;Figure size 2000x1000 with 0 Axes&gt;\n\n\n#### 4.1.6 Hyper-parameter turning\nThe “max_depth” hyper-parameter lets us control the number of layers in our tree.\nLets iterate over “max_depth” and try to find the set of hyper-parameters with the lowest training AND test error.\n\n# COMPLETE THE FOLLOWING CODE TO LOOP OVER POSSIBLE HYPER-PARAMETERS VALUES\ntest_results=[]\ntrain_results=[]\n\nfor num_layer in range(1,20):\n    model = tree.DecisionTreeClassifier(max_depth=num_layer)\n    model = model.fit(x_train, y_train)\n\n    yp_train=model.predict(x_train)\n    yp_test=model.predict(x_test)\n\n    # print(y_pred.shape)\n    test_results.append([num_layer,accuracy_score(y_test, yp_test),recall_score(y_test, yp_test,pos_label=0),recall_score(y_test, yp_test,pos_label=1)])\n    train_results.append([num_layer,accuracy_score(y_train, yp_train),recall_score(y_train, yp_train,pos_label=0),recall_score(y_train, yp_train,pos_label=1)])\n\ntest_results=np.array(test_results)\ntrain_results=np.array(train_results)\ncol=1\nplt.plot (test_results[:,0],test_results[:,col],'-or')\nplt.plot(train_results[:,0],train_results[: ,col],'-ob')\nplt.xlabel('Number of layers in decision tree (max_depth)') \nplt.ylabel ('ACCURACY (Y=0): Training (blue) and Test (red)') \nplt. show()\n\ncol=2\nplt.plot (test_results[:,0],test_results[:,col],'-or')\nplt.plot(train_results[:,0],train_results[: ,col],'-ob')\nplt.xlabel('Number of layers in decision tree (max_depth)') \nplt.ylabel ('RECALL (Y=0): Training (blue) and Test (red)') \nplt. show()\n\ncol=3\nplt.plot (test_results[:,0],test_results[:,col],'-or')\nplt.plot(train_results[:,0],train_results[: ,col],'-ob')\nplt.xlabel('Number of layers in decision tree (max_depth)') \nplt.ylabel ('RECALL (Y=1): Training (blue) and Test (red)') \nplt. show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#### 4.1.7 Train optimal model\nRe-train the decision tree using the optimal hyper-parameter obtained from the plot above\n\n#### COMPLETE THE CODE BELOW TO TRAIN A SKLEARN DECISION TREE MODEL ON x_train,y_train \nfrom sklearn import tree\nmodel = tree.DecisionTreeClassifier(max_depth=3)\nmodel = model.fit(x_train, y_train)\n\nyp_train=model.predict(x_train)\nyp_test=model.predict(x_test)\n\n\n# RUN THE FOLLOWING CODE TO EVALUATE YOUR MODEL\nprint(\"------TRAINING------\")\nconfusion_plot(y_train,yp_train)\nprint(\"------TEST------\")\nconfusion_plot(y_test,yp_test)\n\nplot_tree(model,X,y)\n\n------TRAINING------\nACCURACY: 0.9714285714285714\nNEGATIVE RECALL (Y=0): 0.993103448275862\nNEGATIVE PRECISION (Y=0): 0.9632107023411371\nPOSITIVE RECALL (Y=1): 0.993103448275862\nPOSITIVE PRECISION (Y=1): 0.9632107023411371\n[[154  11]\n [  2 288]]\n------TEST------\nACCURACY: 0.9649122807017544\nNEGATIVE RECALL (Y=0): 0.9850746268656716\nNEGATIVE PRECISION (Y=0): 0.9565217391304348\nPOSITIVE RECALL (Y=1): 0.9850746268656716\nPOSITIVE PRECISION (Y=1): 0.9565217391304348\n[[44  3]\n [ 1 66]]"
  },
  {
    "objectID": "5000-website/decision-tree/reference-notebooks/car-regression.html",
    "href": "5000-website/decision-tree/reference-notebooks/car-regression.html",
    "title": "Load and check data",
    "section": "",
    "text": "import sklearn\nfrom sklearn import datasets\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n#RELOAD FILE AND PRETEND THAT IS OUR STARTING POINT \ndf=pd.read_csv('../eda/cars-data.csv')  \nprint(df.shape)\ndf.head()\n\n(719, 13)\n\n\n\n\n\n\n\n\n\n\nUnnamed: 0\ncity_mpg\nclass\ncombination_mpg\ncylinders\ndisplacement\ndrive\nfuel_type\nhighway_mpg\nmake\nmodel\ntransmission\nyear\n\n\n\n\n0\n0\n18\nmidsize car\n21\n4.0\n2.2\nfwd\ngas\n26\ntoyota\nCamry\na\n1993\n\n\n1\n1\n19\nmidsize car\n22\n4.0\n2.2\nfwd\ngas\n27\ntoyota\nCamry\nm\n1993\n\n\n2\n2\n16\nmidsize car\n19\n6.0\n3.0\nfwd\ngas\n22\ntoyota\nCamry\na\n1993\n\n\n3\n3\n16\nmidsize car\n18\n6.0\n3.0\nfwd\ngas\n22\ntoyota\nCamry\nm\n1993\n\n\n4\n4\n18\nmidsize-large station wagon\n21\n4.0\n2.2\nfwd\ngas\n26\ntoyota\nCamry\na\n1993\nnan_count = df.isna().sum()\n\nprint(nan_count)\n\nUnnamed: 0           0\ncity_mpg             0\nclass                0\ncombination_mpg      0\ncylinders          124\ndisplacement       124\ndrive                8\nfuel_type            0\nhighway_mpg          0\nmake                 0\nmodel                0\ntransmission         0\nyear                 0\ndtype: int64\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 719 entries, 0 to 718\nData columns (total 13 columns):\n #   Column           Non-Null Count  Dtype  \n---  ------           --------------  -----  \n 0   Unnamed: 0       719 non-null    int64  \n 1   city_mpg         719 non-null    int64  \n 2   class            719 non-null    object \n 3   combination_mpg  719 non-null    int64  \n 4   cylinders        595 non-null    float64\n 5   displacement     595 non-null    float64\n 6   drive            711 non-null    object \n 7   fuel_type        719 non-null    object \n 8   highway_mpg      719 non-null    int64  \n 9   make             719 non-null    object \n 10  model            719 non-null    object \n 11  transmission     719 non-null    object \n 12  year             719 non-null    int64  \ndtypes: float64(2), int64(5), object(6)\nmemory usage: 73.2+ KB\n# Convert all 'object' type columns to 'string'\nfor col in df.select_dtypes(include=['object']).columns:\n    df[col] = df[col].astype('string')\n\n# Verify the changes\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 719 entries, 0 to 718\nData columns (total 13 columns):\n #   Column           Non-Null Count  Dtype  \n---  ------           --------------  -----  \n 0   Unnamed: 0       719 non-null    int64  \n 1   city_mpg         719 non-null    int64  \n 2   class            719 non-null    string \n 3   combination_mpg  719 non-null    int64  \n 4   cylinders        595 non-null    float64\n 5   displacement     595 non-null    float64\n 6   drive            711 non-null    string \n 7   fuel_type        719 non-null    string \n 8   highway_mpg      719 non-null    int64  \n 9   make             719 non-null    string \n 10  model            719 non-null    string \n 11  transmission     719 non-null    string \n 12  year             719 non-null    int64  \ndtypes: float64(2), int64(5), string(6)\nmemory usage: 73.2 KB\n# get y \n\ny = df['fuel_type'].unique().tolist()\n\ny\n\n['gas', 'electricity', 'diesel']\n# Dropping non-numerical and unnecessary columns\ndf = df.drop(columns=['Unnamed: 0'])\n# Replace continuous missing values with mean of the column. check for Nan values again.\n\ncols = ['displacement', 'cylinders']\ndf[cols] = df[cols].fillna(df[cols].mean())\n\nnan_count = df.isna().sum()\nprint(nan_count)\n\ncity_mpg           0\nclass              0\ncombination_mpg    0\ncylinders          0\ndisplacement       0\ndrive              8\nfuel_type          0\nhighway_mpg        0\nmake               0\nmodel              0\ntransmission       0\nyear               0\ndtype: int64\n# Replace categorical missing values with mode of the column. check for Nan values again.\n\ndf['drive'] = df['drive'].fillna(df['drive'].mode().iloc[0])\n\nnan_count = df.isna().sum()\nprint(nan_count)\n\ncity_mpg           0\nclass              0\ncombination_mpg    0\ncylinders          0\ndisplacement       0\ndrive              0\nfuel_type          0\nhighway_mpg        0\nmake               0\nmodel              0\ntransmission       0\nyear               0\ndtype: int64\n# Using a for loop to replace categorical values with cat codes\ncat_cols = ['class', 'drive', 'fuel_type', 'make', 'model', 'transmission']\nfor col in cat_cols:\n    df[col] = df[col].astype('category')\n    df[col] = df[col].cat.codes\n\n# Display the altered DataFrame\ndf.head()\n\n\n\n\n\n\n\n\n\ncity_mpg\nclass\ncombination_mpg\ncylinders\ndisplacement\ndrive\nfuel_type\nhighway_mpg\nmake\nmodel\ntransmission\nyear\n\n\n\n\n0\n18\n2\n21\n4.0\n2.2\n2\n2\n26\n24\n33\n0\n1993\n\n\n1\n19\n2\n22\n4.0\n2.2\n2\n2\n27\n24\n33\n1\n1993\n\n\n2\n16\n2\n19\n6.0\n3.0\n2\n2\n22\n24\n33\n0\n1993\n\n\n3\n16\n2\n18\n6.0\n3.0\n2\n2\n22\n24\n33\n1\n1993\n\n\n4\n18\n4\n21\n4.0\n2.2\n2\n2\n26\n24\n33\n0\n1993\ndf.to_csv('cardf_cleaned.csv')\n# Split the dataset in X and y. since this is unsupervised learning, we will not use the y labels. you can choose to normalize the X data by using the StandardScaler function.\n\nx_cols = ['city_mpg', 'class', 'combination_mpg', 'cylinders', 'displacement', 'drive', 'highway_mpg', 'make', 'model', 'transmission', 'year' ]\ny_cols = ['fuel_type']\n\nX = df[x_cols]  #'fuel_type' is the column we want to predict.\ny = df[y_cols]\n\nX = df[x_cols]  # X data\ny = df[y_cols].values.ravel()  # Flatten y to a 1D array\n\n# X.shape, y.shape, y\nprint(X.shape)\nprint(y.shape)\n\n(719, 11)\n(719,)\nprint(pd.DataFrame(X.iloc[0:11, :]))\n\n    city_mpg  class  combination_mpg  cylinders  displacement  drive  \\\n0         18      2               21        4.0           2.2      2   \n1         19      2               22        4.0           2.2      2   \n2         16      2               19        6.0           3.0      2   \n3         16      2               18        6.0           3.0      2   \n4         18      4               21        4.0           2.2      2   \n5         23      0               24        4.0           1.6      2   \n6         23      0               26        4.0           1.6      2   \n7         23      0               25        4.0           1.8      2   \n8         23      0               26        4.0           1.8      2   \n9         23      9               25        4.0           1.8      2   \n10        21     10               23        4.0           2.0      2   \n\n    highway_mpg  make  model  transmission  year  \n0            26    24     33             0  1993  \n1            27    24     33             1  1993  \n2            22    24     33             0  1993  \n3            22    24     33             1  1993  \n4            26    24     33             0  1993  \n5            26    24     37             0  1993  \n6            31    24     37             1  1993  \n7            30    24     37             0  1993  \n8            30    24     37             1  1993  \n9            30    24     37             0  1993  \n10           26    24    104             0  1996\nprint(pd.DataFrame(y[0:10]))\n\n   0\n0  2\n1  2\n2  2\n3  2\n4  2\n5  2\n6  2\n7  2\n8  2\n9  2"
  },
  {
    "objectID": "5000-website/decision-tree/reference-notebooks/car-regression.html#partition-data",
    "href": "5000-website/decision-tree/reference-notebooks/car-regression.html#partition-data",
    "title": "Load and check data",
    "section": "Partition data",
    "text": "Partition data\n\n#DROP FIRST TWO FEATURES\nX = X.iloc[:, 2:]\n\nfrom sklearn.model_selection import train_test_split\ntest_ratio=0.2\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=test_ratio, random_state=0)\ny_train=y_train.flatten()\ny_test=y_test.flatten()\n\nprint(\"x_train.shape        :\",x_train.shape)\nprint(\"y_train.shape        :\",y_train.shape)\n\nprint(\"X_test.shape     :\",x_test.shape)\nprint(\"y_test.shape     :\",y_test.shape)\n\nx_train.shape        : (575, 9)\ny_train.shape        : (575,)\nX_test.shape     : (144, 9)\ny_test.shape     : (144,)"
  },
  {
    "objectID": "5000-website/decision-tree/reference-notebooks/car-regression.html#hyper-parameter-tuning-max_depth",
    "href": "5000-website/decision-tree/reference-notebooks/car-regression.html#hyper-parameter-tuning-max_depth",
    "title": "Load and check data",
    "section": "Hyper-Parameter tuning (max_depth)",
    "text": "Hyper-Parameter tuning (max_depth)\n\nGoal: Find the optimal hyper parameter\n\n\nfrom sklearn.metrics import mean_absolute_percentage_error\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.tree import DecisionTreeRegressor\n\n\n# HYPER PARAMETER SEARCH FOR OPTIMAL NUMBER OF NEIGHBORS \nhyper_param=[]\ntrain_error=[]\ntest_error=[]\n\n# LOOP OVER HYPER-PARAM\nfor i in range(1,40):\n    # INITIALIZE MODEL \n    model = DecisionTreeRegressor(max_depth=i)\n\n    # TRAIN MODEL \n    model.fit(x_train,y_train)\n\n    # OUTPUT PREDICTIONS FOR TRAINING AND TEST SET \n    yp_train = model.predict(x_train)\n    yp_test = model.predict(x_test)\n\n    # shift=1+np.min(y_train) #add shift to remove division by zero \n    err1=mean_absolute_error(y_train, yp_train) \n    err2=mean_absolute_error(y_test, yp_test) \n    \n    # err1=100.0*np.mean(np.absolute((yp_train-y_train)/y_train))\n    # err2=100.0*np.mean(np.absolute((yp_test-y_test)/y_test))\n\n    hyper_param.append(i)\n    train_error.append(err1)\n    test_error.append(err2)\n\n    if(i==1 or i%10==0):\n        print(\"hyperparam =\",i)\n        print(\" train error:\",err1)\n        print(\" test error:\" ,err2)\n\nhyperparam = 1\n train error: 0.0186419929742148\n test error: 0.02268100465323882\nhyperparam = 10\n train error: 7.117663946155022e-15\n test error: 0.020833333333340878\nhyperparam = 20\n train error: 7.117663946155022e-15\n test error: 0.020833333333340878\nhyperparam = 30\n train error: 7.117663946155022e-15\n test error: 0.013888888888896438"
  },
  {
    "objectID": "5000-website/decision-tree/reference-notebooks/car-regression.html#convergence-plot",
    "href": "5000-website/decision-tree/reference-notebooks/car-regression.html#convergence-plot",
    "title": "Load and check data",
    "section": "Convergence plot",
    "text": "Convergence plot\n\nplt.plot(hyper_param,train_error ,linewidth=2, color='k')\nplt.plot(hyper_param,test_error ,linewidth=2, color='b')\n\nplt.xlabel(\"Depth of tree (max depth)\")\nplt.ylabel(\"Training (black) and test (blue) MAE (error)\")\n\ni=1\nprint(hyper_param[i],train_error[i],test_error[i])\n\n2 0.014922360248454326 0.016865079365086914"
  },
  {
    "objectID": "5000-website/decision-tree/reference-notebooks/car-regression.html#hyper-parameter-tuning-min_samples_splitint",
    "href": "5000-website/decision-tree/reference-notebooks/car-regression.html#hyper-parameter-tuning-min_samples_splitint",
    "title": "Load and check data",
    "section": "Hyper-Parameter tuning (min_samples_splitint)",
    "text": "Hyper-Parameter tuning (min_samples_splitint)\n\nGoal: Find the optimal hyper parameter\n\n\n# HYPER PARAMETER SEARCH FOR OPTIMAL NUMBER OF NEIGHBORS \nhyper_param=[]\ntrain_error=[]\ntest_error=[]\n\n# LOOP OVER HYPER-PARAM\nfor i in range(2,100):\n    # INITIALIZE MODEL \n    model = DecisionTreeRegressor(min_samples_split=i)\n\n    # TRAIN MODEL \n    model.fit(x_train,y_train)\n\n    # OUTPUT PREDICTIONS FOR TRAINING AND TEST SET \n    yp_train = model.predict(x_train)\n    yp_test = model.predict(x_test)\n\n    # shift=1+np.min(y_train) #add shift to remove division by zero \n    err1=mean_absolute_error(y_train, yp_train) \n    err2=mean_absolute_error(y_test, yp_test) \n    \n    # err1=100.0*np.mean(np.absolute((yp_train-y_train)/y_train))\n    # err2=100.0*np.mean(np.absolute((yp_test-y_test)/y_test))\n\n    hyper_param.append(i)\n    train_error.append(err1)\n    test_error.append(err2)\n\n    if(i%10==0):\n        print(\"hyperparam =\",i)\n        print(\" train error:\",err1)\n        print(\" test error:\" ,err2)\n\nhyperparam = 10\n train error: 0.002318840579717263\n test error: 0.013888888888896438\nhyperparam = 20\n train error: 0.005978260869572333\n test error: 0.013888888888896438\nhyperparam = 30\n train error: 0.014922360248454315\n test error: 0.016865079365086907\nhyperparam = 40\n train error: 0.014922360248454315\n test error: 0.016865079365086907\nhyperparam = 50\n train error: 0.014922360248454315\n test error: 0.016865079365086907\nhyperparam = 60\n train error: 0.014922360248454322\n test error: 0.016865079365086914\nhyperparam = 70\n train error: 0.014922360248454315\n test error: 0.016865079365086907\nhyperparam = 80\n train error: 0.014922360248454322\n test error: 0.016865079365086914\nhyperparam = 90\n train error: 0.014922360248454315\n test error: 0.016865079365086907"
  },
  {
    "objectID": "5000-website/decision-tree/reference-notebooks/car-regression.html#convergence-plot-1",
    "href": "5000-website/decision-tree/reference-notebooks/car-regression.html#convergence-plot-1",
    "title": "Load and check data",
    "section": "Convergence plot",
    "text": "Convergence plot\n\nplt.plot(hyper_param,train_error ,linewidth=2, color='k')\nplt.plot(hyper_param,test_error ,linewidth=2, color='b')\n\nplt.xlabel(\"Minimum number of points in split (min_samples_split)\")\nplt.ylabel(\"Training (black) and test (blue) MAE (error)\")\n\nText(0, 0.5, 'Training (black) and test (blue) MAE (error)')"
  },
  {
    "objectID": "5000-website/decision-tree/reference-notebooks/car-regression.html#re-train-with-optimal-parameters",
    "href": "5000-website/decision-tree/reference-notebooks/car-regression.html#re-train-with-optimal-parameters",
    "title": "Load and check data",
    "section": "Re-train with optimal parameters",
    "text": "Re-train with optimal parameters\n\n# INITIALIZE MODEL \nmodel = DecisionTreeRegressor(max_depth=1)\nmodel.fit(x_train,y_train)                     # TRAIN MODEL \n\n\n# OUTPUT PREDICTIONS FOR TRAINING AND TEST SET \nyp_train = model.predict(x_train)\nyp_test = model.predict(x_test)\n\nerr1=mean_absolute_error(y_train, yp_train) \nerr2=mean_absolute_error(y_test, yp_test) \n    \nprint(\" train error:\",err1)\nprint(\" test error:\" ,err2)"
  },
  {
    "objectID": "5000-website/decision-tree/reference-notebooks/car-regression.html#parity-plot",
    "href": "5000-website/decision-tree/reference-notebooks/car-regression.html#parity-plot",
    "title": "Load and check data",
    "section": "Parity Plot",
    "text": "Parity Plot\n\nPlotting y_pred vs y_data lets you see how good the fit is\nThe closer to the line y=x the better the fit (ypred=ydata –&gt; prefect fit)\n\n\nplt.plot(y_train,yp_train ,\"o\", color='k')\nplt.plot(y_test,yp_test ,\"o\", color='b')\nplt.plot(y_train,y_train ,\"-\", color='r')\n\nplt.xlabel(\"y_data\")\nplt.ylabel(\"y_pred (blue=test)(black=Train)\")\n\nText(0, 0.5, 'y_pred (blue=test)(black=Train)')\n\n\n\n\n\n\n\n\n\n\nfrom sklearn import tree\ndef plot_tree(model):\n    fig = plt.figure(figsize=(25,20))\n    _ = tree.plot_tree(model, \n                    filled=True)\n    plt.show()\n\n\nplot_tree(model)\n\n\n\n\n\n\n\n\n\nfor i in range(0, x_train.shape[1]):\n    print(i)\n    plt.plot(x_train.iloc[:, i], y_train, \"o\", color='b')\n    plt.plot(x_train.iloc[:, i], yp_train, \"o\", color='r')\n    plt.show()\n\n0\n1\n2\n3\n4\n5\n6\n7\n8"
  },
  {
    "objectID": "5000-website/data-gathering/cars-api.html#read-data-from-json-file-and-save-as-a-dataframe",
    "href": "5000-website/data-gathering/cars-api.html#read-data-from-json-file-and-save-as-a-dataframe",
    "title": "Data Source: Cars API",
    "section": "Read Data from JSON File and Save as a Dataframe",
    "text": "Read Data from JSON File and Save as a Dataframe\n\n\nShow the code\nimport pandas as pd\nimport json\n\n# Read the JSON file\nfile_path = 'cars.json'\nwith open(file_path, 'r') as file:\n    data = json.load(file)\n\n# Convert the JSON data to a pandas DataFrame\ndf_list = []\nfor model, records in data.items():\n    for record in records:\n        # Add the model name to each record\n        record['model'] = model\n        df_list.append(record)\n\n# Create DataFrame from the list of dictionaries\ncars_df = pd.DataFrame(df_list)\n\ncars_df\n\n\n\n\n\n\n\n\n\n\ncity_mpg\nclass\ncombination_mpg\ncylinders\ndisplacement\ndrive\nfuel_type\nhighway_mpg\nmake\nmodel\ntransmission\nyear\n\n\n\n\n0\n18\nmidsize car\n21\n4.0\n2.2\nfwd\ngas\n26\ntoyota\nCamry\na\n1993\n\n\n1\n19\nmidsize car\n22\n4.0\n2.2\nfwd\ngas\n27\ntoyota\nCamry\nm\n1993\n\n\n2\n16\nmidsize car\n19\n6.0\n3.0\nfwd\ngas\n22\ntoyota\nCamry\na\n1993\n\n\n3\n16\nmidsize car\n18\n6.0\n3.0\nfwd\ngas\n22\ntoyota\nCamry\nm\n1993\n\n\n4\n18\nmidsize-large station wagon\n21\n4.0\n2.2\nfwd\ngas\n26\ntoyota\nCamry\na\n1993\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n714\n80\nsmall sport utility vehicle\n76\nNaN\nNaN\n4wd\nelectricity\n72\njaguar\nI-PACE\na\n2021\n\n\n715\n89\nsmall sport utility vehicle\n85\nNaN\nNaN\n4wd\nelectricity\n82\njaguar\nI-PACE\na\n2023\n\n\n716\n79\nsmall sport utility vehicle\n76\nNaN\nNaN\n4wd\nelectricity\n72\njaguar\nI-PACE\na\n2023\n\n\n717\n74\nstandard pickup truck\n70\nNaN\nNaN\n4wd\nelectricity\n66\nrivian\nR1T\na\n2022\n\n\n718\n73\nstandard sport utility vehicle\n69\nNaN\nNaN\n4wd\nelectricity\n65\nrivian\nR1S\na\n2022\n\n\n\n\n719 rows × 12 columns\n\n\n\n\n\nSave Output\n\n\nShow the code\ncars_df.to_csv('cars-data.csv')",
    "crumbs": [
      "EV Insights",
      "Data Gathering",
      "Cars API"
    ]
  },
  {
    "objectID": "5000-website/data-gathering/yahoo.html#looking-at-stock-prices-of-automotive-companies",
    "href": "5000-website/data-gathering/yahoo.html#looking-at-stock-prices-of-automotive-companies",
    "title": "Data Source: Yahoo",
    "section": "Looking at Stock Prices of Automotive Companies:",
    "text": "Looking at Stock Prices of Automotive Companies:\n\nPlease toggle the code for further details and in line comments for how the data is gathered\n\n\n\nShow the code\n# Load necessary libraries\nlibrary(quantmod) # For financial market data\nlibrary(plotly)  # For interactive plotting\n\n# Set options to suppress warnings related to getSymbols function\noptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\n\n# Define stock ticker symbols for various automotive companies\ntickers = c(\"STLA\",\"GM\", \"TM\",\"NSANY\", \"MBGYY\", \"BMWYY\",\"POAHY\", \"TSLA\" )\n\n# Retrieve historical stock data for the tickers from Yahoo Finance\ngetSymbols(tickers, auto.assign = TRUE, src=\"yahoo\")\n\n# Loop through each ticker symbol to get data from a specific start date to the current date\nfor (i in tickers){\n  getSymbols(i, from = \"2018-01-01\", to = Sys.Date())\n}\n\n# Define axis labels for the plot\nx &lt;- list(title = \"date\")\ny &lt;- list(title = \"value\")\n\n# Create a data frame with the adjusted close prices of each stock\nstock &lt;- data.frame(STLA$STLA.Adjusted,\n                    GM$GM.Adjusted,\n                    TM$TM.Adjusted,\n                    NSANY$NSANY.Adjusted,\n                    MBGYY$MBGYY.Adjusted,\n                    BMWYY$BMWYY.Adjusted,\n                    POAHY$POAHY.Adjusted,\n                    TSLA$TSLA.Adjusted)\n\n# Add row names (dates) as a column in the data frame\nstock &lt;- data.frame(stock, rownames(stock))\n\n# Rename columns to reflect ticker symbols and date\ncolnames(stock) &lt;- append(tickers, 'Dates')\nhead(stock)\n\n# Convert the Dates column to Date type\nstock$Dates &lt;- as.Date(stock$Dates, \"%Y-%m-%d\")\nstr(stock)\n\n# Create a ggplot object with lines for each company's stock price over time\ng &lt;- ggplot(stock, aes(x=Dates)) +\n  geom_line(aes(y=STLA, colour=\"STLA\"))+ \n  geom_line(aes(y=GM, colour=\"GM\"))+\n  geom_line(aes(y=TM, colour=\"TM\"))+ \n  geom_line(aes(y=NSANY, colour=\"NSANY\"))+ \n  geom_line(aes(y=MBGYY, colour=\"MBGYY\"))+\n  geom_line(aes(y=BMWYY, colour=\"BMWYY\"))+ \n  geom_line(aes(y=POAHY, colour=\"POAHY\"))+ \n  geom_line(aes(y=TSLA, colour=\"TSLA\"))+ \n  labs(\n    title = \"Stock Prices for Automotive Companies\",\n    subtitle = \"From 2018-2023\",\n    x = \"Date\",\n    y = \"Adjusted Closing Prices\") +\n    guides(colour = guide_legend(title=\"Automotive Companies\"))\n\n# Convert ggplot object to an interactive plotly graph\nggplotly(g) %&gt;%\n  layout(hovermode = \"x\")\n\n# Save the stock data frame as a CSV file\nwrite.csv(stock, file = \"stock.csv\", row.names = FALSE)\n\n\nLoading required package: xts\n\nLoading required package: zoo\n\n\nAttaching package: 'zoo'\n\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\n\nLoading required package: TTR\n\nRegistered S3 method overwritten by 'quantmod':\n  method            from\n  as.zoo.data.frame zoo \n\nLoading required package: ggplot2\n\n\nAttaching package: 'plotly'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following object is masked from 'package:graphics':\n\n    layout\n\n\n\n\n\n'STLA''GM''TM''NSANY''MBGYY''BMWYY''POAHY''TSLA'\n\n\n\n\nA data.frame: 6 × 9\n\n\n\nSTLA\nGM\nTM\nNSANY\nMBGYY\nBMWYY\nPOAHY\nTSLA\nDates\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;chr&gt;\n\n\n\n\n2018-01-02\n11.47271\n37.49935\n128.37\n20.04\n15.42706\n25.13300\n6.742719\n21.36867\n2018-01-02\n\n\n2018-01-03\n11.94581\n38.41441\n130.13\n20.25\n15.54485\n25.27038\n6.887549\n21.15000\n2018-01-03\n\n\n2018-01-04\n12.85466\n39.59860\n132.16\n20.23\n15.75144\n25.44391\n7.072612\n20.97467\n2018-01-04\n\n\n2018-01-05\n13.55187\n39.48196\n133.86\n20.39\n15.93447\n25.74035\n7.145028\n21.10533\n2018-01-05\n\n\n2018-01-08\n13.43359\n39.67036\n134.77\n20.44\n16.00333\n25.91389\n7.161120\n22.42733\n2018-01-08\n\n\n2018-01-09\n13.67637\n39.51786\n133.72\n20.54\n16.04138\n25.99342\n7.225490\n22.24600\n2018-01-09\n\n\n\n\n\n\n'data.frame':   1490 obs. of  9 variables:\n $ STLA : num  11.5 11.9 12.9 13.6 13.4 ...\n $ GM   : num  37.5 38.4 39.6 39.5 39.7 ...\n $ TM   : num  128 130 132 134 135 ...\n $ NSANY: num  20 20.2 20.2 20.4 20.4 ...\n $ MBGYY: num  15.4 15.5 15.8 15.9 16 ...\n $ BMWYY: num  25.1 25.3 25.4 25.7 25.9 ...\n $ POAHY: num  6.74 6.89 7.07 7.15 7.16 ...\n $ TSLA : num  21.4 21.1 21 21.1 22.4 ...\n $ Dates: Date, format: \"2018-01-02\" \"2018-01-03\" ...",
    "crumbs": [
      "EV Insights",
      "Data Gathering",
      "Yahoo"
    ]
  },
  {
    "objectID": "5000-website/data-gathering/wikipedia.html",
    "href": "5000-website/data-gathering/wikipedia.html",
    "title": "Web Scraping from Wikipedia",
    "section": "",
    "text": "Since wikipedia contains a plethora of information about many different topics, it was an ideal source for gathering comprehensive text data regarding electric vehicles",
    "crumbs": [
      "EV Insights",
      "Data Gathering",
      "Wikipedia"
    ]
  },
  {
    "objectID": "5000-website/data-gathering/wikipedia.html#step-1",
    "href": "5000-website/data-gathering/wikipedia.html#step-1",
    "title": "Web Scraping from Wikipedia",
    "section": "Step 1",
    "text": "Step 1\nimporting the essential python libraries:\n\n\nShow the code\nimport wikipedia\nimport nltk\nimport string \nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem import PorterStemmer\nfrom nltk.sentiment import SentimentIntensityAnalyzer\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport numpy as np\n\n\n\n\nShow the code\nnltk.download('vader_lexicon')\nnltk.download('punkt')\nnltk.download('wordnet')\nnltk.download('omw-1.4')\n\n\n[nltk_data] Downloading package vader_lexicon to\n[nltk_data]     /Users/isfarbaset/nltk_data...\n[nltk_data]   Package vader_lexicon is already up-to-date!\n[nltk_data] Downloading package punkt to\n[nltk_data]     /Users/isfarbaset/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package wordnet to\n[nltk_data]     /Users/isfarbaset/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to\n[nltk_data]     /Users/isfarbaset/nltk_data...\n[nltk_data]   Package omw-1.4 is already up-to-date!\n\n\nTrue",
    "crumbs": [
      "EV Insights",
      "Data Gathering",
      "Wikipedia"
    ]
  },
  {
    "objectID": "5000-website/data-gathering/wikipedia.html#step-2",
    "href": "5000-website/data-gathering/wikipedia.html#step-2",
    "title": "Web Scraping from Wikipedia",
    "section": "Step 2",
    "text": "Step 2\nSetting the parameters for the text processing task and intializing the tools for NLP tasks:\n\n\nShow the code\n# PARAMETERS \nlabel_list=['electric vehicle', 'gasoline-powered']\nmax_num_pages=25\nsentence_per_chunk=5\nmin_sentence_length=20\n\n# GET STOPWORDS\n# from nltk.corpus import stopwords\nstop_words=nltk.corpus.stopwords.words('english')\n\n# INITALIZE STEMMER+LEMITZIZER+SIA\nsia = SentimentIntensityAnalyzer()\nstemmer = PorterStemmer()\nlemmatizer = WordNetLemmatizer()",
    "crumbs": [
      "EV Insights",
      "Data Gathering",
      "Wikipedia"
    ]
  },
  {
    "objectID": "5000-website/data-gathering/wikipedia.html#step-3",
    "href": "5000-website/data-gathering/wikipedia.html#step-3",
    "title": "Web Scraping from Wikipedia",
    "section": "Step 3",
    "text": "Step 3\nThe following function takes a string input and filters out any unwanted characterers and performs tasks such as lemmatization to output a more readable version of the raw, unprocessed text\n\n\nShow the code\ndef clean_string(text):\n    # #FILTER OUT UNWANTED CHAR\n    new_text=\"\"\n    # keep=string.printable\n    keep=\" abcdefghijklmnopqrstuvwxyz0123456789\"\n    for character in text:\n        if character.lower() in keep:\n            new_text+=character.lower()\n        else: \n            new_text+=\" \"\n    text=new_text\n    # print(text)\n\n    # #FILTER OUT UNWANTED WORDS\n    new_text=\"\"\n    for word in nltk.tokenize.word_tokenize(text):\n        if word not in nltk.corpus.stopwords.words('english'):\n            #lemmatize \n            tmp=lemmatizer.lemmatize(word)\n            # tmp=stemmer.stem(tmp)\n\n            # update word if there is a change\n            # if(tmp!=word): print(tmp,word)\n            \n            word=tmp\n            if len(word)&gt;1:\n                if word in [\".\",\",\",\"!\",\"?\",\":\",\";\"]:\n                    #remove the last space\n                    new_text=new_text[0:-1]+word+\" \"\n                else: #add a space\n                    new_text+=word.lower()+\" \"\n    text=new_text.strip()\n    return text",
    "crumbs": [
      "EV Insights",
      "Data Gathering",
      "Wikipedia"
    ]
  },
  {
    "objectID": "5000-website/data-gathering/wikipedia.html#step-4",
    "href": "5000-website/data-gathering/wikipedia.html#step-4",
    "title": "Web Scraping from Wikipedia",
    "section": "Step 4",
    "text": "Step 4\nIn the following code snippet, Wikipedia is scraped for text data contaning the specific keywords and sentiments are set to the tedxt chunks to facilitate conducting sentiments analysis later on\n\n\nShow the code\n#INITIALIZE \ncorpus=[]  # list of strings (input variables X)\ntargets=[] # list of targets (labels or response variables Y)\n\n#--------------------------\n# LOOP OVER TOPICS \n#--------------------------\nfor label in label_list:\n\n    #SEARCH FOR RELEVANT PAGES \n    titles=wikipedia.search(label,results=max_num_pages)\n    print(\"Pages for label =\",label,\":\",titles)\n\n    #LOOP OVER WIKI-PAGES\n    for title in titles:\n        try:\n            print(\" \",title)\n            wiki_page = wikipedia.page(title, auto_suggest=True)\n\n            # LOOP OVER SECTIONS IN ARTICLE AND GET PAGE TEXT\n            for section in wiki_page.sections:\n                text=wiki_page.section(section); #print(text)\n\n                #BREAK IN TO SENTANCES \n                sentences=nltk.tokenize.sent_tokenize(text)\n                counter=0\n                text_chunk=''\n\n                #LOOP OVER SENTENCES \n                for sentence in sentences:\n                    if len(sentence)&gt;min_sentence_length:\n                        if(counter%sentence_per_chunk==0 and counter!=0):\n                            # PROCESS COMPLETED CHUNK \n                            \n                            # CLEAN STRING\n                            text_chunk=clean_string(text_chunk)\n\n                            # REMOVE LABEL IF IN STRING (MAKES IT TOO EASY)\n                            text_chunk=text_chunk.replace(label,\"\")\n                            \n                            # REMOVE ANY DOUBLE SPACES\n                            text_chunk=' '.join(text_chunk.split()).strip()\n\n                            #UPDATE CORPUS \n                            corpus.append(text_chunk)\n\n                            #UPDATE TARGETS\n                            score=sia.polarity_scores(text_chunk)\n                            target=[label,score['compound']]\n                            targets.append(target)\n\n                            #print(\"TEXT\\n\",text_chunk,target)\n\n                            # RESET CHUNK FOR NEXT ITERATION \n                            text_chunk=sentence\n                        else:\n                            text_chunk+=sentence\n                        #print(\"--------\\n\", sentence)\n                        counter+=1\n\n        except:\n            print(\"WARNING: SOMETHING WENT WRONG:\", title);  \n\n\nPages for label = electric vehicle : ['Electric vehicle', 'History of the electric vehicle', 'Battery electric vehicle', 'Electric vehicle battery', 'Hybrid electric vehicle', 'Electric car use by country', 'Plug-in electric vehicle', 'List of production battery electric vehicles', 'Neighborhood Electric Vehicle', 'Hybrid vehicle drivetrain', 'Aptera (solar electric vehicle)', 'Electric car', 'Capacitor electric vehicle', 'Electric vehicle conversion', 'Hybrid vehicle', 'Plug-in hybrid', 'Charging station', 'Citroën Ami (electric vehicle)', 'Grumman LLV', 'Fuel cell vehicle', 'London Electric Vehicle Company', 'Electric vehicle industry in China', 'Electric Vehicle Company', 'Tesla, Inc.', 'Plug-in electric vehicles in China']\n     Electric vehicle\n     History of the electric vehicle\n     Battery electric vehicle\n     Electric vehicle battery\n     Hybrid electric vehicle\n     Electric car use by country\n     Plug-in electric vehicle\n     List of production battery electric vehicles\n     Neighborhood Electric Vehicle\n     Hybrid vehicle drivetrain\n     Aptera (solar electric vehicle)\n     Electric car\n     Capacitor electric vehicle\n     Electric vehicle conversion\n     Hybrid vehicle\n     Plug-in hybrid\n     Charging station\nWARNING: SOMETHING WENT WRONG: Charging station\n     Citroën Ami (electric vehicle)\n     Grumman LLV\n     Fuel cell vehicle\nWARNING: SOMETHING WENT WRONG: Fuel cell vehicle\n     London Electric Vehicle Company\n     Electric vehicle industry in China\n     Electric Vehicle Company\n     Tesla, Inc.\n     Plug-in electric vehicles in China\nPages for label = gasoline-powered : ['Petrol engine', 'Tractor', 'Charles Duryea', 'Gasoline', 'History of the automobile', 'Waterloo Gasoline Engine Company', 'Chainsaw', 'Lawn mower', 'Catalytic converter', 'ZF S6-37 transmission', 'Mercedes-Benz MB517 engine', 'Ford Duratec engine', 'Motorized bicycle', 'Chevrolet small-block engine', 'Mazda MZR engine', 'Mercedes-Benz MB507 engine', 'Houseboat', 'Hydrogen internal combustion engine vehicle', 'Trolling motor', 'Radio-controlled car', 'Maxus G90', 'Natural gas vehicle', 'Two-stroke oil', 'String trimmer', 'Hydrogen vehicle']\n     Petrol engine\n     Tractor\nWARNING: SOMETHING WENT WRONG: Tractor\n     Charles Duryea\n     Gasoline\n     History of the automobile\n     Waterloo Gasoline Engine Company\n     Chainsaw\n     Lawn mower\n     Catalytic converter\nWARNING: SOMETHING WENT WRONG: Catalytic converter\n     ZF S6-37 transmission\n     Mercedes-Benz MB517 engine\n     Ford Duratec engine\n     Motorized bicycle\n     Chevrolet small-block engine\n     Mazda MZR engine\n     Mercedes-Benz MB507 engine\n     Houseboat\n     Hydrogen internal combustion engine vehicle\n     Trolling motor\n     Radio-controlled car\n     Maxus G90\n     Natural gas vehicle\n     Two-stroke oil\n     String trimmer\n     Hydrogen vehicle",
    "crumbs": [
      "EV Insights",
      "Data Gathering",
      "Wikipedia"
    ]
  },
  {
    "objectID": "5000-website/data-gathering/wikipedia.html#step-5",
    "href": "5000-website/data-gathering/wikipedia.html#step-5",
    "title": "Web Scraping from Wikipedia",
    "section": "Step 5",
    "text": "Step 5\nThe collected text data is shaped into a dataframe and stored in a csv file:\n\n\nShow the code\n#SANITY CHECKS AND PRINT TO FILE \nprint(\"number of text chunks = \",len(corpus))\nprint(\"number of targets = \",len(targets))\n\ntmp=[]\nfor i in range(0,len(corpus)):\n    tmp.append([corpus[i],targets[i][0],targets[i][1]])\ndf=pd.DataFrame(tmp)\ndf=df.rename(columns={0: \"text\", 1: \"label\", 2: \"sentiment\"})\nprint(df)\ndf.to_csv('wiki-crawl-results.csv',index=False)\n\n\nnumber of text chunks =  723\nnumber of targets =  723\n                                                  text             label  \\\n0    electric motive power started 1827 hungarian p...  electric vehicle   \n1    first mass produced appeared america early 190...  electric vehicle   \n2    20th century uk world largest user electric ro...  electric vehicle   \n3    1900 28 percent car road electric ev popular e...  electric vehicle   \n4    seldom marketed woman luxury car may stigma am...  electric vehicle   \n..                                                 ...               ...   \n718  lifetime hydrogen vehicle emit carbon gasoline...  gasoline-powered   \n719  convert hydrogen back electricity fuel cell an...  gasoline-powered   \n720  2019 video real engineering noted notwithstand...  gasoline-powered   \n721  maybe hydrogen fuel cell car come technology n...  gasoline-powered   \n722  internal combustion engine based compressed na...  gasoline-powered   \n\n     sentiment  \n0      -0.7506  \n1       0.9201  \n2       0.7096  \n3       0.9169  \n4       0.9231  \n..         ...  \n718     0.9100  \n719     0.9136  \n720     0.7964  \n721     0.9260  \n722     0.9118  \n\n[723 rows x 3 columns]",
    "crumbs": [
      "EV Insights",
      "Data Gathering",
      "Wikipedia"
    ]
  },
  {
    "objectID": "5000-website/clustering/clustering.html",
    "href": "5000-website/clustering/clustering.html",
    "title": "Clustering",
    "section": "",
    "text": "The feature data ‘X’ consists of various details about cars, excluding the type of fuel they use, which is represented by the target variable, y. These details include how many miles a car can travel on a gallon of fuel in the city or on the highway, the size of the car’s engine, the year it was made, and whether it has an automatic or manual transmission, amongst others.\nThe goal of the clustering analysis is to sort cars into groups based on these features. By doing this, we hope to find patterns or similar characteristics among cars. This can help us understand what types of cars are more alike and could also help predict the type of fuel a car uses based on its features. This information can be valuable for automobile manufacturers who want to understand what kinds of cars to make, or for buyers who are looking for a new car that fits certain criteria.",
    "crumbs": [
      "EV Insights",
      "Clustering"
    ]
  },
  {
    "objectID": "5000-website/clustering/clustering.html#kmeans",
    "href": "5000-website/clustering/clustering.html#kmeans",
    "title": "Clustering",
    "section": "KMEANS:",
    "text": "KMEANS:\nIn the automotive dataset, we can aim to distinguish between two predominant types of fuel consumption across various car makes and models. K-Means clustering is a suitable method for identifying these distinct groups. To begin, we need to determine the optimal number of clusters, denoted by ‘k’. This number represents the groups we intend to identify within our data.\nThe K-Means algorithm starts by randomly selecting ‘k’ centroids, which are the initial points around which clusters are formed. Each data point is then assigned to the nearest centeroid based on a distance metric, and the centroids are recalculated as the mean of all points in the cluster. This process continues, reassigning data points to clusters and recalculating centroids, until the cluster assignments no longer change significantly, indicating that the clusters have stabilized.\nWhen determining the value of ‘k’, we can utilize the elbow method as a more systematic strategy than simple trial and error. By plotting the variance in the clusters against different values of ‘k’, we look for the ‘elbow’ in the graph - a point where the rate of decrease in variance sharply changes. The value of ‘k’ at this elbow point is often considered an indicator of the optimal number of clusters, as it suggests a balance between minimizing variance and the number of clusters.",
    "crumbs": [
      "EV Insights",
      "Clustering"
    ]
  },
  {
    "objectID": "5000-website/clustering/clustering.html#dbscan",
    "href": "5000-website/clustering/clustering.html#dbscan",
    "title": "Clustering",
    "section": "DBSCAN:",
    "text": "DBSCAN:\nDBSCAN is a clustering algorithm that’s effective for data with complex shapes and varying densities, unlike K-Means which works best with spherical clusters. It starts by checking how many points are within a certain distance, called ‘epsilon’, from a random point. If there are enough points within this radius, a cluster forms around that point. The process continues as DBSCAN searches for more points to add to the cluster, expanding based on the density of points. Points that don’t belong to any cluster are marked as outliers. This method is good for identifying clusters that aren’t obvious and separating them from random noise in the data.\nThe silhouette method is a way to determine how well data is clustered. It calculates a score for each data point to see how similar it is to its own cluster compared to other clusters. This score ranges from -1 (meaning the data might be in the wrong cluster) to +1 (indicating the data is well placed in its own cluster). A higher average score across all data points suggests better clustering. By comparing these scores for different numbers of clusters, we can choose the best number of clusters for the data. This method helps to visually understand and assess the separation and quality of the clusters formed.",
    "crumbs": [
      "EV Insights",
      "Clustering"
    ]
  },
  {
    "objectID": "5000-website/clustering/clustering.html#hierarchical-clustering",
    "href": "5000-website/clustering/clustering.html#hierarchical-clustering",
    "title": "Clustering",
    "section": "Hierarchical Clustering:",
    "text": "Hierarchical Clustering:\nIn hierarchical clustering, each data point is initially treated as a single cluster. For example, every row in a DataFrame is considered an individual cluster, and the algorithm searches for rows with the most similar attributes to gather together. The clustering results are organized and visualized using dendrograms, which offer a bottom-up view of the separation of different clusters.\nVarious distance metrics are used to determine the similarity between data points, with Euclidean distance being the most commonly used. In this notebook, we will explore different distance metrics and interpret their effectiveness in clustering analysis.",
    "crumbs": [
      "EV Insights",
      "Clustering"
    ]
  },
  {
    "objectID": "5000-website/clustering/clustering.html#final-results",
    "href": "5000-website/clustering/clustering.html#final-results",
    "title": "Clustering",
    "section": "Final Results",
    "text": "Final Results",
    "crumbs": [
      "EV Insights",
      "Clustering"
    ]
  },
  {
    "objectID": "5000-website/clustering/clustering.html#kmeans-2",
    "href": "5000-website/clustering/clustering.html#kmeans-2",
    "title": "Clustering",
    "section": "KMEANS",
    "text": "KMEANS\n\n\nShow the code\n# KMEANS\nmodel = sklearn.cluster.KMeans(n_clusters=2).fit(X)\nlabels=model.predict(X)\nplot(X,labels)\n\n\n/Users/isfarbaset/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n\n\n\n\n\n\n\n\n\nWe can see very distinct clusterings for two groups formed. The purple datapoints are more compact & dense while the yellow data points are more spread out.",
    "crumbs": [
      "EV Insights",
      "Clustering"
    ]
  },
  {
    "objectID": "5000-website/clustering/clustering.html#dbscan-2",
    "href": "5000-website/clustering/clustering.html#dbscan-2",
    "title": "Clustering",
    "section": "DBSCAN",
    "text": "DBSCAN\n\n\nShow the code\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.metrics import silhouette_score\n\nopt_eps = 3 # optimal eps value found from silhouette analysis\nopt_dbscan = DBSCAN(eps=opt_eps)\nopt_labels = opt_dbscan.fit_predict(X)\n\n# Calculate the silhouette score for the optimal parameter\nopt_silhouette = silhouette_score(X, opt_labels)\n\n# Print the final silhouette score\nprint(f\"Final silhouette score with eps={opt_eps}: {opt_silhouette}\")\n\n# Plot the final clustering\nplt.scatter(X[:, 0], X[:, 1], c=opt_labels, cmap='viridis', alpha=0.5)\nplt.title(f'DBSCAN Clustering with eps={opt_eps}')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.show()\n\n\nFinal silhouette score with eps=3: 0.34308414158319966\n\n\n\n\n\n\n\n\n\nA silhouette score close to 1 indicates very distinct clusters, so a score of 0.343 implies that while there is some cluster structure, there’s room for improvement. The graph illustrates two very distinct groupings between the purple and yellow points",
    "crumbs": [
      "EV Insights",
      "Clustering"
    ]
  },
  {
    "objectID": "5000-website/clustering/clustering.html#hierchical-clustering-1",
    "href": "5000-website/clustering/clustering.html#hierchical-clustering-1",
    "title": "Clustering",
    "section": "Hierchical Clustering",
    "text": "Hierchical Clustering\n\n\nShow the code\n# Hierchical\nmodel = sklearn.cluster.AgglomerativeClustering(n_clusters=3).fit(X)\nlabels=model.labels_\nplot(X,labels)\n\n\n\n\n\n\n\n\n\nBased on this graph, we can observe that the green group is clearly separate, suggesting strong similarity among its points. The purple and yellow groups overlap a bit, indicating they’re not as clearly defined. This clustering tells us that the data likely has a natural grouping, especially for the green group, and that the second feature is probably more important for grouping than the first.\nWhich method seemed to work the best and why: &gt; Kmeans seemed to have worked best since it creates two distict groups using the optimal paramter of 2 which was the result obtained of the elbow method of hyperparameter tuning.\nWhich was easier to use or preferable: &gt; All three of them were easy to use, thanks to python’s schikit-learn library making the algorithms very accessible. Kmeans and DBSCAN was more intuitive to use when compared to hierchical agglomerative clustering.\nDid the clustering results provide any new insights into your data: &gt; From the way the clusters are arranged, it can be observe that there is a clear distinction between the two clusters along the Feature 2 (x_2) axis, but not as much along the Feature 1 (x_1) axis. The separation suggests that the clustering algorithms were able to find a division in the data, possibly around a significant difference in the values of Feature 2. This might imply that Feature 2 is a key variable in differentiating the data points into two groups.",
    "crumbs": [
      "EV Insights",
      "Clustering"
    ]
  },
  {
    "objectID": "5000-website/clustering/clustering.html#conclusion",
    "href": "5000-website/clustering/clustering.html#conclusion",
    "title": "Clustering",
    "section": "Conclusion",
    "text": "Conclusion\nWe’ve completed an in-depth analysis of our dataset and identified clear groupings which align with our initial goal of investigating the fuel types used by vehicles, such as electric and gas. The results show a distinct separation between these groups. By further examining the size of these clusters, we’ll be able to discern the more common type of vehicle on the road, thus gaining insight into the prevalence of each fuel type. This can be beneficial information for automotive manufacturers to gain insight into the demand of a particular fuel type and adjust production capacity accordingly. If we did not have previous knowledge about the variation in fuel types beforehand, these clustering analysis mechanisms would sufficiently have revealed the existence of two primary categories of cars distinguished by a significant feature.",
    "crumbs": [
      "EV Insights",
      "Clustering"
    ]
  },
  {
    "objectID": "5000-website/clustering/clustering.html#references",
    "href": "5000-website/clustering/clustering.html#references",
    "title": "Clustering",
    "section": "References",
    "text": "References\n\nhttps://medium.com/nerd-for-tech/k-means-clustering-using-python-2150769bd0b9 \nhttps://www.geeksforgeeks.org/hierarchical-clustering-in-data-mining/ \nhttps://jfh.georgetown.domains/dsan5000/slides-and-labs/_site/content/slides/clustering/notes.html#run-with-true-number-of-clusters \nhttps://towardsdatascience.com/k-means-dbscan-gmm-agglomerative-clustering-mastering-the-popular-models-in-a-segmentation-c891a3818e29 \nhttps://www.youtube.com/watch?v=4b5d3muPQmA \nhttps://www.youtube.com/watch?v=RDZUdRSDOok",
    "crumbs": [
      "EV Insights",
      "Clustering"
    ]
  },
  {
    "objectID": "5000-website/conclusions.html",
    "href": "5000-website/conclusions.html",
    "title": "Conclusions",
    "section": "",
    "text": "Data Gathering: I utilized several sources to collect a diverse set of data, working with both text and record data to apply different machine learning algorithms. In hindsight, a larger dataset might have yielded more fruitful results. Nevertheless, I gained significant knowledge about web scraping methods for text data collection and the use of APIs for data gathering.\nData Cleaning: This is an iterative process and I learned its importance quite a bit through the experience of building the project. While cleaning the data it’s absolutely necessary to ensure that the integrity is maintained. If data is not cleaned properly, several issues may arise later on while appyling different kinds of ML algorthims to the data at hand.\nData Exploration: This is my personal favorite stage in the Data Science lifecycle. Several insights and visualizations were genrated during this stage. Exploring the data uncovers many interesting facts and helps generate a more informative narrative to the data storytelling process.\nNaïve Bayes: Using python to carry out the naive bayes algorithm on both text and record data was hugely educational. It was interesting to get practical, hands on experience on training a model based on the naive bayes theory and observe how well it performs with unseen data.\nDimensionality Reduction: Observing how efficiently both PCA and t-SNE performs dimensionality reduction while preserving essential information was a highlight of the portfolio creation process. I was able to perform the two metholodogies and then create a comparison report for them both.\nClustering: Kmeans, dbscan and hierchical clustering all showed clear groupings within the dataset that were used to perform the clustering analyses. This helped observe the common characteristics amongst the automobile vehicles that are grouped together and the difference amongst the ones which are grouped farther apart.\nDecision Trees: Performing the classfication and regression decision trees unraveled how well the datasets I collected perform with the decision tree algorithms provided by scikit-learn. It is an useful method of grouping text data and visualize the context within which certain keywords are being utilised.\nARM: Yet another extremely beneficial algorithm when it comes to finding patterns in text data. The findings from this algorithm and how different words are connected was very insightful. Though this is a computationally intensive process and my result was likely not the most optimal one, it was still great to learn from the outcome.\n\nAdditionally…\nWhile the initial questions may still require further analysis to provide strong, evidence-based answers to, applying machine learning algorithms at different project phases has revealed numerous trends and patterns. Although there is room for fine tuning and making adjustments, the insights gained at this stage are already beneficial and clearly demonstrate how they can aid businesses in making better, data-driven decisions.\nThroughout the process of closely following the Data Science Lifecycle to analyze the EV market, several key insights were discovered. Let’s revisit some of the visualizations from the various analyses carried out so far:\n\n\nStock Prices Graph\nThis graph was generated from data gathered from Yahoo!\n\nThe visual representation of the stock performance of EVs, illustrating their rise to surpass market leaders like Toyota, followed by their subsequent slump, and then their quick recovery, was one of the most insightful outcomes of the project.\n\n\nWord Cloud\nThis was generated using the text data gathered from News API.\n\nSome prominent words:  - EV: This stands for ‘Electric Vehicles’ and it makes sense that this is one of the most commonly found terms within the dataset  - Tesla: We have closely followed their market journey via share market. As one the early pioneers of the EV industry and mainstream manufacturers of EVs, it is understandable that their name appears commonly whenever their is a conversation about electric vehicles.  - Electric: this is the type of vehicles I was interested in learning more about  - will: not sure what this one means… maybe the future tense?  - Lucid: An up and coming EV OEM. Them showing up in news articles about EVs is testament to the fact that they are making strong strides in the market.\n\n\nTesla Stock\n\nSince Tesla is the most dominant company when it comes to EVs, it was interesting explore how their stocks performed over the course of this year.\n\n\nTesla VS. GM\n\nIn 2023 Tesla performed consistently better than GM stocks. Further market analysis could be performed to understand the cause. However, this comparison highlights just how much in demand EVs, in particula Teslas, currently are.\n\n\nTesla/STLA Correlation\n\nThis was an interesting finding, a positive correlation between the stock prices of Tesla and Stellantis. Correlation does not mean causation, hence there is no definite reason behind this phenomenon, but an interesting observation nonetheless.\n\n\nCorrelation amongst various OEMs\n\nTaking a broader look at the correlation amongst different OEMs and it seems like some brands have positive correlations while others do not.\n\n\nSentiment Analysis\nText data for generating this graph was collected from News API.\n\nThis was an outcome during the phase of performing ARM. In general sentiment regarding EV is positive which is a great conclusion. This provides strong evidence that EV is leaving a positive impact.",
    "crumbs": [
      "EV Insights",
      "Conclusions"
    ]
  },
  {
    "objectID": "5000-website/naive-bayes/naive_bayes.html",
    "href": "5000-website/naive-bayes/naive_bayes.html",
    "title": "Naïve Bayes",
    "section": "",
    "text": "The Naive Bayes Algorithm is a vital machine learning tool for classification based on Bayes’ probability theory. It excels in training high-dimensional datasets for text classification. Common uses include sentiment analysis, news article classification, and spam filtering. It assumes all features are independent and uses various distributions like Bernoulli for binary data, Multinomial for word counts, and Gaussian for continuous values. By storing feature probabilities for each class, it calculates the likelihood of an observation belonging to a particular class.\nThe objective in this section includes preparing textual and record data for performing naive bayes classification. The goal is to select the optimal number of features that will increase the likelihood of the model to predict different gorupings accurately.\nThere are different variants of Naive Bayes such as Gaussian, Multinomial and Bernoulli Naive Bayes. These variants are tailored for different types of data: Gaussian for continuous data that follow a normal distribution, Multinomial for discrete counts like in text analysis, and Bernoulli for binary data outcomes. Selecting the right variant is crucial; for example, text classification would be best served by Multinomial, while binary outcomes would lean on Bernoulli, and continuous data on Gaussian.",
    "crumbs": [
      "EV Insights",
      "Naïve Bayes"
    ]
  },
  {
    "objectID": "5000-website/naive-bayes/naive_bayes.html#preparing-data-for-naïve-bayes",
    "href": "5000-website/naive-bayes/naive_bayes.html#preparing-data-for-naïve-bayes",
    "title": "Naïve Bayes",
    "section": "Preparing Data for Naïve Bayes:",
    "text": "Preparing Data for Naïve Bayes:",
    "crumbs": [
      "EV Insights",
      "Naïve Bayes"
    ]
  },
  {
    "objectID": "5000-website/naive-bayes/naive_bayes.html#search-1-remove-features-from-high-to-low",
    "href": "5000-website/naive-bayes/naive_bayes.html#search-1-remove-features-from-high-to-low",
    "title": "Naïve Bayes",
    "section": "Search-1: Remove features from high to low",
    "text": "Search-1: Remove features from high to low\n\n\nShow the code\n#UTILITY FUNCTION TO INITIALIZE RELEVANT ARRAYS\ndef initialize_arrays():\n    global num_features,train_accuracies\n    global test_accuracies,train_time,eval_time\n    num_features=[]\n    train_accuracies=[]\n    test_accuracies=[]\n    train_time=[]\n    eval_time=[]\n\n\n\n\nShow the code\n# INITIALIZE ARRAYS\ninitialize_arrays()\n\n# DEFINE SEARCH FUNCTION\ndef partial_grid_search(num_runs, min_index, max_index):\n    for i in range(1, num_runs+1):\n        # SUBSET FEATURES \n        upper_index=min_index+i*int((max_index-min_index)/num_runs)\n        xtmp=x[:,0:upper_index]\n\n        #TRAIN \n        (acc_train,acc_test,time_train,time_eval)=train_MNB_model(xtmp,y,i_print=False)\n\n        if(i%5==0):\n            print(i,upper_index,xtmp.shape[1],acc_train,acc_test)\n            \n        #RECORD \n        num_features.append(xtmp.shape[1])\n        train_accuracies.append(acc_train)\n        test_accuracies.append(acc_test)\n        train_time.append(time_train)\n        eval_time.append(time_eval)\n\n# DENSE SEARCH (SMALL NUMBER OF FEATURES (FAST))\npartial_grid_search(num_runs=100, min_index=0, max_index=1000)\n\n# SPARSE SEARCH (LARGE NUMBER OF FEATURES (SLOWER))\npartial_grid_search(num_runs=20, min_index=1000, max_index=10000)\n\n\n5 50 50 82.44897959183673 77.9591836734694\n10 100 100 84.28571428571429 76.32653061224491\n15 150 150 85.0 74.28571428571429\n20 200 200 84.28571428571429 74.6938775510204\n25 250 250 85.91836734693878 75.51020408163265\n30 300 300 85.81632653061224 74.28571428571429\n35 350 350 85.81632653061224 76.32653061224491\n40 400 400 86.73469387755102 75.91836734693878\n45 450 450 88.26530612244898 76.73469387755102\n50 500 500 87.55102040816325 78.36734693877551\n55 550 550 88.06122448979592 79.59183673469387\n60 600 600 88.46938775510203 79.18367346938776\n65 650 650 89.48979591836735 79.18367346938776\n70 700 700 89.38775510204081 80.0\n75 750 750 89.59183673469387 80.81632653061224\n80 800 800 90.0 80.81632653061224\n85 850 850 90.10204081632654 80.81632653061224\n90 900 900 90.20408163265307 79.59183673469387\n95 950 950 89.6938775510204 80.81632653061224\n100 1000 1000 90.10204081632654 82.85714285714286\n5 3250 3250 94.48979591836735 82.85714285714286\n10 5500 5500 95.40816326530613 84.08163265306122\n15 7750 7140 95.71428571428572 84.48979591836735\n20 10000 7140 95.71428571428572 84.48979591836735\n\n\n\nTILITY FUNCTION TO SAVE RESULTS\n\n\nShow the code\n#UTILITY FUNCTION TO SAVE RESULTS\ndef save_results(path_root):\n    out=np.transpose(np.array([num_features,train_accuracies,test_accuracies,train_time,eval_time])) \n    out=pd.DataFrame(out)\n    out.to_csv(path_root+\".csv\")\n\n\n\n\nUTILITY FUNCTION TO PLOT RESULTS\n\n\nShow the code\n#UTILITY FUNCTION TO PLOT RESULTS\ndef plot_results(path_root):\n\n    #PLOT-1\n    plt.plot(num_features,train_accuracies,'-or')\n    plt.plot(num_features,test_accuracies,'-ob')\n    plt.xlabel('Number of features')\n    plt.ylabel('ACCURACY: Training (blue) and Test (red)')\n    plt.savefig(path_root+'-1.png')\n    plt.show()\n\n    # #PLOT-2\n    plt.plot(num_features,train_time,'-or')\n    plt.plot(num_features,eval_time,'-ob')\n    plt.xlabel('Number of features')\n    plt.ylabel('Runtime: training time (red) and evaluation time(blue)')\n    plt.savefig(path_root+'-2.png')\n    plt.show()\n\n    # #PLOT-3\n    plt.plot(np.array(test_accuracies),train_time,'-or')\n    plt.plot(np.array(test_accuracies),eval_time,'-ob')\n    plt.xlabel('test_accuracies')\n    plt.ylabel('Runtime: training time (red) and evaluation time (blue)')\n    plt.savefig(path_root+'-3.png')\n    plt.show()\n\n    # #PLOT-3\n    plt.plot(num_features,np.array(train_accuracies)-np.array(test_accuracies),'-or')\n    plt.xlabel('Number of features')\n    plt.ylabel('train_accuracies-test_accuracies')\n    plt.savefig(path_root+'-4.png')\n    plt.show()\n\n\n\n\nShow the code\nsave_results(output_dir+\"/partial_grid_search\")\nplot_results(output_dir+\"/partial_grid_search\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFrom the images we can conclude that there’s a trade-off between test accuracy and runtime; higher accuracies demand more processing time. As the number of features increases, initially there’s a spike in test accuracy, which stabilizes and then slightly drops after peaking. The runtime generally rises with the number of features, but with several fluctuations.Training accuracy is consistently higher than test accuracy, indicating potential overfitting with more features.",
    "crumbs": [
      "EV Insights",
      "Naïve Bayes"
    ]
  },
  {
    "objectID": "5000-website/naive-bayes/naive_bayes.html#variancethreshold",
    "href": "5000-website/naive-bayes/naive_bayes.html#variancethreshold",
    "title": "Naïve Bayes",
    "section": "VarianceThreshold",
    "text": "VarianceThreshold\n\n\nShow the code\nx_var=np.var(x,axis=0)\nprint(np.min(x_var))\nprint(np.max(x_var))\n\n\n0.0008156601416076161\n0.24855910037484058\n\n\n\n\nShow the code\nfrom sklearn.feature_selection import VarianceThreshold\n\n# DEFINE GRID OF THRESHOLDS \nnum_thresholds=30\nthresholds=np.linspace(np.min(x_var),np.max(x_var),num_thresholds)\n\n#DOESN\"T WORK WELL WITH EDGE VALUES \nthresholds=thresholds[1:-2]; #print(thresholds)\n\n# INITIALIZE ARRAYS\ninitialize_arrays()\n\n# SEARCH FOR OPTIMAL THRESHOLD\nfor THRESHOLD in thresholds:\n    feature_selector = VarianceThreshold(threshold=THRESHOLD)\n    xtmp=feature_selector.fit_transform(x)\n    print(\"THRESHOLD =\",THRESHOLD, xtmp.shape[1])\n\n    (acc_train,acc_test,time_train,time_eval)=train_MNB_model(xtmp,y,i_print=False)\n             \n    #RECORD \n    num_features.append(xtmp.shape[1])\n    train_accuracies.append(acc_train)\n    test_accuracies.append(acc_test)\n    train_time.append(time_train)\n    eval_time.append(time_eval)\n\n\nTHRESHOLD = 0.009358537391029442 1088\nTHRESHOLD = 0.01790141464045127 609\nTHRESHOLD = 0.026444291889873094 414\nTHRESHOLD = 0.03498716913929492 278\nTHRESHOLD = 0.04353004638871675 211\nTHRESHOLD = 0.05207292363813857 155\nTHRESHOLD = 0.0606158008875604 127\nTHRESHOLD = 0.06915867813698222 95\nTHRESHOLD = 0.07770155538640404 73\nTHRESHOLD = 0.08624443263582587 61\nTHRESHOLD = 0.0947873098852477 52\nTHRESHOLD = 0.10333018713466952 43\nTHRESHOLD = 0.11187306438409135 36\nTHRESHOLD = 0.12041594163351317 32\nTHRESHOLD = 0.128958818882935 28\nTHRESHOLD = 0.13750169613235683 24\nTHRESHOLD = 0.14604457338177865 21\nTHRESHOLD = 0.15458745063120047 17\nTHRESHOLD = 0.16313032788062232 13\nTHRESHOLD = 0.17167320513004414 13\nTHRESHOLD = 0.18021608237946596 11\nTHRESHOLD = 0.18875895962888778 11\nTHRESHOLD = 0.1973018368783096 8\nTHRESHOLD = 0.20584471412773142 8\nTHRESHOLD = 0.21438759137715327 7\nTHRESHOLD = 0.2229304686265751 6\nTHRESHOLD = 0.2314733458759969 6\n\n\n\nCHECK RESULTS\n\n\nShow the code\n# CHECK RESULTS \nsave_results(output_dir+\"/variance_threshold\")\nplot_results(output_dir+\"/variance_threshold\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFrom the images, as test accuracies increase, runtime for training and evaluation generally rises. Using more features initially leads to fluctuating train-test accuracy differences, but stabilizes as features approach 1000. Overall accuracy for both training and test sets tends to increase with more features, despite some fluctuations. Training and evaluation time spikes around 200 features, then gradually decreases and remains relatively stable.",
    "crumbs": [
      "EV Insights",
      "Naïve Bayes"
    ]
  },
  {
    "objectID": "5000-website/naive-bayes/naive_bayes.html#naïve-bayes-nb-with-labeled-text-data",
    "href": "5000-website/naive-bayes/naive_bayes.html#naïve-bayes-nb-with-labeled-text-data",
    "title": "Naïve Bayes",
    "section": "Naïve Bayes (NB) with Labeled Text Data",
    "text": "Naïve Bayes (NB) with Labeled Text Data\nWhen the model is trained, insert code to output the following information about the training and test set * Remember that the test set was NOT seen during the training process, and therefore “test” predictions show how the model does on new “unseen” data\n\n\nShow the code\nfrom sklearn.naive_bayes import MultinomialNB\n\n# INITIALIZE MODEL \nmodel = MultinomialNB()\n\n# TRAIN MODEL \nmodel.fit(x_train,y_train)\n\n# PRINT REPORT USING UTILITY FUNCTION ABOVE\nprint_model_summary()\n\n\nACCURACY CALCULATION\n\nTRAINING SET:\nAccuracy: 74.48979591836735\nNumber of mislabeled points out of a total 980 points = 250\n\nTEST SET (UNTRAINED DATA):\nAccuracy: 49.38775510204081\nNumber of mislabeled points out of a total 245 points = 124\n\nCHECK FIRST 20 PREDICTIONS\nTRAINING SET:\n[0 1 1 2 1 2 2 1 0 0 1 1 2 0 2 0 1 0 0 2]\n[2 2 2 0 1 2 2 1 0 0 1 1 2 0 0 0 0 0 0 0]\nERRORS: [ 2  1  1 -2  0  0  0  0  0  0  0  0  0  0 -2  0 -1  0  0 -2]\n\nTEST SET (UNTRAINED DATA):\n[2 0 2 0 1 2 2 2 0 1 2 1 0 0 2 0 0 1 2 2]\n[0 1 0 2 1 0 2 2 0 1 1 1 2 0 1 0 2 1 2 0]\nERRORS: [-2  1 -2  2  0 -2  0  0  0  0 -1  0  2  0 -1  0  2  0  0 -2]\n\n\nOverfitting happens when a model performs well on training data but poorly on new, unseen data. Under-fitting is when a model performs badly on both. With a training accuracy of 74.49% and a testing accuracy of 49.39%, the model seems to be overfitting, as it’s not generalizing well to the test data.\n\n\nShow the code\n# source: https://www.datacamp.com/tutorial/naive-bayes-scikit-learn\nfrom sklearn.metrics import (\n    accuracy_score,\n    confusion_matrix,\n    ConfusionMatrixDisplay,\n    f1_score,\n    classification_report,\n)\n\ny_pred = model.predict(x_test)\n\naccuray = accuracy_score(y_pred, y_test)\nf1 = f1_score(y_pred, y_test, average=\"weighted\")\n\nprint(\"Accuracy:\", accuray)\nprint(\"F1 Score:\", f1)\n\n\nAccuracy: 0.49387755102040815\nF1 Score: 0.4899780925165164\n\n\nThe model was trained on a dataset and achieved an accuracy of 74.49% on the training set, but only 49.39% on the test set. The first 20 predictions and their errors are provided for both sets, indicating how much each prediction deviated from the expected result.\n\nConfusion matrix\n\n\nShow the code\n# source: https://developer.ibm.com/tutorials/awb-classifying-data-multinomial-naive-bayes-algorithm/\nlabels = [\"electric vehicle\", \"gasoline vehicle\", \"hybrid vehicle\"]\n\ncm = confusion_matrix(y_test, y_pred)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\ndisp.plot();\n\n\n\n\n\n\n\n\n\nGasoline vehicles are most accurately predicted with 61 correct predictions. Electric vehicles are often misclassified as hybrid, with 34 such cases. Hybrid vehicles have a relatively even misclassification spread between electric and gasoline, with 33 and 18 instances, respectively.\n\n\nCompute distance matrix\nDistance between sentence vectors for a subset of data\n\n\nShow the code\nnum_rows_keep=250\nindex=np.sort(np.random.choice(X.shape[0], num_rows_keep, replace=False))\n# print(y1[index])\n#print(index)\ntmp1=X[index, :]\n# print(tmp1.shape,tmp1.dtype,tmp1[:,].shape)\n\n#COMPUTE DISTANCE MATRIX\ndij=[]\n\n#LOOP OVER ROWS\nfor i in range(0,tmp1.shape[0]):\n    tmp2=[]\n    #LOOP OVER ROWS\n    for j in range(0,tmp1.shape[0]):\n\n        #EXTRACT VECTORS\n        vi=tmp1[i,:]\n        vj=tmp1[j,:]\n        #print(vi.shape,vj.shape)\n\n        #COMPUTE DISTANCES\n        dist=np.dot(vi, vj)/(np.linalg.norm(vi)*np.linalg.norm(vj)) #cosine sim\n        #dist=np.linalg.norm(vi-vj) #euclidean\n\n        # BUILD DISTANCE MATRIX\n        if(i==j or np.max(vi) == 0 or np.max(vj)==0):\n            tmp2.append(0)\n        else:\n            tmp2.append(dist)\n    dij.append(tmp2); #print(dij)\n        # raise\ndij=np.array(dij)\n\n#normalize\n# dij=(dij-np.min(dij))/(np.max(dij)-np.min(dij))\n\n#Lower triangle of an array.\n# dij=np.sort(dij,axis=0)\n# dij=np.sort(dij,axis=1)\n# dij=np.tril(dij, k=-1) \n\n\nimport seaborn as sns\n# sns.heatmap(np.exp(dij), annot=False) #,  linewidths=.05)\nsns.heatmap(dij, annot=False) #,  linewidths=.05)\nprint(dij.shape)\nprint(dij)\n\n\n(250, 250)\n[[0.         0.14535047 0.06180232 ... 0.02722664 0.06460021 0.12036163]\n [0.14535047 0.         0.1328735  ... 0.04682929 0.11111111 0.05175492]\n [0.06180232 0.1328735  0.         ... 0.08960215 0.1062988  0.02200594]\n ...\n [0.02722664 0.04682929 0.08960215 ... 0.         0.06243905 0.03877834]\n [0.06460021 0.11111111 0.1062988  ... 0.06243905 0.         0.02300219]\n [0.12036163 0.05175492 0.02200594 ... 0.03877834 0.02300219 0.        ]]\n\n\n\n\n\n\n\n\n\n\n\nPCA\n\n\nShow the code\nfrom sklearn.decomposition import PCA\n\n# COMPUTE PCA WITH 10 COMPONENTS\npca = PCA(n_components=10)\npca.fit(X)\nprint(pca.explained_variance_ratio_)\nprint(pca.singular_values_)\n\n# GET PRINCIPLE COMPONENT PROJECTIONS \nprincipal_components = pca.fit_transform(X)\ndf2 = pd.DataFrame(data = principal_components) #, columns = ['PC1','PC2','PC3','PC4','PC5'])\ndf3=pd.concat([df2,df['label']], axis=1)\n\n# FIRST TWO COMPONENTS\nsns.scatterplot(data=df2, x=0, y=1,hue=df[\"label\"]) \nplt.show()\n\n#3D PLOT\nax = plt.axes(projection='3d')\nax.scatter3D(df2[0], df2[1], df2[2], c=y1);\nplt.show()\n\n#PAIRPLOT\nsns.pairplot(data=df3,hue=\"label\") #.to_numpy()) #,hue=df[\"label\"]) #, hue=\"time\")\nplt.show()\n\n\n[0.02781853 0.01541431 0.0131166  0.01048456 0.00973911 0.00917533\n 0.00862828 0.00807481 0.00696539 0.00647609]\n[43.65947765 32.49926167 29.97933483 26.80319506 25.83277827 25.07392669\n 24.31495725 23.52218064 21.84661137 21.06531287]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n/Users/isfarbaset/anaconda3/lib/python3.11/site-packages/seaborn/axisgrid.py:118: UserWarning: The figure layout has changed to tight\n  self._figure.tight_layout(*args, **kwargs)\n\n\n\n\n\n\n\n\n\nThe visualizations display data on vehicle types: electric, gasoline, and hybrid. The first two plots show the spread of data points in 2D and 3D, with each color representing a vehicle type. The third plot provides a pairwise comparison of data features, showcasing distributions and relationships between features for each vehicle type.",
    "crumbs": [
      "EV Insights",
      "Naïve Bayes"
    ]
  },
  {
    "objectID": "5000-website/naive-bayes/naive_bayes.html#preparing-data-for-naïve-bayes-1",
    "href": "5000-website/naive-bayes/naive_bayes.html#preparing-data-for-naïve-bayes-1",
    "title": "Naïve Bayes",
    "section": "Preparing Data for Naïve Bayes:",
    "text": "Preparing Data for Naïve Bayes:",
    "crumbs": [
      "EV Insights",
      "Naïve Bayes"
    ]
  },
  {
    "objectID": "5000-website/naive-bayes/naive_bayes.html#naïve-bayes-nb-with-labeled-record-data",
    "href": "5000-website/naive-bayes/naive_bayes.html#naïve-bayes-nb-with-labeled-record-data",
    "title": "Naïve Bayes",
    "section": "Naïve Bayes (NB) with Labeled Record Data",
    "text": "Naïve Bayes (NB) with Labeled Record Data\nThe record data used in this section is the automobile dataset collected from Rapid API.\n\n\nShow the code\nfrom sklearn.naive_bayes import MultinomialNB\n\n# INITIALIZE MODEL \nmodel = MultinomialNB()\n\n# TRAIN MODEL \nmodel.fit(x_train,y_train)\n\n# PRINT REPORT USING UTILITY FUNCTION ABOVE\nprint_model_summary()\n\n\nACCURACY CALCULATION\n\nTRAINING SET:\nAccuracy: 83.75\nNumber of mislabeled points out of a total 80 points = 13\n\nTEST SET (UNTRAINED DATA):\nAccuracy: 60.0\nNumber of mislabeled points out of a total 20 points = 8\n\nCHECK FIRST 20 PREDICTIONS\nTRAINING SET:\n[11  9 13 14  1 13  3 11  0 11 11 11 13  7 13 13 11 10 11 13]\n[11  9 13 11  1 13 13 11  0 11 11 11 13 13 13 13 11 13 11 13]\nERRORS: [ 0  0  0 -3  0  0 10  0  0  0  0  0  0  6  0  0  0  3  0  0]\n\nTEST SET (UNTRAINED DATA):\n[13  1  5  5 11 13  8 13 13 11 13  3  6 10  1 13  2  3 13 13]\n[13  3 13  5 11 13 13 13 13 11 13  3 13 13 11 13 11 11 13 13]\nERRORS: [ 0  2  8  0  0  0  5  0  0  0  0  0  7  3 10  0  9  8  0  0]\n\n\nOverfitting is when a model performs well on training data but poorly on testing data Under-fitting is when a model performs poorly on both. This model has a training accuracy of 83.75% and a testing accuracy of 60%. This significant drop suggests the model might be overfitting since it’s not generalizing well to unseen data.\n\nCalculate Accuray and F1 Score:\n\n\nShow the code\n# source: https://www.datacamp.com/tutorial/naive-bayes-scikit-learn\nfrom sklearn.metrics import (\n    accuracy_score,\n    confusion_matrix,\n    ConfusionMatrixDisplay,\n    f1_score,\n    classification_report,\n)\n\ny_pred = model.predict(x_test)\n\naccuray = accuracy_score(y_pred, y_test)\nf1 = f1_score(y_pred, y_test, average=\"weighted\")\n\nprint(\"Accuracy:\", accuray)\nprint(\"F1 Score:\", f1)\n\n\nAccuracy: 0.6\nF1 Score: 0.7061904761904763\n\n\nThe model achieved ~ 81.25% accuracy on the training set and 60% on the test set. The test set had 8 mislabeled points out of 20. The provided predictions for both sets show specific errors compared to the expected values.\n\n\nShow the code\nprint(\"Unique classes in y_test:\", np.unique(y_test))\nprint(\"Number of unique classes in y_test:\", len(np.unique(y_test)))\n\nprint(\"Unique classes in y_pred:\", np.unique(y_pred))\nprint(\"Number of unique classes in y_pred:\", len(np.unique(y_pred)))\n\n\nUnique classes in y_test: [ 1  2  3  5  6  8 10 11 13]\nNumber of unique classes in y_test: 9\nUnique classes in y_pred: [ 3  5 11 13]\nNumber of unique classes in y_pred: 4\n\n\n\n\nConfusion matrix\n\n\nShow the code\n# source: https://developer.ibm.com/tutorials/awb-classifying-data-multinomial-naive-bayes-algorithm/\n\nlabels = [\"BMW\", \"Cadillac\", \"Chevrolet\", \"Ford\", \"Honda\", \"Jaguar\", \"Kia\", \"Nissan\", \"Tesla\"]\n\ncm = confusion_matrix(y_test, y_pred)\n# Normalize the confusion matrix\ncm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n\nfig, ax = plt.subplots(figsize=(10, 8))\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm_normalized, display_labels=labels)\ndisp.plot(cmap='Blues', ax=ax, values_format=\".2f\")\n\nplt.title('Confusion Matrix')\nplt.xlabel('Predicted Label')\nplt.ylabel('True Label')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nThe confusion matrix shows predictions for car brands. Most brands were correctly predicted with a 1.0 score, except for Ford, which was misclassified 50% of the time.\n\n\nCompute distance matrix\nDistance between sentence vectors for a subset of data\n\n\nShow the code\nimport numpy as np\n\nnum_rows_keep=250\nindex=np.sort(np.random.choice(x.shape[0], num_rows_keep))\n# print(y1[index])\n#print(index)\n# tmp1=x[index, :]\nx_np = x.to_numpy()\ntmp1 = x_np[index, :]\n# print(tmp1.shape,tmp1.dtype,tmp1[:,].shape)\n\n#COMPUTE DISTANCE MATRIX\ndij=[]\n\n#LOOP OVER ROWS\nfor i in range(0,tmp1.shape[0]):\n    tmp2=[]\n    #LOOP OVER ROWS\n    for j in range(0,tmp1.shape[0]):\n\n        #EXTRACT VECTORS\n        vi=tmp1[i,:]\n        vj=tmp1[j,:]\n        #print(vi.shape,vj.shape)\n\n        #COMPUTE DISTANCES\n        dist=np.dot(vi, vj)/(np.linalg.norm(vi)*np.linalg.norm(vj)) #cosine sim\n        #dist=np.linalg.norm(vi-vj) #euclidean\n\n        # BUILD DISTANCE MATRIX\n        if(i==j or np.max(vi) == 0 or np.max(vj)==0):\n            tmp2.append(0)\n        else:\n            tmp2.append(dist)\n    dij.append(tmp2); #print(dij)\n        # raise\ndij=np.array(dij)\n\n#normalize\n# dij=(dij-np.min(dij))/(np.max(dij)-np.min(dij))\n\n#Lower triangle of an array.\n# dij=np.sort(dij,axis=0)\n# dij=np.sort(dij,axis=1)\n# dij=np.tril(dij, k=-1) \n\n\nimport seaborn as sns\n# sns.heatmap(np.exp(dij), annot=False) #,  linewidths=.05)\nsns.heatmap(dij, annot=False) #,  linewidths=.05)\nprint(dij.shape)\nprint(dij)\n\n\n(250, 250)\n[[ 0.          0.00315014  0.00315014 ... -0.00280833 -0.00280833\n  -0.00280833]\n [ 0.00315014  0.          0.00315014 ... -0.00280833 -0.00280833\n  -0.00280833]\n [ 0.00315014  0.00315014  0.         ... -0.00280833 -0.00280833\n  -0.00280833]\n ...\n [-0.00280833 -0.00280833 -0.00280833 ...  0.         -0.00220795\n  -0.00220795]\n [-0.00280833 -0.00280833 -0.00280833 ... -0.00220795  0.\n  -0.00220795]\n [-0.00280833 -0.00280833 -0.00280833 ... -0.00220795 -0.00220795\n   0.        ]]\n\n\n\n\n\n\n\n\n\nThis is a heatmap representing data values in a matrix format. The diagonal line suggests a correlation between corresponding x and y values. Color intensity indicates the magnitude of the data, with darker shades likely representing higher values and lighter shades representing lower values. The color bar on the right provides a reference for the data values.\n\n\nPCA\n\n\nShow the code\nfrom sklearn.decomposition import PCA\n\n# COMPUTE PCA WITH 10 COMPONENTS\npca = PCA(n_components=10)\npca.fit(X)\nprint(pca.explained_variance_ratio_)\nprint(pca.singular_values_)\n\n# GET PRINCIPLE COMPONENT PROJECTIONS \nprincipal_components = pca.fit_transform(X)\ndf2 = pd.DataFrame(data = principal_components) #, columns = ['PC1','PC2','PC3','PC4','PC5'])\ndf3=pd.concat([df2,df['Make']], axis=1)\n\n# FIRST TWO COMPONENTS\nsns.scatterplot(data=df2, x=0, y=1,hue=df[\"Make\"]) \nplt.show()\n\n#3D PLOT\nax = plt.axes(projection='3d')\nax.scatter3D(df2[0], df2[1], df2[2], c=y1);\nplt.show()\n\n#PAIRPLOT\nsns.pairplot(data=df3,hue=\"Make\") #.to_numpy()) #,hue=df[\"label\"]) #, hue=\"time\")\nplt.show()\n\n\n[0.02781853 0.01541431 0.01311659 0.01048451 0.00973921 0.00917557\n 0.00862737 0.00806667 0.00700501 0.00655575]\n[43.65947765 32.49926222 29.97933139 26.80312728 25.83290382 25.07424996\n 24.3136751  23.51032071 21.90865485 21.19447635]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n/Users/isfarbaset/anaconda3/lib/python3.11/site-packages/seaborn/axisgrid.py:118: UserWarning: The figure layout has changed to tight\n  self._figure.tight_layout(*args, **kwargs)\n\n\n\n\n\n\n\n\n\nThe first image is a pair plot showing relationships between multiple variables, with histograms on the diagonal indicating distributions. The second image is a 2D scatter plot with points colored based on ‘Make’. The third is a 3D scatter plot displaying data distribution in three dimensions, with points colored by categories.",
    "crumbs": [
      "EV Insights",
      "Naïve Bayes"
    ]
  },
  {
    "objectID": "5000-website/dim-reduction/dim-reduction.html",
    "href": "5000-website/dim-reduction/dim-reduction.html",
    "title": "Dimensionality Reduction",
    "section": "",
    "text": "The project’s objective is to explore and demonstrate the effectiveness of PCA and t-SNE in reducing the dimensionality of complex, multimodal data while preserving essential information and enhancing data visualization. The dataset that will be used here is a record dataset containing detailed information regarding different motor vehicles. The tools and libraries are exlusively Python and it’s scikit-learn library.\nBelow are the python libraries used:\nShow the code\nimport json\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import silhouette_samples, silhouette_score\nfrom sklearn.decomposition import PCA\nfrom numpy import linalg as LA\nfrom sklearn.manifold import TSNE\nfrom sklearn.model_selection import train_test_split\nHere’s a snippet of the dataset in question:\nShow the code\nrecord_data = pd.read_csv('../eda/cars-data.csv')\n\nrecord_data.head()\n\n\n\n\n\n\n\n\n\n\nUnnamed: 0\ncity_mpg\nclass\ncombination_mpg\ncylinders\ndisplacement\ndrive\nfuel_type\nhighway_mpg\nmake\nmodel\ntransmission\nyear\n\n\n\n\n0\n0\n18\nmidsize car\n21\n4.0\n2.2\nfwd\ngas\n26\ntoyota\nCamry\na\n1993\n\n\n1\n1\n19\nmidsize car\n22\n4.0\n2.2\nfwd\ngas\n27\ntoyota\nCamry\nm\n1993\n\n\n2\n2\n16\nmidsize car\n19\n6.0\n3.0\nfwd\ngas\n22\ntoyota\nCamry\na\n1993\n\n\n3\n3\n16\nmidsize car\n18\n6.0\n3.0\nfwd\ngas\n22\ntoyota\nCamry\nm\n1993\n\n\n4\n4\n18\nmidsize-large station wagon\n21\n4.0\n2.2\nfwd\ngas\n26\ntoyota\nCamry\na\n1993",
    "crumbs": [
      "EV Insights",
      "Dimensionality Reduction"
    ]
  },
  {
    "objectID": "5000-website/dim-reduction/dim-reduction.html#some-preliminary-cleaning",
    "href": "5000-website/dim-reduction/dim-reduction.html#some-preliminary-cleaning",
    "title": "Dimensionality Reduction",
    "section": "Some preliminary cleaning",
    "text": "Some preliminary cleaning\n\n\nShow the code\ndf = record_data.copy()\n\n\n\n\nShow the code\nnan_count = record_data.isna().sum()\n\nprint(nan_count)\n\n\nUnnamed: 0          0\ncity_mpg            0\nclass               0\ncombination_mpg     0\ncylinders          25\ndisplacement       25\ndrive               3\nfuel_type           0\nhighway_mpg         0\nmake                0\nmodel               0\ntransmission        0\nyear                0\ndtype: int64\n\n\n\n\nShow the code\nrecord_data.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 80 entries, 0 to 79\nData columns (total 13 columns):\n #   Column           Non-Null Count  Dtype  \n---  ------           --------------  -----  \n 0   Unnamed: 0       80 non-null     int64  \n 1   city_mpg         80 non-null     int64  \n 2   class            80 non-null     object \n 3   combination_mpg  80 non-null     int64  \n 4   cylinders        55 non-null     float64\n 5   displacement     55 non-null     float64\n 6   drive            77 non-null     object \n 7   fuel_type        80 non-null     object \n 8   highway_mpg      80 non-null     int64  \n 9   make             80 non-null     object \n 10  model            80 non-null     object \n 11  transmission     80 non-null     object \n 12  year             80 non-null     int64  \ndtypes: float64(2), int64(5), object(6)\nmemory usage: 8.3+ KB\n\n\n\n\nShow the code\n# Convert all 'object' type columns to 'string'\nfor col in record_data.select_dtypes(include=['object']).columns:\n    record_data[col] = record_data[col].astype('string')\n\n# Verify the changes\nrecord_data.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 80 entries, 0 to 79\nData columns (total 13 columns):\n #   Column           Non-Null Count  Dtype  \n---  ------           --------------  -----  \n 0   Unnamed: 0       80 non-null     int64  \n 1   city_mpg         80 non-null     int64  \n 2   class            80 non-null     string \n 3   combination_mpg  80 non-null     int64  \n 4   cylinders        55 non-null     float64\n 5   displacement     55 non-null     float64\n 6   drive            77 non-null     string \n 7   fuel_type        80 non-null     string \n 8   highway_mpg      80 non-null     int64  \n 9   make             80 non-null     string \n 10  model            80 non-null     string \n 11  transmission     80 non-null     string \n 12  year             80 non-null     int64  \ndtypes: float64(2), int64(5), string(6)\nmemory usage: 8.3 KB\n\n\n\n\nShow the code\n# get y \n\ny = record_data['fuel_type'].unique().tolist()\n\ny\n\n\n['gas', 'electricity']\n\n\n\n\nShow the code\n\n# Dropping non-numerical and unnecessary columns\nrecord_data = record_data.drop(columns=['Unnamed: 0'])\n\n\n\n\nShow the code\n# Replace continuous missing values with mean of the column. check for Nan values again.\n\ncols = ['displacement', 'cylinders']\nrecord_data[cols] = record_data[cols].fillna(record_data[cols].mean())\n\nnan_count = record_data.isna().sum()\nprint(nan_count)\n\n\ncity_mpg           0\nclass              0\ncombination_mpg    0\ncylinders          0\ndisplacement       0\ndrive              3\nfuel_type          0\nhighway_mpg        0\nmake               0\nmodel              0\ntransmission       0\nyear               0\ndtype: int64\n\n\n\n\nShow the code\n# Replace categorical missing values with mode of the column. check for Nan values again.\n\nrecord_data['drive'] = record_data['drive'].fillna(record_data['drive'].mode().iloc[0])\n\nnan_count = record_data.isna().sum()\nprint(nan_count)\n\n\ncity_mpg           0\nclass              0\ncombination_mpg    0\ncylinders          0\ndisplacement       0\ndrive              0\nfuel_type          0\nhighway_mpg        0\nmake               0\nmodel              0\ntransmission       0\nyear               0\ndtype: int64\n\n\n\n\nShow the code\n# Using a for loop to replace categorical values with cat codes\ncat_cols = ['class', 'drive', 'fuel_type', 'make', 'model', 'transmission']\nfor col in cat_cols:\n    record_data[col] = record_data[col].astype('category')\n    record_data[col] = record_data[col].cat.codes\n\n# Display the altered DataFrame\nrecord_data.head()\n\n\n\n\n\n\n\n\n\n\ncity_mpg\nclass\ncombination_mpg\ncylinders\ndisplacement\ndrive\nfuel_type\nhighway_mpg\nmake\nmodel\ntransmission\nyear\n\n\n\n\n0\n18\n2\n21\n4.0\n2.2\n1\n1\n26\n6\n3\n0\n1993\n\n\n1\n19\n2\n22\n4.0\n2.2\n1\n1\n27\n6\n3\n1\n1993\n\n\n2\n16\n2\n19\n6.0\n3.0\n1\n1\n22\n6\n3\n0\n1993\n\n\n3\n16\n2\n18\n6.0\n3.0\n1\n1\n22\n6\n3\n1\n1993\n\n\n4\n18\n3\n21\n4.0\n2.2\n1\n1\n26\n6\n3\n0\n1993\n\n\n\n\n\n\n\n\n\n\nShow the code\nrecord_data.to_csv('cardf_cleaned.csv')\n\n\n\n\nShow the code\n# Split the dataset in X and y. since this is unsupervised learning, we will not use the y labels. you can choose to normalize the X data by using the StandardScaler function.\n\nx_cols = ['city_mpg', 'class', 'combination_mpg', 'cylinders', 'displacement', 'drive', 'highway_mpg', 'make', 'model', 'transmission', 'year' ]\ny_cols = ['fuel_type']\n\nX = record_data[x_cols]  #'fuel_type' is the column we want to predict.\ny = record_data[y_cols]\n\n# Standardizing the data\nX = StandardScaler().fit_transform(X)",
    "crumbs": [
      "EV Insights",
      "Dimensionality Reduction"
    ]
  },
  {
    "objectID": "5000-website/dim-reduction/dim-reduction.html#manual-pca-compared-to-pca-using-scikit-learn",
    "href": "5000-website/dim-reduction/dim-reduction.html#manual-pca-compared-to-pca-using-scikit-learn",
    "title": "Dimensionality Reduction",
    "section": "Manual PCA compared to PCA using scikit-learn",
    "text": "Manual PCA compared to PCA using scikit-learn\n\n\nShow the code\n# EIGEN VALUES/VECTOR\nfrom numpy import linalg as LA\n# w, v1 = LA.eig(cov)\nw, v1 = LA.eig(np.cov(X.T))\nprint(\"\\nCOV EIGENVALUES:\",w)\nprint(\"COV EIGENVECTORS (across rows):\")\nprint(v1.T)\n\n# X = np.random.multivariate_normal(u, cov, NPOINTS)\n\n# PCA CALCULATION\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=5)\npca.fit(X)\nprint('\\nPCA')\nprint(pca.components_)\nv2=pca.components_\n\n# print(v1/v2)\n\n# PLOT\nfig = plt.figure()\nax = fig.add_subplot(projection='3d')\nax.scatter(X[:,0],X[:,1],X[:,2],marker=\".\", cmap=\"viridis\")\nv1=v1*1000\n# v2=v2*1000\n\nax.quiver(0,0,0,v1[0,0],v1[1,0],v1[2,0])\nax.quiver(0,0,0,v1[0,1],v1[1,1],v1[2,1])\nax.quiver(0,0,0,v1[0,2],v1[1,2],v1[2,2])\n\n# ax.quiver(0,0,0,v2[0,0],v2[1,0],v2[2,0])\n# ax.quiver(0,0,0,v2[0,1],v2[1,1],v2[2,1])\n# ax.quiver(0,0,0,v2[0,2],v2[1,2],v2[2,2])\nplt.show()\n\n\n\nCOV EIGENVALUES: [4.31172312e+00 2.36325475e+00 1.32691097e+00 7.68388326e-05\n 7.92815371e-03 4.89509105e-02 1.13477930e-01 4.60499041e-01\n 9.07988737e-01 7.93025808e-01 8.05404242e-01]\nCOV EIGENVECTORS (across rows):\n[[ 4.61010541e-01 -2.23979219e-01  4.64227185e-01 -9.17157811e-02\n  -8.48885324e-02 -4.15471132e-02  4.65607349e-01  2.23387117e-01\n   8.34611344e-02 -2.22356490e-01  4.25853876e-01]\n [ 7.96617828e-02  2.57048400e-01  7.40365343e-02  5.85812440e-01\n   5.89632136e-01  4.39078415e-02  6.50260082e-02 -3.29840413e-01\n   2.35575675e-01 -1.57473730e-01  1.89767195e-01]\n [ 1.54748253e-01 -7.25987869e-02  1.44267231e-01  1.16162593e-01\n   1.37904859e-01  7.17888611e-01  1.36118014e-01  8.15420030e-03\n  -5.64784496e-01  7.99644799e-02 -2.41087856e-01]\n [-4.60665906e-01 -1.83195529e-03  8.14075148e-01 -9.31577614e-04\n   2.33721258e-03 -8.33861783e-04 -3.53608981e-01 -3.28651760e-04\n  -2.91537653e-03 -1.40098149e-03 -2.88319025e-03]\n [ 6.50527993e-01  2.16133784e-02  4.24157197e-02  3.46465466e-02\n  -5.54672887e-02  2.15326568e-02 -7.51053984e-01  3.47335220e-02\n  -5.84715020e-04  8.14862014e-03  6.67172663e-02]\n [ 2.05442370e-01 -3.05435368e-02  1.41223375e-01 -4.90286210e-01\n   5.83054989e-01 -1.86039523e-01  6.66364145e-02  3.04649464e-02\n   1.42365254e-01 -4.88346725e-02 -5.42172367e-01]\n [-1.63571819e-01 -2.35195234e-02 -1.57224120e-01 -5.05780778e-01\n   3.95852928e-01  1.17365540e-01 -1.48542443e-01 -1.39199228e-01\n  -2.41013449e-01  8.43564813e-02  6.44234803e-01]\n [ 6.54404703e-02 -5.23757864e-01  3.56875681e-02  1.03238708e-01\n  -4.97544150e-02 -3.97658842e-01  4.80420136e-03 -6.13948827e-01\n  -3.93314405e-01 -1.08111352e-01 -7.42618771e-02]\n [-1.21605972e-02 -4.86648660e-01 -1.59265506e-02 -9.65260700e-02\n  -7.70127257e-02  4.48757358e-01 -2.52657942e-02 -3.24438691e-01\n   6.16244250e-01  2.45617697e-01 -1.25013919e-02]\n [-5.44589683e-02 -4.53359213e-01 -5.02214694e-02  3.33315607e-01\n   3.18522346e-01 -2.20709955e-01 -4.31672602e-02  4.88903689e-01\n  -6.10240073e-02  5.27795089e-01  6.72827641e-02]\n [-2.13115204e-01 -3.99810557e-01 -2.12057350e-01  8.02958928e-02\n   1.28500743e-01  1.54607730e-01 -2.05673965e-01  3.13855856e-01\n   4.48943013e-02 -7.47805002e-01 -2.94378889e-02]]\n\nPCA\n[[ 0.46101054 -0.22397922  0.46422719 -0.09171578 -0.08488853 -0.04154711\n   0.46560735  0.22338712  0.08346113 -0.22235649  0.42585388]\n [ 0.07966178  0.2570484   0.07403653  0.58581244  0.58963214  0.04390784\n   0.06502601 -0.32984041  0.23557567 -0.15747373  0.18976719]\n [ 0.15474825 -0.07259879  0.14426723  0.11616259  0.13790486  0.71788861\n   0.13611801  0.0081542  -0.5647845   0.07996448 -0.24108786]\n [-0.0121606  -0.48664866 -0.01592655 -0.09652607 -0.07701273  0.44875736\n  -0.02526579 -0.32443869  0.61624425  0.2456177  -0.01250139]\n [ 0.2131152   0.39981056  0.21205735 -0.08029589 -0.12850074 -0.15460773\n   0.20567396 -0.31385586 -0.0448943   0.747805    0.02943789]]\n\n\n/var/folders/w3/pq9m5d810bz24_jz2zl3s6pr0000gn/T/ipykernel_18364/4289367140.py:24: UserWarning: No data for colormapping provided via 'c'. Parameters 'cmap' will be ignored\n  ax.scatter(X[:,0],X[:,1],X[:,2],marker=\".\", cmap=\"viridis\")\n\n\n\n\n\n\n\n\n\n\n\nShow the code\n# Only taking the top 3 eigenvalues and their corresponding eigenvectors for comparison\ntop_eigenvalues = w[:5]\ntop_eigenvectors = v1[:, :5]\n\n# Creating a DataFrame to show the top 3 principal components and their explained variance\npca_results = pd.DataFrame({\n    'Eigenvalue': top_eigenvalues,\n    'Explained Variance Ratio': pca.explained_variance_ratio_,\n    'Cumulative Explained Variance': np.cumsum(pca.explained_variance_ratio_)\n})\n\npca_results\n\n\n\n\n\n\n\n\n\n\nEigenvalue\nExplained Variance Ratio\nCumulative Explained Variance\n\n\n\n\n0\n4.311723\n0.387075\n0.387075\n\n\n1\n2.363255\n0.212156\n0.599231\n\n\n2\n1.326911\n0.119120\n0.718351\n\n\n3\n0.000077\n0.081513\n0.799864\n\n\n4\n0.007928\n0.072303\n0.872167\n\n\n\n\n\n\n\n\nThe matching results from the manual calculation and the scikit-learn PCA show that the method works correctly. The scatter plot with the main lines drawn on it helps us see and understand how the data is arranged and how PCA changes it to a different form.",
    "crumbs": [
      "EV Insights",
      "Dimensionality Reduction"
    ]
  },
  {
    "objectID": "5000-website/dim-reduction/dim-reduction.html#apply-pca-with-all-components-to-examine-the-explained-variance-ratio",
    "href": "5000-website/dim-reduction/dim-reduction.html#apply-pca-with-all-components-to-examine-the-explained-variance-ratio",
    "title": "Dimensionality Reduction",
    "section": "Apply PCA with all components to examine the explained variance ratio",
    "text": "Apply PCA with all components to examine the explained variance ratio\n\n\nShow the code\n# Apply PCA with all components to examine the explained variance ratio\npca = PCA()\npca.fit_transform(X)\n\n# Explained variance ratio\nexplained_variance = pca.explained_variance_ratio_\n\n# Plotting the cumulative explained variance to determine the optimal number of components\nplt.figure(figsize=(10, 6))\nplt.plot(np.cumsum(explained_variance))\nplt.xlabel('Number of Components')\nplt.ylabel('Cumulative Explained Variance')\nplt.title('Explained Variance by Different Principal Components')\nplt.grid(True)\nplt.show()\n\n# Returning the cumulative explained variance for interpretation\nexplained_variance.cumsum()\n\n\n\n\n\n\n\n\n\narray([0.38707514, 0.59923097, 0.71835139, 0.79986401, 0.87216735,\n       0.94335944, 0.98469969, 0.99488691, 0.99928137, 0.9999931 ,\n       1.        ])\n\n\nThe curve suggests that around 90% of the variance is explained by the first 4 principal components, as the curve starts to level off beyond this point.",
    "crumbs": [
      "EV Insights",
      "Dimensionality Reduction"
    ]
  },
  {
    "objectID": "5000-website/dim-reduction/dim-reduction.html#pca---reduced-dimensional-data-visualization",
    "href": "5000-website/dim-reduction/dim-reduction.html#pca---reduced-dimensional-data-visualization",
    "title": "Dimensionality Reduction",
    "section": "PCA - Reduced Dimensional Data Visualization",
    "text": "PCA - Reduced Dimensional Data Visualization\n\n\nShow the code\n# Applying PCA with the optimal number of components (2 components for visualization)\npca_optimal = PCA(n_components=2)\nreduced_data = pca_optimal.fit_transform(X)\n\n# Visualizing the reduced-dimensional data\nplt.figure(figsize=(10, 6))\nplt.scatter(reduced_data[:, 0], reduced_data[:, 1])\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nplt.title('PCA - Reduced Dimensional Data Visualization')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nfuel_type_labels = y['fuel_type'].map({0: 'electricity', 1: 'gas'}) \n\n# Add the fuel type labels to the DataFrame\npca_df['fuel_type'] = fuel_type_labels\n\n# use seaborn to plot the scatter plot with hues\nplt.figure(figsize=(10, 6))\nsns.scatterplot(data=pca_df, x='PC1', y='PC2', hue='fuel_type', palette='tab10')\nplt.title('PCA - First two principal components with Fuel Type color coding')\nplt.xlabel('Principal Component 1 (PC1)')\nplt.ylabel('Principal Component 2 (PC2)')\nplt.legend(title='Fuel Type')\nplt.grid(True)\nplt.show()",
    "crumbs": [
      "EV Insights",
      "Dimensionality Reduction"
    ]
  },
  {
    "objectID": "5000-website/dim-reduction/dim-reduction.html#analysis-and-interpretation-of-pca-results",
    "href": "5000-website/dim-reduction/dim-reduction.html#analysis-and-interpretation-of-pca-results",
    "title": "Dimensionality Reduction",
    "section": "Analysis and Interpretation of PCA Results",
    "text": "Analysis and Interpretation of PCA Results\nAnalyzing the first graph, which shows the cumulative explained variance, the curve starts to plateau after the fourth component. By the fifth component, the cumulative explained variance is close to 90%. Therefore, retaining four or five components would be optimal for balancing data dimensionality reduction and information retention.\nThe first two principal components capture the majority of the variance in the data, as seen in the scatter plot without color coding. This indicates that most of the information can be compressed into two dimensions. When the fuel type is used as a hue in the scatter plot, there is a visible separation between electric and gas vehicles along the principal components, suggesting that PCA has managed to reduce the dimensions while still retaining the characteristics that can distinguish between the two fuel types.\nIn conclusion, for this dataset, retaining four or five components principal components is optimal for analysis, allowing most of the variance to be explained while significantly reducing the number of dimensions (almost by half). The visualizations indicate that these components are meaningful as they help in distinguishing between electric and gas vehicles.",
    "crumbs": [
      "EV Insights",
      "Dimensionality Reduction"
    ]
  },
  {
    "objectID": "5000-website/dim-reduction/dim-reduction.html#perplexity-value-30",
    "href": "5000-website/dim-reduction/dim-reduction.html#perplexity-value-30",
    "title": "Dimensionality Reduction",
    "section": "Perplexity Value = 30",
    "text": "Perplexity Value = 30\n\n\nShow the code\n# LOAD DATA\n\nX = record_data.drop('fuel_type', axis=1)\nprint(X.shape)\n\n# DO DIMENSIONALITY REDUCTION\nX_embedded = TSNE(n_components=2, learning_rate='auto',init='random', perplexity=30).fit_transform(X)\n\n# EXPLORE RESULTS\nprint(\"RESULTS\") \nprint(\"shape : \",X_embedded.shape)\nprint(\"First few points : \\n\",X_embedded[0:2,:])\n\n# PLOT \nplt.scatter(X_embedded[:,0],X_embedded[:,1], alpha=0.5)\n\n\n(80, 11)\nRESULTS\nshape :  (80, 2)\nFirst few points : \n [[ 1.2051195 -3.1006622]\n [ 1.4165833 -2.87457  ]]",
    "crumbs": [
      "EV Insights",
      "Dimensionality Reduction"
    ]
  },
  {
    "objectID": "5000-website/dim-reduction/dim-reduction.html#perplexity-value-16",
    "href": "5000-website/dim-reduction/dim-reduction.html#perplexity-value-16",
    "title": "Dimensionality Reduction",
    "section": "Perplexity Value = 16",
    "text": "Perplexity Value = 16\n\n\nShow the code\n### Perplexity Value = 16\n\n# LOAD DATA\nX = record_data.drop('fuel_type', axis=1)\nprint(X.shape)\n\n# DO DIMENSIONALITY REDUCTION\nX_embedded = TSNE(n_components=2, learning_rate='auto',init='random', perplexity=16).fit_transform(X)\n\n# EXPLORE RESULTS\nprint(\"RESULTS\") \nprint(\"shape : \",X_embedded.shape)\nprint(\"First few points : \\n\",X_embedded[0:2,:])\n\n# PLOT \nplt.scatter(X_embedded[:,0],X_embedded[:,1], alpha=0.5)\n\n\n(80, 11)\nRESULTS\nshape :  (80, 2)\nFirst few points : \n [[-5.9813023  4.654429 ]\n [-5.8530436  5.1955423]]",
    "crumbs": [
      "EV Insights",
      "Dimensionality Reduction"
    ]
  },
  {
    "objectID": "5000-website/dim-reduction/dim-reduction.html#perplexity-value-1",
    "href": "5000-website/dim-reduction/dim-reduction.html#perplexity-value-1",
    "title": "Dimensionality Reduction",
    "section": "Perplexity Value = 1",
    "text": "Perplexity Value = 1\n\n\nShow the code\n### Perplexity Value = 1\n\n# LOAD DATA\n\nX = record_data.drop('fuel_type', axis=1)\nprint(X.shape)\n\n# DO DIMENSIONALITY REDUCTION\nX_embedded = TSNE(n_components=2, learning_rate='auto',init='random', perplexity=1).fit_transform(X)\n\n# EXPLORE RESULTS\nprint(\"RESULTS\") \nprint(\"shape : \",X_embedded.shape)\nprint(\"First few points : \\n\",X_embedded[0:2,:])\n\n# PLOT \nplt.scatter(X_embedded[:,0],X_embedded[:,1], alpha=0.5)\n\n\n(80, 11)\nRESULTS\nshape :  (80, 2)\nFirst few points : \n [[-48.516888 -15.93984 ]\n [-52.88673  -22.104866]]\n\n\n\n\n\n\n\n\n\nAnalysis and Interpretation of t-SNE Results\nWith a perplexity value of 30, the t-SNE algorithm has successfully reduced the dimensionality of the data and the resulting plot shows the data in two dimensions. Based on the plots, we can infer that the dataset has some intrinsic clusters or groupings, which could be further analyzed for patterns or similarities. A lower perplexity of 16 means the model pays more attention to local structure, which can result in tighter clusters that may reveal more local groupings or subclusters within the data. Here, the clusters appear more compact and less dispersed, reflecting a focus on local similarities. For the perplexity level of 1, the t-SNE algorithm is primarily focusing on the local relationships between very close neighbors, which can lead to a more fragmented distribution of points with many small clusters that may not accurately reflect the overall structure of the data. The resulting visualization is much more scattered and less cohesive than those with higher perplexity values, making it difficult to derive any insight from the data.\nOverall, t-SNE efficiently reduced the dimensions from 11 to 2. We can see clear groupings have formed with the perplexity values of 16 and 30.",
    "crumbs": [
      "EV Insights",
      "Dimensionality Reduction"
    ]
  },
  {
    "objectID": "5000-website/dim-reduction/dim-reduction.html#evaluation-and-comparison",
    "href": "5000-website/dim-reduction/dim-reduction.html#evaluation-and-comparison",
    "title": "Dimensionality Reduction",
    "section": "Evaluation and Comparison",
    "text": "Evaluation and Comparison\nEvaluation of the effectiveness of PCA and t-SNE in terms of preserving data structure and information, comparison of the visualization capabilities of PCA and t-SNE. and the trade-offs and scenarios where one technique may outperform the other:\n\n\n\n\n\n\n\n\nAspect\nPCA\nt-SNE\n\n\n\n\nPreservation of Structure and Information\nPreserves global structure and variance.\nPreserves local structure and relationships.\n\n\nVisualization\nClear, interpretable axes representing transformed features.\nComplex, with no interpretable axes and coordinates.\n\n\nComputation\nFast and deterministic.\nSlow and stochastic.\n\n\nUse Case\nSuitable for large datasets and initial analysis.\nSuitable for detailed exploration on smaller datasets.\n\n\nEffectiveness\nBest for datasets with linear relationships.\nBest for datasets with non-linear relationships.\n\n\nInterpretability\nComponents directly relate to original features.\nComponents have no direct relationship to features.\n\n\nScenarios\nUsed for dimensionality reduction and feature extraction.\nUsed for clustering and data visualization.\n\n\nTrade-offs\nPoor at capturing non-linear relationships, therefore the approach might oversimplify.\nMay not preserve global structure and is sensitive to hyperparameters.\n\n\n\nReferences Used:  - https://towardsdatascience.com/principal-component-analysis-for-dimensionality-reduction-115a3d157bad  - https://www.datacamp.com/tutorial/introduction-t-sne  - https://towardsdatascience.com/how-t-sne-outperforms-pca-in-dimensionality-reduction-7a3975e8cbdb  - https://www.geeksforgeeks.org/difference-between-pca-vs-t-sne/",
    "crumbs": [
      "EV Insights",
      "Dimensionality Reduction"
    ]
  },
  {
    "objectID": "index.html#airy-tales",
    "href": "index.html#airy-tales",
    "title": "Projects",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "story-project/evap.html",
    "href": "story-project/evap.html",
    "title": "Shifting Evaporation Rates",
    "section": "",
    "text": "Let’s delve into the evaporation trends over the years. Previously, we observed low soil moisture levels and an increase in soil dryness which raises several questions about the effects of evaporation. Is high evaporation the potential cause of soil dryness, or are the two factors unrelated?\n\n\nCode\nlibrary(reticulate)\n\nuse_condaenv(\"base\", required = TRUE)\n\n\n\n\nCode\nimport pandas as pd\nimport plotly.graph_objects as go\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\n\n# Load the cleaned datasets\nhistoric_data = pd.read_csv('./data/clean-data/historic_data_cleaned.csv')\nnearterm_data = pd.read_csv('./data/clean-data/nearterm_data_cleaned.csv')\n\n# Combine datasets for some visualizations\ncombined_data = pd.concat([historic_data, nearterm_data])\n\n# Extracting and merging the relevant columns for Evap_Summer analysis\ncombined_evap_data = combined_data[['year', 'Evap_Summer']]\n\n# Aggregating the data by year using the mean\ncombined_evap_aggregated = combined_evap_data.groupby('year').mean().reset_index()\n\n# Calculate trendline for Evap_Summer\ndef calculate_trendline(x, y):\n    model = LinearRegression()\n    x_reshaped = np.array(x).reshape(-1, 1)\n    model.fit(x_reshaped, y)\n    trend = model.predict(x_reshaped)\n    return trend\n\nevap_trend = calculate_trendline(combined_evap_aggregated['year'], combined_evap_aggregated['Evap_Summer'])\n\n# Create the Evaporation in Summer trend plot in one step\nfig1 = go.Figure(\n    data=[\n        go.Scatter(\n            x=combined_evap_aggregated['year'],\n            y=combined_evap_aggregated['Evap_Summer'],\n            mode='lines+markers',\n            line=dict(color='orange'),\n            hovertemplate='Year: %{x}&lt;br&gt;Evaporation: %{y}&lt;extra&gt;&lt;/extra&gt;',\n            name='Evaporation in Summer'\n        ),\n        go.Scatter(\n            x=combined_evap_aggregated['year'],\n            y=evap_trend,\n            mode='lines',\n            line=dict(color='blue', dash='dash'),\n            name='Trendline'\n        )\n    ],\n    layout=dict(\n        title='Evaporation in Summer Trend',\n        xaxis_title='Year',\n        yaxis_title='Evaporation in Summer',\n        title_font=dict(size=18, family='Arial, sans-serif'),\n        xaxis=dict(tickfont=dict(size=14)),\n        yaxis=dict(tickfont=dict(size=14)),\n        width=800,\n        height=600,\n        plot_bgcolor='#f7f7f7',\n        hoverlabel=dict(font_size=16),\n        showlegend=True,\n        legend=dict(\n            orientation='h',\n            yanchor='top',\n            y=-0.2,\n            xanchor='center',\n            x=0.5\n        ),\n        title_x=0.5\n    )\n)\n\n# Display the first plot\nfig1.show()\n\n\n                        \n                                            \nThe line chart shows the trend in evaporation rates during summer. The orange line represents evaporation levels and the trend line indicates a decreasing trend, suggesting lower rate in evaporation patterns over the years 📉\n\n\nThis potentially indicates that other factors such as reduced precipitation or higher temperatures are contributing to the drier soil conditions.\n\nNext up:\nHow is precipitation affected by all of this? What is the potential impact of worsening dry stress conditions and lower rainfall?",
    "crumbs": [
      "Temp Talk",
      "Evaporation Station 🌤️"
    ]
  },
  {
    "objectID": "story-project/temp.html",
    "href": "story-project/temp.html",
    "title": "The Summer Heat Story",
    "section": "",
    "text": "In this section, we will take a closer look at summer temperature levels. Previously, we observed a rising trend in maximum temperatures over the past few years. This section continues that discussion by focusing on summer heat levels from the past to the present!\n\n\nCode\nlibrary(reticulate)\n\nuse_condaenv(\"base\", required = TRUE)\n\n\n\n\nCode\nimport pandas as pd\nimport plotly.graph_objects as go\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\n\n# Load the cleaned datasets\nhistoric_data = pd.read_csv('./data/clean-data/historic_data_cleaned.csv')\nnearterm_data = pd.read_csv('./data/clean-data/nearterm_data_cleaned.csv')\n\n# Combine datasets for some visualizations\ncombined_data = pd.concat([historic_data, nearterm_data])\n\n# Extracting and merging the relevant columns for T_Summer analysis\ncombined_t_summer_data = combined_data[['year', 'T_Summer']]\n\n# Aggregating the data by year using the mean\ncombined_t_summer_aggregated = combined_t_summer_data.groupby('year').mean().reset_index()\n\n# Calculate trendline for combined data\ndef calculate_trendline(x, y):\n    model = LinearRegression()\n    x_reshaped = np.array(x).reshape(-1, 1)\n    model.fit(x_reshaped, y)\n    trend = model.predict(x_reshaped)\n    return trend\n\nt_summer_trend = calculate_trendline(combined_t_summer_aggregated['year'], combined_t_summer_aggregated['T_Summer'])\n\n# Create a custom colorscale \ncustom_colorscale = [\n    [0.0, 'rgb(255,192,12)'],  # yellow ochre\n    [1.0,  'rgb(150,0,0)']  # deep red\n]\n\n# Create bubble chart with trendline and update layout\nfig = go.Figure(\n    data=[\n        go.Scatter(\n            x=combined_t_summer_aggregated['year'],\n            y=combined_t_summer_aggregated['T_Summer'],\n            mode='markers',\n            marker=dict(\n                size=combined_t_summer_aggregated['T_Summer'],\n                color=combined_t_summer_aggregated['T_Summer'],\n                colorscale=custom_colorscale,\n                sizemode='area',\n                sizeref=2.*max(combined_t_summer_aggregated['T_Summer'])/(40.**2),  # Adjusted sizeref to make bubbles smaller\n                sizemin=3  # Adjusted sizemin to make the smallest bubbles smaller\n            ),\n            hovertemplate='Year: %{x}&lt;br&gt;Temperature: %{y:.2f} °C&lt;extra&gt;&lt;/extra&gt;',\n            name='Summer Temperature'\n        ),\n        go.Scatter(\n            x=combined_t_summer_aggregated['year'],\n            y=t_summer_trend,\n            mode='lines',\n            line=dict(color='red', dash='dash'),\n            hovertemplate='Year: %{x}&lt;br&gt;Trend: %{y:.2f} °C&lt;extra&gt;&lt;/extra&gt;',\n            name='Trendline'\n        )\n    ],\n    layout=dict(\n        title='Summer Temperature Trend',\n        xaxis_title='Year',\n        yaxis_title='Summer Temperature (°C)',\n        title_font=dict(size=18, family='Arial, sans-serif'),\n        xaxis=dict(tickfont=dict(size=14)),\n        yaxis=dict(tickfont=dict(size=14)),\n        width=800,\n        height=600,\n        plot_bgcolor='#f7f7f7',\n        hoverlabel=dict(font_size=16),\n        showlegend=False,\n        legend=dict(\n            orientation='h',\n            yanchor='top',\n            y=-0.2,\n            xanchor='center',\n            x=0.5\n        ),\n        title_x=0.5\n    )\n)\n\n# Display the plot\nfig.show()\n\n\n                        \n                                            \nThis scatter plot and trend line depicts the changes in average summer temperatures over the years. The color gradient from yellow to red indicates temperature levels, with the darker red hues representing higher temperatures. The increasing trend line suggests a gradual rise in summer temperatures 📈\n\n\n\nNext up:\nSoutheastern Utah is home to a diverse ecosystem that relies on soil conditions. But are rising temperatures impacting this vital resource?",
    "crumbs": [
      "Temp Talk",
      "Blazing Trends 🔥"
    ]
  },
  {
    "objectID": "story-project/index.html",
    "href": "story-project/index.html",
    "title": "Introduction",
    "section": "",
    "text": "For the preservation of Southeastern Utah Group National Parks, it is essential to understand the trends that lie in the evolving climate conditions and how they might impact the region’s delicate ecosystems. This is an in-depth exploration and analysis of the interconnected effects of heat, soil conditions, evaporation and precipitation on the national parks.\n\n\nCode\nlibrary(reticulate)\n\nuse_condaenv(\"base\", required = TRUE)\n\n\n\n\nCode\nimport pandas as pd\nimport plotly.express as px\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\n\n# Load the cleaned datasets\nhistoric_data_1 = pd.read_csv('./data/clean-data/historic_data_cleaned.csv')\nnearterm_data_1 = pd.read_csv('./data/clean-data/nearterm_data_cleaned.csv')\n\n# Combine datasets for some visualizations\ncombined_data_1 = pd.concat([historic_data_1, nearterm_data_1])\n\n# Calculate the center of the map\ncenter_lat_1 = combined_data_1['lat'].mean()\ncenter_long_1 = combined_data_1['long'].mean()\n\n# Function to calculate an appropriate zoom level based on the spread of data points\ndef calculate_zoom_level_1(latitudes_1, longitudes_1):\n    max_lat_1, min_lat_1 = np.max(latitudes_1), np.min(latitudes_1)\n    max_long_1, min_long_1 = np.max(longitudes_1), np.min(longitudes_1)\n    lat_diff_1 = max_lat_1 - min_lat_1\n    long_diff_1 = max_long_1 - min_long_1\n    max_diff_1 = max(lat_diff_1, long_diff_1)\n    \n    if max_diff_1 &lt; 0.01:\n        return 15\n    elif max_diff_1 &lt; 0.1:\n        return 12\n    elif max_diff_1 &lt; 1:\n        return 10\n    elif max_diff_1 &lt; 10:\n        return 8\n    else:\n        return 6\n\n# Calculate zoom level (but we will manually adjust it for more zoom)\nzoom_level_1 = calculate_zoom_level_1(combined_data_1['lat'], combined_data_1['long'])\n\n# Manually adjust zoom level for a more zoomed-in view\nzoom_level_1 = zoom_level_1 * 0.95  # Increase the zoom factor\n\n# Set Mapbox access token\n# References:\n# https://towardsdatascience.com/simple-plotly-tutorials-868bd0890b8b\n# https://plotly.com/python/animations/\npx.set_mapbox_access_token(\"pk.eyJ1IjoiaXNmYXJiYXNldCIsImEiOiJjbHdiOWVtY2IwbGxsMmtraHZoYnB1YTMwIn0.10XSE1rNVsmXSnFmYYa0Cw\")\n\n# Create the Plotly scatter_mapbox map with enhanced aesthetics and detailed map layers\nmap_fig_1 = px.scatter_mapbox(combined_data_1, lon='long', lat='lat', color='Tmax_Summer',\n                            hover_data={'Tmax_Summer': ':.2f', 'lat': ':.2f', 'long': ':.2f', 'year': True}, size='Tmax_Summer',\n                            labels={'Tmax_Summer': 'max temp', 'lat': 'latitude', 'long': 'longitude'},\n                            animation_frame='year', title='Geospatial Distribution of Maximum Temperature Over Time',\n                            color_continuous_scale=px.colors.sequential.Sunsetdark, size_max=15, zoom=zoom_level_1,\n                            center={\"lat\": center_lat_1, \"lon\": center_long_1}\n                            )\n\n# Update the layout for better aesthetics and visibility of state lines\nmap_fig_1.update_layout(\n    mapbox_style=\"carto-positron\",  # Change the map style to show more details\n    mapbox=dict(\n        center=dict(lat=center_lat_1, lon=center_long_1),\n        zoom=zoom_level_1,  # Adjusted zoom level\n    ),\n    title_font=dict(size=18, family='Arial, sans-serif'),  \n    title_x=0.5,\n    coloraxis_colorbar=dict(\n        title=\"Maximum Temperature(°C)\",\n        title_side=\"right\",  # Align the title horizontally with the colorbar\n        title_font=dict(size=14)  # Adjust the font size as needed\n    ),\n    autosize=True,\n    width=750,  # Increase the width of the map\n    height=700   # Increase the height of the map\n)\n\n\n                        \n                                            \nThis geospatial visualization highlights the maximum temperatures reached within a year across the Southeastern National Parks in Utah. The timeline spans from 1980 to 2024. The animation over the years shows the geographical spread and varying intensity of the highest temperatures in the region.\n\n\n\nHeat on the rise?\n\n\nCode\nimport pandas as pd\nimport plotly.express as px\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\n\n# Load the cleaned datasets\nhistoric_data_1 = pd.read_csv('./data/clean-data/historic_data_cleaned.csv')\nnearterm_data_1 = pd.read_csv('./data/clean-data/nearterm_data_cleaned.csv')\n\n# Combine datasets for some visualizations\ncombined_data_1 = pd.concat([historic_data_1, nearterm_data_1])\n\n# Extracting the relevant column for analysis\nhistoric_tmax_data = historic_data_1[['year', 'Tmax_Summer']]\nnearterm_tmax_data = nearterm_data_1[['year', 'Tmax_Summer']]\n\n# Merging both datasets for a comprehensive analysis\ncombined_tmax_data = pd.concat([historic_tmax_data, nearterm_tmax_data])\n\n# Aggregating the data by year using the mean\ncombined_tmax_data_aggregated = combined_tmax_data.groupby('year').mean().reset_index()\n\n# Setting the style of the plot\nsns.set_theme(style=\"whitegrid\")\n\n# Plotting the Tmax_Summer over the years with enhancements and a trendline\nplt.figure(figsize=(16, 9))\nsns.lineplot(data=combined_tmax_data_aggregated, x='year', y='Tmax_Summer', marker='o', color='maroon', linewidth=2.5, label='Maximum Temperature')\nsns.regplot(data=combined_tmax_data_aggregated, x='year', y='Tmax_Summer', scatter=False, color='red', ci=None, line_kws={\"linewidth\":2.5, \"linestyle\":\"--\"}, label='Trendline')\n\n# Adding titles and labels with enhanced aesthetics\nplt.title('Maximum Temperature Trend', fontsize=32)\nplt.xlabel('Year', fontsize=25, weight='bold')\nplt.ylabel('Maximum Temperature (°C)', fontsize=25, weight='bold')\n\n# Setting the x-axis range from 1980 to 2024\n_ = plt.xlim(1980, 2024)\n\n# Adding a grid with customized appearance\n_ = plt.grid(True, linestyle='--', linewidth=0.7, alpha=0.7)\n\n# Enhancing the tick labels for better readability\n_ = plt.xticks(fontsize=20)\n_ = plt.yticks(fontsize=20)\n\n# Adding a light background color to the plot area\nplt.gca().set_facecolor('#f7f7f7')\n\n# Removing the top and right spines for a cleaner look\nsns.despine()\n\n# Adding legend\n_ = plt.legend(fontsize=20)\n\n# Display the plot\nplt.show()\n\n\n\n\n\nThe chart above illustrates the trend in maximum temperatures from 1980 up until the current year. The visualization suggests an overall increase, indicating a trend of rising temperatures in the region.\n\n\n\n\n\n\nNext up:\nThis sparks curiosity to take a closer look at past summer temperature trends and explore how they have changed over the years ➡️",
    "crumbs": [
      "Temp Talk",
      "Temperature Tales 🌡️"
    ]
  },
  {
    "objectID": "story-project/about.html",
    "href": "story-project/about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "story-project/ppt.html",
    "href": "story-project/ppt.html",
    "title": "Higher Rains, Higher Gains",
    "section": "",
    "text": "Since we discovered that evaporation rates have been decreasing over the past years, it would be interesting to see if rainfall has also decreased. In the water cycle, evaporation ultimately leads to rainfall, so it makes sense that less evaporation over the years could result in less rainfall.\n\n\nCode\nlibrary(reticulate)\n\nuse_condaenv(\"base\", required = TRUE)\n\n\n\n\nCode\nimport pandas as pd\nimport plotly.express as px\n\n# Load the cleaned datasets\nhistoric_data = pd.read_csv('./data/clean-data/historic_data_cleaned.csv')\nnearterm_data = pd.read_csv('./data/clean-data/nearterm_data_cleaned.csv')\n\n# Combine datasets for some visualizations\ncombined_data = pd.concat([historic_data, nearterm_data])\n\n# Group by year and calculate the average values for PPT_Summer and Evap_Summer\naverage_data = combined_data.groupby('year').agg({'PPT_Summer': 'mean', 'Evap_Summer': 'mean'}).reset_index()\n\n# Create the scatter plot to show the relationship between PPT_Summer and Evap_Summer and update layout for better aesthetics\nfig = px.scatter(average_data, x='PPT_Summer', y='Evap_Summer',\n                 trendline='ols',  # Add a trendline\n                 title='Relationship Between Summer Precipitation and Evaporation',\n                 labels={'PPT_Summer': 'Summer Precipitation (mm)', 'Evap_Summer': 'Summer Evaporation (mm)'})\n\nfig.update_layout(\n    title_font=dict(size=18, family='Arial, sans-serif'),\n    xaxis_title='Summer Precipitation (mm)',\n    # xaxis=dict(autorange='reversed'),  # Reverse x-axis\n    yaxis_title='Summer Evaporation (mm)',\n    width=800,\n    height=600,\n    title_x=0.5\n)\n\n\n                        \n                                            \nThe scatter plot shows a positive correlation between summer precipitation and summer evaporation, with a trendline indicating that higher precipitation is associated with higher evaporation.\n\n\n\nSummer Rain Stats\n\n\nCode\nimport pandas as pd\nimport plotly.express as px\nimport numpy as np\nimport plotly.express as px\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Load the cleaned datasets\nhistoric_data_1 = pd.read_csv('./data/clean-data/historic_data_cleaned.csv')\nnearterm_data_1 = pd.read_csv('./data/clean-data/nearterm_data_cleaned.csv')\n\n# Combine datasets for some visualizations\ncombined_data_1 = pd.concat([historic_data_1, nearterm_data_1])\n\n# Calculate the center of the map\ncenter_lat_1 = combined_data_1['lat'].mean()\ncenter_long_1 = combined_data_1['long'].mean()\n\n# Function to calculate an appropriate zoom level based on the spread of data points\ndef calculate_zoom_level_1(latitudes_1, longitudes_1):\n    max_lat_1, min_lat_1 = np.max(latitudes_1), np.min(latitudes_1)\n    max_long_1, min_long_1 = np.max(longitudes_1), np.min(longitudes_1)\n    lat_diff_1 = max_lat_1 - min_lat_1\n    long_diff_1 = max_long_1 - min_long_1\n    max_diff_1 = max(lat_diff_1, long_diff_1)\n    \n    if max_diff_1 &lt; 0.01:\n        return 15\n    elif max_diff_1 &lt; 0.1:\n        return 12\n    elif max_diff_1 &lt; 1:\n        return 10\n    elif max_diff_1 &lt; 10:\n        return 8\n    else:\n        return 6\n\n# Calculate zoom level (but we will manually adjust it for more zoom)\nzoom_level_1 = calculate_zoom_level_1(combined_data_1['lat'], combined_data_1['long'])\n\n# Manually adjust zoom level for a more zoomed-in view\nzoom_level_1 = zoom_level_1 * 0.95  # Increase the zoom factor\n\n# Set Mapbox access token\n# References:\n# https://towardsdatascience.com/simple-plotly-tutorials-868bd0890b8b\n# https://plotly.com/python/animations/\npx.set_mapbox_access_token(\"pk.eyJ1IjoiaXNmYXJiYXNldCIsImEiOiJjbHdiOWVtY2IwbGxsMmtraHZoYnB1YTMwIn0.10XSE1rNVsmXSnFmYYa0Cw\")\n\n# Create the Plotly scatter_mapbox map with enhanced aesthetics and detailed map layers\nmap_fig_1 = px.scatter_mapbox(combined_data_1, lon='long', lat='lat', color='PPT_Summer',\n                              hover_data={'PPT_Summer': ':.2f', 'lat': ':.2f', 'long': ':.2f', 'year': True}, size='PPT_Summer',\n                              labels={'PPT_Summer': 'summer precipitation', 'lat': 'latitude', 'long': 'longitude'},\n                              animation_frame='year', title='Geospatial Distribution of Summer Precipitation Over Time',\n                              color_continuous_scale=px.colors.sequential.Viridis_r, size_max=15, zoom=zoom_level_1,\n                              center={\"lat\": center_lat_1, \"lon\": center_long_1}\n                              )\n\n# Update the layout for better aesthetics and visibility of state lines\nmap_fig_1.update_layout(\n    mapbox_style=\"carto-positron\",  # Change the map style to show more details\n    mapbox=dict(\n        center=dict(lat=center_lat_1, lon=center_long_1),\n        zoom=zoom_level_1,  # Adjusted zoom level\n    ),\n    title_font=dict(size=18, family='Arial, sans-serif'),  \n    title_x=0.5,\n    coloraxis_colorbar=dict(\n        title=\"Summer Precipitation (inches)\",\n        title_side=\"right\",  # Align the title horizontally with the colorbar\n        title_font=dict(size=14)  # Adjust the font size as needed\n    ),\n    autosize=True,\n    width=750,  # Increase the width of the map\n    height=700   # Increase the height of the map\n)\n\n\n                        \n                                            \nThis animated map displays the geospatial distribution of summer precipitation in Southeastern Utah from 1980 to 2024. It visualizes changes in precipitation levels over time with the help of the color gradient, providing insights into evolving rainfall patterns in the region.\n\n\n\n\nPrecipitation Plot Twist\n\n\nCode\nimport pandas as pd\nimport plotly.express as px\nimport numpy as np\nimport plotly.express as px\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\n# Load the cleaned datasets\nhistoric_data_1 = pd.read_csv('./data/clean-data/historic_data_cleaned.csv')\nnearterm_data_1 = pd.read_csv('./data/clean-data/nearterm_data_cleaned.csv')\n\n# Combine datasets for some visualizations\ncombined_data_1 = pd.concat([historic_data_1, nearterm_data_1])\n\n# Extracting the relevant column for analysis\nhistoric_ppt_data = historic_data_1[['year', 'PPT_Summer']]\nnearterm_ppt_data = nearterm_data_1[['year', 'PPT_Summer']]\n\n# Merging both datasets for a comprehensive analysis\ncombined_ppt_data = pd.concat([historic_ppt_data, nearterm_ppt_data])\n\n# Aggregating the data by year using the mean\ncombined_ppt_data_aggregated = combined_ppt_data.groupby('year').mean().reset_index()\n\n# Setting the style of the plot\nsns.set_theme(style=\"whitegrid\")\n\n# Plotting the PPT_Summer over the years with enhancements and a trendline\nplt.figure(figsize=(16, 9))\nsns.lineplot(data=combined_ppt_data_aggregated, x='year', y='PPT_Summer', marker='o', color='navy', linewidth=2.5, label='Summer Precipitation')\nsns.regplot(data=combined_ppt_data_aggregated, x='year', y='PPT_Summer', scatter=False, color='#4682B4', ci=None, line_kws={\"linewidth\":2.5, \"linestyle\":\"--\"}, label='Trendline')\n\n# Adding titles and labels with enhanced aesthetics\nplt.title('Summer Precipitation Trend\\n in Southeastern Utah', fontsize=32)\nplt.xlabel('Year', fontsize=25, weight='bold')\nplt.ylabel('Summer Precipitation (inches)', fontsize=25, weight='bold')\n\n# Setting the x-axis range from 1980 to 2024\n_ = plt.xlim(1980, 2024)\n\n# Adding a grid with customized appearance\n_ = plt.grid(True, linestyle='--', linewidth=0.7, alpha=0.7)\n\n# Enhancing the tick labels for better readability\n_ = plt.xticks(fontsize=20)\n_ = plt.yticks(fontsize=20)\n\n# Adding a light background color to the plot area\nplt.gca().set_facecolor('#f7f7f7')\n\n# Removing the top and right spines for a cleaner look\nsns.despine()\n\n# Adding legend\n_ = plt.legend(fontsize=20)\n\n# Display the plot\nplt.show()\n\n\n\n\n\nThis chart helps us examine the trend in summer precipitation in Southeastern Utah from 1980 to 2024. The trend line indicates a slightly decreasing trend, suggesting a potential decline in summer rainfall over time.\n\n\n\n\n\n\nNext up:\nLet’s summarize all our findings!",
    "crumbs": [
      "Temp Talk",
      "Raindrops and Vapors 🌧️"
    ]
  },
  {
    "objectID": "story-project/soil.html",
    "href": "story-project/soil.html",
    "title": "Hotter Summers, Drier Soils?",
    "section": "",
    "text": "Summer temperatures are on the rise, so what does this mean for soil health? Does warmer weather deplete soil moisture levels? Let’s see what the trend suggests ⬇️\n\n\nCode\nlibrary(reticulate)\n\nuse_condaenv(\"base\", required = TRUE)\n\n\n\n\nCode\nimport pandas as pd\nimport plotly.express as px\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport plotly.graph_objects as go\nimport seaborn as sns\nfrom sklearn.linear_model import LinearRegression\n\n# Load the cleaned datasets\nhistoric_data = pd.read_csv('./data/clean-data/historic_data_cleaned.csv')\nnearterm_data = pd.read_csv('./data/clean-data/nearterm_data_cleaned.csv')\n\n# Combine datasets for some visualizations\ncombined_data = pd.concat([historic_data, nearterm_data])\n\n# Group by year and calculate the average values for Tmax_Summer and VWC_Summer_whole\naverage_data = combined_data.groupby('year').agg({'Tmax_Summer': 'mean', 'VWC_Summer_whole': 'mean'}).reset_index()\n\n# Create the scatter plot to show the relationship between Tmax_Summer and VWC_Summer_whole\nfig = px.scatter(average_data, x='Tmax_Summer', y='VWC_Summer_whole',\n                 trendline='ols',  # Add a trendline\n                 title='Relationship Between Summer Maximum Temperature and Soil Moisture',\n                 labels={'Tmax_Summer': 'Maximum Summer Temperature (°C)', 'VWC_Summer_whole': 'Average Soil Moisture (%)'})\n\n# Update layout for better aesthetics\nfig.update_layout(\n    title_font=dict(size=18, family='Arial, sans-serif'),\n    xaxis_title='Maximum Summer Temperature (°C)',\n    yaxis_title='Average Soil Moisture (%)',\n    width=800,\n    height=600,\n    title_x=0.5\n)\n\n\n                        \n                                            \nThe scatter plot explores the relationship between maximum summer temperatures and average soil moisture. The trend line shows a slight negative correlation, suggesting that higher temperatures are associated with lower soil moisture levels.\n\n\n\nMoisture Matters\n\n\nCode\nimport pandas as pd\nimport plotly.express as px\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport plotly.graph_objects as go\nimport seaborn as sns\nfrom sklearn.linear_model import LinearRegression\n\n# Load the cleaned datasets\nhistoric_data = pd.read_csv('./data/clean-data/historic_data_cleaned.csv')\nnearterm_data = pd.read_csv('./data/clean-data/nearterm_data_cleaned.csv')\n\n# Extracting the relevant column for VWC_Summer_whole analysis\nhistoric_soil_data_vwc = historic_data[['year', 'VWC_Summer_whole']]\nnearterm_soil_data_vwc = nearterm_data[['year', 'VWC_Summer_whole']]\ncombined_soil_data_vwc = pd.concat([historic_soil_data_vwc, nearterm_soil_data_vwc])\ncombined_soil_data_vwc_aggregated = combined_soil_data_vwc.groupby('year').mean().reset_index()\n\n\n# Function to calculate trendline\ndef calculate_trendline(x, y):\n    model = LinearRegression()\n    x_reshaped = np.array(x).reshape(-1, 1)\n    model.fit(x_reshaped, y)\n    trend = model.predict(x_reshaped)\n    return trend\n\n# Calculate trendline for VWC_Summer_whole\nvwc_trend = calculate_trendline(combined_soil_data_vwc_aggregated['year'], combined_soil_data_vwc_aggregated['VWC_Summer_whole'])\n\n# Determine the y-axis range to better show changes in moisture levels\ny_min = combined_soil_data_vwc_aggregated['VWC_Summer_whole'].min() - 0.005\ny_max = combined_soil_data_vwc_aggregated['VWC_Summer_whole'].max() + 0.005\n\n# Create the first area chart for VWC_Summer_whole with trendline\nfig1 = go.Figure(\n    data=[\n        go.Scatter(\n            x=combined_soil_data_vwc_aggregated['year'],\n            y=combined_soil_data_vwc_aggregated['VWC_Summer_whole'],\n            fill='tozeroy',\n            name='Soil Moisture in Summer',\n            mode='none',\n            fillcolor='#003c54'\n        ),\n        go.Scatter(\n            x=combined_soil_data_vwc_aggregated['year'],\n            y=vwc_trend,\n            mode='lines',\n            name='Trendline',\n            line=dict(color='red', dash='dash')\n        )\n    ],\n    layout=dict(\n        title='Soil Moisture in Summer Trend',\n        xaxis_title='Year',\n        yaxis_title='Soil Moisture (%)',\n        yaxis=dict(range=[y_min, y_max], tickfont=dict(size=14)),  # Set the y-axis range\n        title_font=dict(size=18, family='Arial, sans-serif'),\n        xaxis=dict(tickfont=dict(size=14)),\n        width=800,\n        height=600,\n        title_x=0.5,\n        plot_bgcolor='#f7f7f7',\n        hoverlabel=dict(font_size=16),\n        showlegend=True,\n        legend=dict(\n            orientation='h',\n            yanchor='top',\n            y=-0.2,\n            xanchor='center',\n            x=0.5\n        )\n    )\n)\n\nfig1.show()\n\n\n                        \n                                            \nThe chart here displays soil moisture during summer from the 1980s to the 2020s. The red dashed trend line indicates an overall stable trend with minor fluctuations over the years, though it remains quite low.\n\n\n\n\nTracking Summer Soil Dryness\n\n\nCode\nimport pandas as pd\nimport plotly.express as px\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport plotly.graph_objects as go\nimport seaborn as sns\nfrom sklearn.linear_model import LinearRegression\n\n# Load the cleaned datasets\nhistoric_data = pd.read_csv('./data/clean-data/historic_data_cleaned.csv')\nnearterm_data = pd.read_csv('./data/clean-data/nearterm_data_cleaned.csv')\n\n# Extracting the relevant column for DrySoilDays_Summer_whole analysis\nhistoric_soil_data_dry = historic_data[['year', 'DrySoilDays_Summer_whole']]\nnearterm_soil_data_dry = nearterm_data[['year', 'DrySoilDays_Summer_whole']]\ncombined_soil_data_dry = pd.concat([historic_soil_data_dry, nearterm_soil_data_dry])\ncombined_soil_data_dry_aggregated = combined_soil_data_dry.groupby('year').mean().reset_index()\n\n# Function to calculate trendline\ndef calculate_trendline(x, y):\n    model = LinearRegression()\n    x_reshaped = np.array(x).reshape(-1, 1)\n    model.fit(x_reshaped, y)\n    trend = model.predict(x_reshaped)\n    return trend\n\n# Calculate trendline for DrySoilDays_Summer_whole\ndry_soil_trend = calculate_trendline(combined_soil_data_dry_aggregated['year'], combined_soil_data_dry_aggregated['DrySoilDays_Summer_whole'])\n\n# Create the second area chart for DrySoilDays_Summer_whole with trendline\nfig2 = go.Figure(\n    data=[\n        go.Scatter(\n            x=combined_soil_data_dry_aggregated['year'],\n            y=combined_soil_data_dry_aggregated['DrySoilDays_Summer_whole'],\n            fill='tozeroy',\n            name='Dry Soil Days in Summer',\n            mode='none',\n            fillcolor='rgba(165,42,42,0.5)'\n        ),\n        go.Scatter(\n            x=combined_soil_data_dry_aggregated['year'],\n            y=dry_soil_trend,\n            mode='lines',\n            name='Trendline',\n            line=dict(color='blue', dash='dash')\n        )\n    ],\n    layout=dict(\n        title='Dry Soil Days in Summer Trend',\n        xaxis_title='Year',\n        yaxis_title='Dry Soil Days',\n        title_font=dict(size=18, family='Arial, sans-serif'),\n        xaxis=dict(tickfont=dict(size=14)),\n        yaxis=dict(tickfont=dict(size=14)),\n        width=800,\n        height=600,\n        plot_bgcolor='#f7f7f7',\n        hoverlabel=dict(font_size=16),\n        showlegend=True,\n        legend=dict(\n            orientation='h',\n            yanchor='top',\n            y=-0.2,\n            xanchor='center',\n            x=0.5\n        ),\n        title_x=0.5\n    )\n)\n\n# Display the second plot\nfig2.show()\n\n\n                        \n                                            \nThis area chart outlines the number of dry soil days during summer over the years. The blue dashed trend line shows an increasing trend, indicating that the region is experiencing more dry soil days over time.\n\n\n\n\nNext up:\nSince rising temperatures and hotter days cause drier soils, does this mean evaporation speeds up in the summer? This in turn could potentially affect the area’s rainfall!",
    "crumbs": [
      "Temp Talk",
      "Soil Sizzle 🍂"
    ]
  },
  {
    "objectID": "index.html#temp-talk",
    "href": "index.html#temp-talk",
    "title": "Projects",
    "section": "Temp Talk",
    "text": "Temp Talk\n\n\n\n\n\n\n\n\n\n\nIntroduction\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#us-insights",
    "href": "index.html#us-insights",
    "title": "Projects",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "6000-website/website-source/ml.html",
    "href": "6000-website/website-source/ml.html",
    "title": "Machine Learning Analysis",
    "section": "",
    "text": "This machine learning (ML) analysis focuses on Reddit discussions about U.S. states, aiming to uncover patterns and categorize conversations. Using advanced unsupervised learning techniques like Latent Dirichlet Allocation (LDA), we identify the dominant themes and visualize their distribution geographically. Each section dives into one aspect of state discussions, contributing to a broader understanding of state-based dynamics on Reddit. The data source for this analysis was the subreddit data\n\n\n\n\n\nUsing LDA, Reddit discussions were categorized into four primary topics:\n\nTourism and Travel\nPolitics and Governance\nLifestyle and Culture\nEconomy and Business\n\nThe most common topic for each state was identified and visualized on the interactive map below.\n\n\n\n\n\n\n\n\n\n\n\nStates like Florida, Nevada, and Hawaii prominently feature travel-related discussions.\nThese discussions are likely tied to vacation planning, major tourist attractions, and seasonal trends.\n\nTakeaway: Travel topics reflect Reddit’s role in sharing experiences and discussing popular destinations with strong tourism appeal.\n\n\n\n\n\nStates like Montana, Washington, Arizona, and Michigan dominate political discussions.\nThese states likely feature political conversations tied to regional issues, legislative matters, and elections.\n\nTakeaway: Political topics highlight regional political activity and Reddit’s ability to surface diverse political conversations.\n\n\n\n\n\nStates like New York and Louisiana lead in discussions about lifestyle and culture.\nThese states are recognized for their unique traditions, festivals, food, and cultural diversity.\n\nTakeaway: Discussions about lifestyle and culture reflect the importance of local traditions and regional identity in state-focused conversations.\n\n\n\n\n\nStates like California and Texas dominate commerce-related discussions.\nThese discussions center around industries, employment opportunities, and economic trends in these major business hubs.\n\nTakeaway: Economic topics highlight Reddit’s relevance in tracking business activity and labor market dynamics in key economic states.\n\n\n\n\n\n\n\nThis is one of many analyses that explore Reddit discussions about U.S. states. Together with other insights, such as temporal trends, sentiment analysis, and subreddit-level dynamics, these findings provide a multifaceted view of online state discussions.\n\n\n\nFurther exploration could involve:\n\nTemporal Trends: How do discussions about topics like tourism and politics evolve over time?\nEngagement Analysis: Which topics drive the most interactions (upvotes and comments) on Reddit?\nSubreddit Activity: How do specific subreddits amplify or shape state discussions?\n\n\n\n\n\n\n\n\n\nUsing different spark regression models, the goal was to see if we could determine the score of a post (the total of likes minus the total of dislikes) using the text of the post itself and whether a state is mentioned or not. The goal of this was to determine if the presence of certain states were able to predict the score of a post, which is a stand in for the popularity.\n\n\n\nBoth a linear regression and random forest model were applied and both ended up with a RMSE of 68.3275. Two different models getting the same RMSE was a bit of a wake up call but the results were confirmed upon multile re-runs. It must be the case that 68.3275 is the best RMSE possible with the structure built in.\n\n\n\n\n\nA RMSE of 68.3275 is not a great result especially given that the majority of posts only have a score in the thousands. This leads us to believe that a model predicting the score is not feasible and that a state being mentioned does not have a tangible effect on the score.\n\n\n\nFurhter exploration could involve:\n\nAlternate Models: We could try out other machine learning models to see if we could get better results.\nImproved data cleaning: The data could be further cleaned through normalization and text editting which would improve results\nChange Target: While this is more of an expansion, changing the feature being targetted could allow for us to learn what we want through a different measure.\n\n\n\n\n\n\n\n\n\nTwo machine learning models, Random Forest Classifier and Logistic Regression, were implemented in PySpark to categorize product descriptions into predefined categories (main_category). The models used a text-processing pipeline, including tokenization, TF-IDF feature extraction, and classification. Their performance was evaluated on various metrics, including accuracy, F1-score, precision, and AUC-ROC scores for each class.\n\n\n\nRandom Forest Classifier achieved the best performance with following hyperparameters - Number of Features (HashingTF) : 50000 - Number of Trees : 100 - Max Depth : 5\nLogistic Regression achieved the best performance with the following hyperparameters - Number of Features (HashingTF) : 10000 - Regularization Parameter (RegParam) : 0.1\n\n\n\nComparison of ML Models\n\n\nBoth models achieve an AUC-ROC score greater than 0.5, indicating that they are capable of distinguishing between the categories of the subreddit with some level of reliability. However, the logistic regression model (AUC-ROC: 0.6739) performs better than the reinforcement model (AUC-ROC: 0.6251) in terms of distinguishing the classes in the main_category.\n\n\n\nCombine Random Forest and Logistic Regression for an ensemble model to leverage the strengths of both algorithms. Explore advanced NLP models like BERT or Transformer-based architectures for better semantic understanding.\n\n\n\n\nBy identifying the most common topic for each state, this analysis lays the groundwork for deeper investigations. Insights into Reddit conversations highlight the platform’s ability to capture real-world trends, opinions, and interests, making it a valuable source for understanding public discourse at a state level. Logistic Regression performs better in Accuracy, F1-Score, and AUC-ROC, making it the stronger model overall for predicting the subreddit based on the post.",
    "crumbs": [
      "ML"
    ]
  },
  {
    "objectID": "6000-website/website-source/ml.html#most-common-topics-by-state",
    "href": "6000-website/website-source/ml.html#most-common-topics-by-state",
    "title": "Machine Learning Analysis",
    "section": "",
    "text": "Using LDA, Reddit discussions were categorized into four primary topics:\n\nTourism and Travel\nPolitics and Governance\nLifestyle and Culture\nEconomy and Business\n\nThe most common topic for each state was identified and visualized on the interactive map below.\n\n\n\n\n\n\n\n\n\n\n\nStates like Florida, Nevada, and Hawaii prominently feature travel-related discussions.\nThese discussions are likely tied to vacation planning, major tourist attractions, and seasonal trends.\n\nTakeaway: Travel topics reflect Reddit’s role in sharing experiences and discussing popular destinations with strong tourism appeal.\n\n\n\n\n\nStates like Montana, Washington, Arizona, and Michigan dominate political discussions.\nThese states likely feature political conversations tied to regional issues, legislative matters, and elections.\n\nTakeaway: Political topics highlight regional political activity and Reddit’s ability to surface diverse political conversations.\n\n\n\n\n\nStates like New York and Louisiana lead in discussions about lifestyle and culture.\nThese states are recognized for their unique traditions, festivals, food, and cultural diversity.\n\nTakeaway: Discussions about lifestyle and culture reflect the importance of local traditions and regional identity in state-focused conversations.\n\n\n\n\n\nStates like California and Texas dominate commerce-related discussions.\nThese discussions center around industries, employment opportunities, and economic trends in these major business hubs.\n\nTakeaway: Economic topics highlight Reddit’s relevance in tracking business activity and labor market dynamics in key economic states.\n\n\n\n\n\n\n\nThis is one of many analyses that explore Reddit discussions about U.S. states. Together with other insights, such as temporal trends, sentiment analysis, and subreddit-level dynamics, these findings provide a multifaceted view of online state discussions.\n\n\n\nFurther exploration could involve:\n\nTemporal Trends: How do discussions about topics like tourism and politics evolve over time?\nEngagement Analysis: Which topics drive the most interactions (upvotes and comments) on Reddit?\nSubreddit Activity: How do specific subreddits amplify or shape state discussions?",
    "crumbs": [
      "ML"
    ]
  },
  {
    "objectID": "6000-website/website-source/ml.html#predicting-the-score-of-a-post",
    "href": "6000-website/website-source/ml.html#predicting-the-score-of-a-post",
    "title": "Machine Learning Analysis",
    "section": "",
    "text": "Using different spark regression models, the goal was to see if we could determine the score of a post (the total of likes minus the total of dislikes) using the text of the post itself and whether a state is mentioned or not. The goal of this was to determine if the presence of certain states were able to predict the score of a post, which is a stand in for the popularity.\n\n\n\nBoth a linear regression and random forest model were applied and both ended up with a RMSE of 68.3275. Two different models getting the same RMSE was a bit of a wake up call but the results were confirmed upon multile re-runs. It must be the case that 68.3275 is the best RMSE possible with the structure built in.\n\n\n\n\n\nA RMSE of 68.3275 is not a great result especially given that the majority of posts only have a score in the thousands. This leads us to believe that a model predicting the score is not feasible and that a state being mentioned does not have a tangible effect on the score.\n\n\n\nFurhter exploration could involve:\n\nAlternate Models: We could try out other machine learning models to see if we could get better results.\nImproved data cleaning: The data could be further cleaned through normalization and text editting which would improve results\nChange Target: While this is more of an expansion, changing the feature being targetted could allow for us to learn what we want through a different measure.",
    "crumbs": [
      "ML"
    ]
  },
  {
    "objectID": "6000-website/website-source/ml.html#predicting-the-subreddit-based-on-the-post",
    "href": "6000-website/website-source/ml.html#predicting-the-subreddit-based-on-the-post",
    "title": "Machine Learning Analysis",
    "section": "",
    "text": "Two machine learning models, Random Forest Classifier and Logistic Regression, were implemented in PySpark to categorize product descriptions into predefined categories (main_category). The models used a text-processing pipeline, including tokenization, TF-IDF feature extraction, and classification. Their performance was evaluated on various metrics, including accuracy, F1-score, precision, and AUC-ROC scores for each class.\n\n\n\nRandom Forest Classifier achieved the best performance with following hyperparameters - Number of Features (HashingTF) : 50000 - Number of Trees : 100 - Max Depth : 5\nLogistic Regression achieved the best performance with the following hyperparameters - Number of Features (HashingTF) : 10000 - Regularization Parameter (RegParam) : 0.1\n\n\n\nComparison of ML Models\n\n\nBoth models achieve an AUC-ROC score greater than 0.5, indicating that they are capable of distinguishing between the categories of the subreddit with some level of reliability. However, the logistic regression model (AUC-ROC: 0.6739) performs better than the reinforcement model (AUC-ROC: 0.6251) in terms of distinguishing the classes in the main_category.\n\n\n\nCombine Random Forest and Logistic Regression for an ensemble model to leverage the strengths of both algorithms. Explore advanced NLP models like BERT or Transformer-based architectures for better semantic understanding.",
    "crumbs": [
      "ML"
    ]
  },
  {
    "objectID": "6000-website/website-source/ml.html#conclusion",
    "href": "6000-website/website-source/ml.html#conclusion",
    "title": "Machine Learning Analysis",
    "section": "",
    "text": "By identifying the most common topic for each state, this analysis lays the groundwork for deeper investigations. Insights into Reddit conversations highlight the platform’s ability to capture real-world trends, opinions, and interests, making it a valuable source for understanding public discourse at a state level. Logistic Regression performs better in Accuracy, F1-Score, and AUC-ROC, making it the stronger model overall for predicting the subreddit based on the post.",
    "crumbs": [
      "ML"
    ]
  },
  {
    "objectID": "6000-website/website-source/index.html",
    "href": "6000-website/website-source/index.html",
    "title": "Analyzing U.S. State Sentiments on Reddit",
    "section": "",
    "text": "Public Opinion on US States\n\n\n\n\nHow do online conversations reflect our perceptions of U.S. states? During the brainstorming process for this project, our team noticed an absence of geography in the Reddit data. To add a spatial component, we decided to examine mentions of U.S. states and the sentiment of posts containing these mentions. Our project analyzes Reddit discussions to uncover insights about public opinion, trends, and key topics of interest for each state. Through sentiment analysis, trend discovery, and keyword extraction, we aim to understand the connection between online discourse and real-world perceptions.\n\n\n\n\n\n\nExamine state mentions over time to explore trends and correlations with news events.\nDetermine the states that are talked about the most and least to discover which states generate the highest interest.\n\nIdentify the subreddits where states are being mentioned the most.\nExplore the top authors that mention states.\nAnalyze state mentions relative to population and examine how the mentions-per-population rank compares to other state rankings.\nDiscover the states that are talked about most in state-specific subreddits and identify the state-specific subreddits that are most insular.\n\n\n\n\n\nAnalyze state sentiment over time and how news events drive changes in sentiment.\nExamine the states with the highest and lowest sentiment.\nAnalyze state sentiment in state-specific subreddits to determine universal or mixed state opinions.\nExplore how state sentiment aligns with popular state rankings on various factors including economy, healthcare, and education.\nInvestigate the most popular words used in posts mentioning states.\n\n\n\n\n\nIdentify the most common topics discussed in state-mentioned posts.\nPredict the score of a post based on its text and mentions of states.\nPredict the state mentioned based on the text of a post.\n\nPlease see the EDA, NLP, or ML tabs for more information on the specific research objectives.\n\n\n\n\nOur group decided to tackle the topic with two different cuts of Reddit data. We started with the cleaned Reddit data provided to the class, which included submissions and posts from June 2023 to July 2024.\nTo create the first cut of data, we selected specific subreddits where we believed states would be mentioned and opinions about states discussed. We limited the data to the following subreddits: r/Travel, r/AskAnAmerican, r/USATravel, as well as the 50 individual state subreddits (e.g., r/NewYork, r/California, r/Ohio). From these subreddits, we further filtered the data to include only submissions and comments where full state names were mentioned. This dataset (referred to as the Subreddit Data) contained over 120,000 submissions and posts where full state names were mentioned.\nThe second cut of data took a simpler and more universal approach. From the full dataset, we filtered for comments containing full state names. We decided to focus on comments, as they are more likely to contain opinions about states. After cleaning, this dataset (referred to as the Full Data) contained over 28 million comments mentioning full state names.\nA potential data issue is the use of full state names. Posts regarding notable figures like George Washington or sports teams such as the New York Yankees may introduce noise into our analysis. This issue is discussed further in the Summary section.\n\n\n\nData Collection Overview\n\n\n\n\n\nWe decided to use three different external data sources for our analysis:\n\nWhile tracking mentions of states seemed essential, we thought it would be important to examine mentions of states relative to their population. To do this, we obtained state-level population estimates from the United States Census Bureau.\nNext, we wanted to make comparisons between state mentions and how often a state had been visited (assuming more visits mean more mentions). For this, we used a YouGov survey of U.S. adults that examined the percentage of respondents who had visited a given state before.\nFinally, we wanted to see how the mentions and sentiment of states compared with other perceptions and rankings of states. We decided to use the U.S. News & World Report Best State Rankings for this analysis. These rankings comprise a range of data to evaluate states on education, livability, health, and much more.\n\n\n\n\nExternal Data Overview\n\n\n\n\n\nEnsuring high data quality is essential for reliable insights. Here are the challenges we encountered and how we addressed them:\n\nMissing Data: Several posts lacked critical metadata such as timestamps or subreddit names.\nIrrelevant Mentions: Posts occasionally referenced states ambiguously (e.g., “Georgia” as a name vs. the state).\nText Noise: Many posts contained irrelevant or low-quality text, such as excessive links, emojis, or advertisements.\nSentiment Overlap: Sentiment analysis tools sometimes misclassified posts due to sarcasm or complex sentence structures.\nInconsistent Formatting: User-generated content often featured inconsistent capitalization, punctuation, and spelling, complicating keyword extraction.\n\n\n\n\nFiltering irrelevant posts and ambiguous mentions using regex and manual review.\nImputing or discarding missing data based on its significance to the analysis.\nCleaning and normalizing text using standard preprocessing techniques like lowercasing, stopword removal, and punctuation handling.\n\n\n\n\n\n\n\n\n\nTechnical Proposal:\nThe daily mentions of full state names will be calculated for the entire time period for each state. A smoothed and Z-scored 7-day rolling average of daily state mention counts will then be computed and plotted. States with abnormal peaks will also be investigated to see if major news stories occurred simultaneously. This analysis will be conducted using the Subreddit Data.\n\n\n\nTechnical Proposal:\nReddit comments with mentions of full states and territories will be examined for the entire time period. A count of state mentions for each state will be calculated and plotted. This analysis will be conducted using the Full Data.\n\n\n\nTechnical Proposal:\nReddit comments with mentions of full states and territories will be examined for the entire time period. A count of state mentions for each subreddit will be calculated and plotted/tabulated. The top 10 subreddits that mention states will be plotted. This analysis will be conducted using the Full Data.\n\n\n\nTechnical Proposal:\nReddit comments with mentions of full states and territories will be examined for the entire time period. A count of state mentions for each author will be calculated and plotted/tabulated. The top 10 authors that mention states will be plotted. This analysis will be conducted using the Full Data.\n\n\n\nTechnical Proposal:\nThe total mentions of full state names will be calculated for the entire time period for each state. Using external data, state mentions will be normalized to state population, and the state mention rank will be compared to external state ranks. This analysis will be conducted using the Subreddit Data.\n\n\n\nTechnical Proposal:\nReddit comments with mentions of full states and territories will be examined for the entire time period. State mentions in state-specific subreddits will be examined to explore how often state subreddits mention themselves in addition to other states. This analysis will be conducted using the Subreddit Data.\n\n\n\n\n\n\n\nTechnical Proposal:\nUsing daily posts that mention states, evaluate the sentiment of states over time. Evaluate sentiment by state with a pre-trained NLP model to categorize posts into positive, negative, or neutral. Calculate average daily sentiment for each state and smooth the results with a rolling average. Finally, examine major news events and changes to sentiment during the same time period. This analysis will be conducted using the Subreddit Data.\n\n\n\nTechnical Proposal:\nUsing Reddit comments that mention states, evaluate the sentiment of states over the entire time period. Evaluate sentiment by state with a pre-trained NLP model to categorize posts into positive, negative, or neutral. Calculate average sentiment for each state and order states and territories by the sentiment score. This analysis will be conducted using the Full Data.\n\n\n\nTechnical Proposal:\nUsing posts that mention states in state-specific subreddits, evaluate the sentiment of states. Evaluate sentiment by state mentioned and subreddit with a pre-trained NLP model to categorize posts into positive, negative, or neutral. Calculate average sentiment for each state-subreddit combination to see opinions of other states across state-specific subreddits. This analysis will be conducted using the Subreddit Data.\n\n\n\nTechnical Proposal:\nUsing Reddit comments that mention states, evaluate the sentiment of states over the entire time period. Evaluate sentiment by state with a pre-trained NLP model to categorize posts into positive, negative, or neutral. Calculate average sentiment for each state and order states and territories by the sentiment score. With this ranking of states, compare it to other external state rankings to examine correlations. This analysis will be conducted using the Full Data.\n\n\n\nTechnical Proposal:\nExamine posts that mention full state names. From these posts, count the most popular words on posts to get a sense of key words associated with mentions of states. Visualize this with a word cloud. This analysis will be conducted using the Full Data.\n\n\n\n\n\n\n\nTechnical Proposal:\nAnalyze posts that mention states to identify common topics using topic modeling techniques such as Latent Dirichlet Allocation (LDA). LDA is a statistical model that clusters words frequently appearing together, uncovering hidden topics within a dataset. By applying LDA to the text of posts mentioning states, we aim to extract overarching themes, such as tourism, politics, local events, or cultural identity.\nThe analysis involves:\n\nPreprocessing the text data by cleaning it (e.g., removing stop words, punctuation, and irrelevant text) and converting it into a suitable format for topic modeling (e.g., bag-of-words or TF-IDF matrix).\nDetermining the optimal number of topics using methods like coherence score analysis to ensure quality topics.\nRunning the LDA model to assign topics to each post and extract the most representative keywords for each topic.\nInterpreting topics by reviewing keywords within each cluster to understand the context of discussions.\nVisualizing the results using tools like topic distribution charts or interactive plots (e.g., pyLDAvis) to explore dominant themes in posts mentioning states.\n\nThis approach allows us to explore the diversity of conversations surrounding U.S. states on Reddit and identify patterns in the types of discussions associated with different states. For instance, one topic may focus on travel and tourism, another on politics, while others may emphasize cultural events or economic issues. These insights can reveal how users engage with state-specific topics and how perceptions vary across states.\n\n\n\nTechnical Proposal:\nTrain a supervised machine learning model to predict the upvote score of a post based on its text and mentions of states. Features will include text embeddings, sentiment, and state mentions. This analysis will be conducted using the Subreddit Data.\n\n\n\nTechnical Proposal:\nTrain a supervised machine learning model to predict the state mentioned in a post based on its text. Features will include text embeddings and other contextual features derived from the post. This analysis will be conducted using the Subreddit Data.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "6000-website/website-source/index.html#introduction",
    "href": "6000-website/website-source/index.html#introduction",
    "title": "Analyzing U.S. State Sentiments on Reddit",
    "section": "",
    "text": "How do online conversations reflect our perceptions of U.S. states? During the brainstorming process for this project, our team noticed an absence of geography in the Reddit data. To add a spatial component, we decided to examine mentions of U.S. states and the sentiment of posts containing these mentions. Our project analyzes Reddit discussions to uncover insights about public opinion, trends, and key topics of interest for each state. Through sentiment analysis, trend discovery, and keyword extraction, we aim to understand the connection between online discourse and real-world perceptions.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "6000-website/website-source/index.html#research-questions-business-goals",
    "href": "6000-website/website-source/index.html#research-questions-business-goals",
    "title": "Analyzing U.S. State Sentiments on Reddit",
    "section": "",
    "text": "Examine state mentions over time to explore trends and correlations with news events.\nDetermine the states that are talked about the most and least to discover which states generate the highest interest.\n\nIdentify the subreddits where states are being mentioned the most.\nExplore the top authors that mention states.\nAnalyze state mentions relative to population and examine how the mentions-per-population rank compares to other state rankings.\nDiscover the states that are talked about most in state-specific subreddits and identify the state-specific subreddits that are most insular.\n\n\n\n\n\nAnalyze state sentiment over time and how news events drive changes in sentiment.\nExamine the states with the highest and lowest sentiment.\nAnalyze state sentiment in state-specific subreddits to determine universal or mixed state opinions.\nExplore how state sentiment aligns with popular state rankings on various factors including economy, healthcare, and education.\nInvestigate the most popular words used in posts mentioning states.\n\n\n\n\n\nIdentify the most common topics discussed in state-mentioned posts.\nPredict the score of a post based on its text and mentions of states.\nPredict the state mentioned based on the text of a post.\n\nPlease see the EDA, NLP, or ML tabs for more information on the specific research objectives.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "6000-website/website-source/index.html#data-collection",
    "href": "6000-website/website-source/index.html#data-collection",
    "title": "Analyzing U.S. State Sentiments on Reddit",
    "section": "",
    "text": "Our group decided to tackle the topic with two different cuts of Reddit data. We started with the cleaned Reddit data provided to the class, which included submissions and posts from June 2023 to July 2024.\nTo create the first cut of data, we selected specific subreddits where we believed states would be mentioned and opinions about states discussed. We limited the data to the following subreddits: r/Travel, r/AskAnAmerican, r/USATravel, as well as the 50 individual state subreddits (e.g., r/NewYork, r/California, r/Ohio). From these subreddits, we further filtered the data to include only submissions and comments where full state names were mentioned. This dataset (referred to as the Subreddit Data) contained over 120,000 submissions and posts where full state names were mentioned.\nThe second cut of data took a simpler and more universal approach. From the full dataset, we filtered for comments containing full state names. We decided to focus on comments, as they are more likely to contain opinions about states. After cleaning, this dataset (referred to as the Full Data) contained over 28 million comments mentioning full state names.\nA potential data issue is the use of full state names. Posts regarding notable figures like George Washington or sports teams such as the New York Yankees may introduce noise into our analysis. This issue is discussed further in the Summary section.\n\n\n\nData Collection Overview",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "6000-website/website-source/index.html#external-data",
    "href": "6000-website/website-source/index.html#external-data",
    "title": "Analyzing U.S. State Sentiments on Reddit",
    "section": "",
    "text": "We decided to use three different external data sources for our analysis:\n\nWhile tracking mentions of states seemed essential, we thought it would be important to examine mentions of states relative to their population. To do this, we obtained state-level population estimates from the United States Census Bureau.\nNext, we wanted to make comparisons between state mentions and how often a state had been visited (assuming more visits mean more mentions). For this, we used a YouGov survey of U.S. adults that examined the percentage of respondents who had visited a given state before.\nFinally, we wanted to see how the mentions and sentiment of states compared with other perceptions and rankings of states. We decided to use the U.S. News & World Report Best State Rankings for this analysis. These rankings comprise a range of data to evaluate states on education, livability, health, and much more.\n\n\n\n\nExternal Data Overview",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "6000-website/website-source/index.html#data-quality-issues",
    "href": "6000-website/website-source/index.html#data-quality-issues",
    "title": "Analyzing U.S. State Sentiments on Reddit",
    "section": "",
    "text": "Ensuring high data quality is essential for reliable insights. Here are the challenges we encountered and how we addressed them:\n\nMissing Data: Several posts lacked critical metadata such as timestamps or subreddit names.\nIrrelevant Mentions: Posts occasionally referenced states ambiguously (e.g., “Georgia” as a name vs. the state).\nText Noise: Many posts contained irrelevant or low-quality text, such as excessive links, emojis, or advertisements.\nSentiment Overlap: Sentiment analysis tools sometimes misclassified posts due to sarcasm or complex sentence structures.\nInconsistent Formatting: User-generated content often featured inconsistent capitalization, punctuation, and spelling, complicating keyword extraction.\n\n\n\n\nFiltering irrelevant posts and ambiguous mentions using regex and manual review.\nImputing or discarding missing data based on its significance to the analysis.\nCleaning and normalizing text using standard preprocessing techniques like lowercasing, stopword removal, and punctuation handling.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "6000-website/website-source/index.html#appendix-research-questions-business-goals",
    "href": "6000-website/website-source/index.html#appendix-research-questions-business-goals",
    "title": "Analyzing U.S. State Sentiments on Reddit",
    "section": "",
    "text": "Technical Proposal:\nThe daily mentions of full state names will be calculated for the entire time period for each state. A smoothed and Z-scored 7-day rolling average of daily state mention counts will then be computed and plotted. States with abnormal peaks will also be investigated to see if major news stories occurred simultaneously. This analysis will be conducted using the Subreddit Data.\n\n\n\nTechnical Proposal:\nReddit comments with mentions of full states and territories will be examined for the entire time period. A count of state mentions for each state will be calculated and plotted. This analysis will be conducted using the Full Data.\n\n\n\nTechnical Proposal:\nReddit comments with mentions of full states and territories will be examined for the entire time period. A count of state mentions for each subreddit will be calculated and plotted/tabulated. The top 10 subreddits that mention states will be plotted. This analysis will be conducted using the Full Data.\n\n\n\nTechnical Proposal:\nReddit comments with mentions of full states and territories will be examined for the entire time period. A count of state mentions for each author will be calculated and plotted/tabulated. The top 10 authors that mention states will be plotted. This analysis will be conducted using the Full Data.\n\n\n\nTechnical Proposal:\nThe total mentions of full state names will be calculated for the entire time period for each state. Using external data, state mentions will be normalized to state population, and the state mention rank will be compared to external state ranks. This analysis will be conducted using the Subreddit Data.\n\n\n\nTechnical Proposal:\nReddit comments with mentions of full states and territories will be examined for the entire time period. State mentions in state-specific subreddits will be examined to explore how often state subreddits mention themselves in addition to other states. This analysis will be conducted using the Subreddit Data.\n\n\n\n\n\n\n\nTechnical Proposal:\nUsing daily posts that mention states, evaluate the sentiment of states over time. Evaluate sentiment by state with a pre-trained NLP model to categorize posts into positive, negative, or neutral. Calculate average daily sentiment for each state and smooth the results with a rolling average. Finally, examine major news events and changes to sentiment during the same time period. This analysis will be conducted using the Subreddit Data.\n\n\n\nTechnical Proposal:\nUsing Reddit comments that mention states, evaluate the sentiment of states over the entire time period. Evaluate sentiment by state with a pre-trained NLP model to categorize posts into positive, negative, or neutral. Calculate average sentiment for each state and order states and territories by the sentiment score. This analysis will be conducted using the Full Data.\n\n\n\nTechnical Proposal:\nUsing posts that mention states in state-specific subreddits, evaluate the sentiment of states. Evaluate sentiment by state mentioned and subreddit with a pre-trained NLP model to categorize posts into positive, negative, or neutral. Calculate average sentiment for each state-subreddit combination to see opinions of other states across state-specific subreddits. This analysis will be conducted using the Subreddit Data.\n\n\n\nTechnical Proposal:\nUsing Reddit comments that mention states, evaluate the sentiment of states over the entire time period. Evaluate sentiment by state with a pre-trained NLP model to categorize posts into positive, negative, or neutral. Calculate average sentiment for each state and order states and territories by the sentiment score. With this ranking of states, compare it to other external state rankings to examine correlations. This analysis will be conducted using the Full Data.\n\n\n\nTechnical Proposal:\nExamine posts that mention full state names. From these posts, count the most popular words on posts to get a sense of key words associated with mentions of states. Visualize this with a word cloud. This analysis will be conducted using the Full Data.\n\n\n\n\n\n\n\nTechnical Proposal:\nAnalyze posts that mention states to identify common topics using topic modeling techniques such as Latent Dirichlet Allocation (LDA). LDA is a statistical model that clusters words frequently appearing together, uncovering hidden topics within a dataset. By applying LDA to the text of posts mentioning states, we aim to extract overarching themes, such as tourism, politics, local events, or cultural identity.\nThe analysis involves:\n\nPreprocessing the text data by cleaning it (e.g., removing stop words, punctuation, and irrelevant text) and converting it into a suitable format for topic modeling (e.g., bag-of-words or TF-IDF matrix).\nDetermining the optimal number of topics using methods like coherence score analysis to ensure quality topics.\nRunning the LDA model to assign topics to each post and extract the most representative keywords for each topic.\nInterpreting topics by reviewing keywords within each cluster to understand the context of discussions.\nVisualizing the results using tools like topic distribution charts or interactive plots (e.g., pyLDAvis) to explore dominant themes in posts mentioning states.\n\nThis approach allows us to explore the diversity of conversations surrounding U.S. states on Reddit and identify patterns in the types of discussions associated with different states. For instance, one topic may focus on travel and tourism, another on politics, while others may emphasize cultural events or economic issues. These insights can reveal how users engage with state-specific topics and how perceptions vary across states.\n\n\n\nTechnical Proposal:\nTrain a supervised machine learning model to predict the upvote score of a post based on its text and mentions of states. Features will include text embeddings, sentiment, and state mentions. This analysis will be conducted using the Subreddit Data.\n\n\n\nTechnical Proposal:\nTrain a supervised machine learning model to predict the state mentioned in a post based on its text. Features will include text embeddings and other contextual features derived from the post. This analysis will be conducted using the Subreddit Data.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "6000-website/website-source/eda.html",
    "href": "6000-website/website-source/eda.html",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "",
    "text": "In this exploratory data analysis (EDA), we examine various aspects of Reddit discussions related to U.S. states. By analyzing trends over time, identifying popular subreddits and authors, and measuring engagement levels, we gain insights into how frequently and in what contexts different states are mentioned. Visualizations such as line plots, bar charts, and heatmaps help highlight patterns in state mentions, engagement disparities, and subreddit activity. These findings provide a foundational understanding of state-based discussions on Reddit, paving the way for deeper sentiment analysis and further exploration into the factors driving online interest in specific states.\n\n\n\nThis plot tracks the frequency of state mentions over time, helping us identify trends, seasonal patterns, or spikes in discussion. Peaks in certain months may correlate with state-related events, holidays, or news, offering insights into when and why specific states gain attention on Reddit. A seven-day rolling average is calculated to smooth the curve, and Z-scored trends provide a normalized view.\nWhat did we learn?\n\nStates like Maine, Nevada, and Louisiana exhibit noticeable spikes that align with seasonal events (e.g., tourism peaks, Mardi Gras, or holiday travel).\nSome anomalies in mentions may be driven by significant state-specific news or cultural events, such as major sporting events or natural disasters.\n\nInsights\nPeaks in state mentions reflect the dynamic nature of Reddit as a platform for real-time discussions about events and trends.\n\n\n\n\n\nIn this analysis, we observe daily mentions of Louisiana and Maine, smoothed and normalized using Z-scores to highlight significant spikes in public attention. For Louisiana, a sharp peak in mentions occurs in July 2024 coinciding with the mandate requiring public classrooms to display the Ten Commandments. In contrast, Maine experiences a dramatic spike in October 2023 during a manhunt following mass shootings in Lewiston. These trends underscore the strong correlation between major events and public discourse, as captured by daily mention data.\n\n\n\nLouisiana and Maine\n\n\n\n\n\n\nThis line plot shows the count of posts mentioning the top 10 most frequently discussed U.S. states over time on Reddit. By focusing on these states, we observe fluctuations in discussion levels and identify their timing.\nWhat did we learn?\n\nStates like California, Texas, and Florida consistently dominate mentions due to their large populations, cultural influence, and relevance in national conversations.\nSmaller states like Hawaii experience spikes likely tied to tourism, particularly during vacation seasons.\n\nInsights\nLarger states dominate overall mentions, while smaller states gain temporary prominence tied to tourism or specific events.\n\n\n\n\n\nThis bar chart is shows which states were talked about the most to the least. In contrast to the other graphs, this data is pulled from a sweep of the entire dataset for when a state was mentioned by name. Thus, the subreddits mentions reflect every time a state was mentioned, and the next two graphs will use the same database to show other facets of this data\nWhat did we learn?\n\nOf the states, Texas, Florida, and California were mentioned the most by far with the Dakotas being the two least mentioned states\nTerritories like American Samoa, Washington DC, and the virgin islands did not come up as much, and, as will be seen later, this led to them having more polarized sentiment scores.\n\nInsights\nWhile this data shows and interesting trend amongst the mentions of states, this data also references cases where the state name may be mentioned in other contexts like in names, sports, songs, or other reasons. This may lead to popular names being over represented.\n\n\n\nMentions of States\n\n\n\n\n\n\nThis Table chart highlights the top subreddits by submission count. Communities like CFB and CollegeBasketball lead, showing significant relevance to sports-related state discussions.\nWhat did we learn?\n\nSports-focused subreddits are key contributors to state mentions, particularly during major sports seasons.\nGeneral-interest subreddits like AskReddit also play a significant role in broader state discussions.\n\nInsights: Sports and general-interest subreddits are pivotal in driving state-related activity on Reddit, making them valuable sources for analysis.\n\n\n\n\n\n\n\n\n\nTop Subreddits by Submission Count\n\n\n\n\n\n\nThis plot showcases the top authors on Reddit based on the number of submissions containing state mentions. It provides insight into the most active contributors in the dataset.\nWhat did we learn?\n\nAutomated accounts like AutoModerator play a significant role in structured conversations or subreddit management.\nA small number of highly active users contribute disproportionately to state-related discussions.\n\nInsights: Both bots and human users significantly influence the volume and nature of state mentions on Reddit.\n\n\n\n\n\n\n\n\n\n\n\nBy normalizing state mentions against population size, we identify states that are “punching above their weight” in online popularity. The bar chart orders the states by their mentions per 100,000 population of the state. Meanwhile, the scatter plots below show this mention-per-population rank compared against four other rankings:\n\nState population from the U.S. Census Bureau\nState visit popularity from a YouGov survey\nState overall ranking from U.S. News & World Report\nState nature ranking from U.S. News & World Report\n\nThis analysis was conducted using the Subreddit Data.\nWhat did we learn?\n\nSmall states like Wyoming and Alaska dominate mentions per capita, perhaps due to their unique attractions (e.g., Yellowstone, northern lights).\nStates with major tourist draws often outperform larger states in per capita mentions. However, when comparing these rankings with external data, we see that these high ranks for small states are more of a function of their low population and less about their perception or natural attractions.\n\nInsights\nPer capita metrics potentially highlight states with niche appeal or tourism-based popularity.\n\n\n\n\n\n\n\n\n\n\n\n\n\nMentions of states in state-specific subreddits are examined in the map and heatmap below. In the map, the percent of state mentions that are the name of the state-specific subreddit are shown where darker colors represent state subreddits where state mentions are mostly the subreddit’s state. In the heatmap, we can see both the percent of state mentions of the subreddit’s state but also all of the states mentioned in a state’s subreddit. Darker reds represent state subreddits where state mentions are mostly the subreddit’s state, while darker blues represent lower percents of mentions for non-subreddit states. This analysis was conducted using the Subreddit Data.\nWhat did we learn?\n\nNearby states are often mentioned together, reflecting regional proximity or shared cultural ties. For example, Maine and Massachusetts are mentioned a fair amount in New Hampshire’s subreddit.\nStates like California, Texas, and Florida have broad relevance and are mentioned across numerous subreddits.\nSome state subreddits are also more insular than others. For example, in California’s subreddit, 84% of state mentions are California, while only 45% of state mentions are New Hampshire in New Hampshire’s subreddit.\n\nInsights\nCross-state mentions suggest correlations with mobility, shared regional issues, or cultural relevance. Perhaps a measure of state self-importance can be gleamed from how much a state is mentioned in its own subreddit.\n\n\n\n\n\n\n\n\n\n\n\n\nEach visualization provides unique insights into how U.S. states are discussed on Reddit:\n\nEvent-driven activity: Peaks in mentions are often tied to cultural or regional events, underscoring the importance of temporal context.\nSubreddit diversity: A mix of niche and general-interest subreddits contributes to state mentions, reflecting the varied interests of Reddit users.\nEngagement disparities: Larger states dominate engagement, but smaller states excel in normalized metrics, highlighting unique aspects like tourism.\nCross-state mentions: Regional relationships and mobility trends are evident in subreddit-specific data.\n\nThe next steps involve applying sentiment analysis to measure the tone of discussions and exploring how these online perceptions align with real-world data.",
    "crumbs": [
      "EDA"
    ]
  },
  {
    "objectID": "6000-website/website-source/eda.html#state-mentions-over-time",
    "href": "6000-website/website-source/eda.html#state-mentions-over-time",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "",
    "text": "This plot tracks the frequency of state mentions over time, helping us identify trends, seasonal patterns, or spikes in discussion. Peaks in certain months may correlate with state-related events, holidays, or news, offering insights into when and why specific states gain attention on Reddit. A seven-day rolling average is calculated to smooth the curve, and Z-scored trends provide a normalized view.\nWhat did we learn?\n\nStates like Maine, Nevada, and Louisiana exhibit noticeable spikes that align with seasonal events (e.g., tourism peaks, Mardi Gras, or holiday travel).\nSome anomalies in mentions may be driven by significant state-specific news or cultural events, such as major sporting events or natural disasters.\n\nInsights\nPeaks in state mentions reflect the dynamic nature of Reddit as a platform for real-time discussions about events and trends.\n\n\n\n\n\nIn this analysis, we observe daily mentions of Louisiana and Maine, smoothed and normalized using Z-scores to highlight significant spikes in public attention. For Louisiana, a sharp peak in mentions occurs in July 2024 coinciding with the mandate requiring public classrooms to display the Ten Commandments. In contrast, Maine experiences a dramatic spike in October 2023 during a manhunt following mass shootings in Lewiston. These trends underscore the strong correlation between major events and public discourse, as captured by daily mention data.\n\n\n\nLouisiana and Maine",
    "crumbs": [
      "EDA"
    ]
  },
  {
    "objectID": "6000-website/website-source/eda.html#top-10-states-by-mentions-over-time",
    "href": "6000-website/website-source/eda.html#top-10-states-by-mentions-over-time",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "",
    "text": "This line plot shows the count of posts mentioning the top 10 most frequently discussed U.S. states over time on Reddit. By focusing on these states, we observe fluctuations in discussion levels and identify their timing.\nWhat did we learn?\n\nStates like California, Texas, and Florida consistently dominate mentions due to their large populations, cultural influence, and relevance in national conversations.\nSmaller states like Hawaii experience spikes likely tied to tourism, particularly during vacation seasons.\n\nInsights\nLarger states dominate overall mentions, while smaller states gain temporary prominence tied to tourism or specific events.",
    "crumbs": [
      "EDA"
    ]
  },
  {
    "objectID": "6000-website/website-source/eda.html#top-subreddit-in-full-data-mentions",
    "href": "6000-website/website-source/eda.html#top-subreddit-in-full-data-mentions",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "",
    "text": "This bar chart is shows which states were talked about the most to the least. In contrast to the other graphs, this data is pulled from a sweep of the entire dataset for when a state was mentioned by name. Thus, the subreddits mentions reflect every time a state was mentioned, and the next two graphs will use the same database to show other facets of this data\nWhat did we learn?\n\nOf the states, Texas, Florida, and California were mentioned the most by far with the Dakotas being the two least mentioned states\nTerritories like American Samoa, Washington DC, and the virgin islands did not come up as much, and, as will be seen later, this led to them having more polarized sentiment scores.\n\nInsights\nWhile this data shows and interesting trend amongst the mentions of states, this data also references cases where the state name may be mentioned in other contexts like in names, sports, songs, or other reasons. This may lead to popular names being over represented.\n\n\n\nMentions of States",
    "crumbs": [
      "EDA"
    ]
  },
  {
    "objectID": "6000-website/website-source/eda.html#top-subreddits-by-submission-count",
    "href": "6000-website/website-source/eda.html#top-subreddits-by-submission-count",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "",
    "text": "This Table chart highlights the top subreddits by submission count. Communities like CFB and CollegeBasketball lead, showing significant relevance to sports-related state discussions.\nWhat did we learn?\n\nSports-focused subreddits are key contributors to state mentions, particularly during major sports seasons.\nGeneral-interest subreddits like AskReddit also play a significant role in broader state discussions.\n\nInsights: Sports and general-interest subreddits are pivotal in driving state-related activity on Reddit, making them valuable sources for analysis.\n\n\n\n\n\n\n\n\n\nTop Subreddits by Submission Count",
    "crumbs": [
      "EDA"
    ]
  },
  {
    "objectID": "6000-website/website-source/eda.html#top-authors-by-submission-count",
    "href": "6000-website/website-source/eda.html#top-authors-by-submission-count",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "",
    "text": "This plot showcases the top authors on Reddit based on the number of submissions containing state mentions. It provides insight into the most active contributors in the dataset.\nWhat did we learn?\n\nAutomated accounts like AutoModerator play a significant role in structured conversations or subreddit management.\nA small number of highly active users contribute disproportionately to state-related discussions.\n\nInsights: Both bots and human users significantly influence the volume and nature of state mentions on Reddit.",
    "crumbs": [
      "EDA"
    ]
  },
  {
    "objectID": "6000-website/website-source/eda.html#state-mentions-relative-to-population",
    "href": "6000-website/website-source/eda.html#state-mentions-relative-to-population",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "",
    "text": "By normalizing state mentions against population size, we identify states that are “punching above their weight” in online popularity. The bar chart orders the states by their mentions per 100,000 population of the state. Meanwhile, the scatter plots below show this mention-per-population rank compared against four other rankings:\n\nState population from the U.S. Census Bureau\nState visit popularity from a YouGov survey\nState overall ranking from U.S. News & World Report\nState nature ranking from U.S. News & World Report\n\nThis analysis was conducted using the Subreddit Data.\nWhat did we learn?\n\nSmall states like Wyoming and Alaska dominate mentions per capita, perhaps due to their unique attractions (e.g., Yellowstone, northern lights).\nStates with major tourist draws often outperform larger states in per capita mentions. However, when comparing these rankings with external data, we see that these high ranks for small states are more of a function of their low population and less about their perception or natural attractions.\n\nInsights\nPer capita metrics potentially highlight states with niche appeal or tourism-based popularity.",
    "crumbs": [
      "EDA"
    ]
  },
  {
    "objectID": "6000-website/website-source/eda.html#state-mentions-by-subreddit",
    "href": "6000-website/website-source/eda.html#state-mentions-by-subreddit",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "",
    "text": "Mentions of states in state-specific subreddits are examined in the map and heatmap below. In the map, the percent of state mentions that are the name of the state-specific subreddit are shown where darker colors represent state subreddits where state mentions are mostly the subreddit’s state. In the heatmap, we can see both the percent of state mentions of the subreddit’s state but also all of the states mentioned in a state’s subreddit. Darker reds represent state subreddits where state mentions are mostly the subreddit’s state, while darker blues represent lower percents of mentions for non-subreddit states. This analysis was conducted using the Subreddit Data.\nWhat did we learn?\n\nNearby states are often mentioned together, reflecting regional proximity or shared cultural ties. For example, Maine and Massachusetts are mentioned a fair amount in New Hampshire’s subreddit.\nStates like California, Texas, and Florida have broad relevance and are mentioned across numerous subreddits.\nSome state subreddits are also more insular than others. For example, in California’s subreddit, 84% of state mentions are California, while only 45% of state mentions are New Hampshire in New Hampshire’s subreddit.\n\nInsights\nCross-state mentions suggest correlations with mobility, shared regional issues, or cultural relevance. Perhaps a measure of state self-importance can be gleamed from how much a state is mentioned in its own subreddit.",
    "crumbs": [
      "EDA"
    ]
  },
  {
    "objectID": "6000-website/website-source/eda.html#analysis-summary",
    "href": "6000-website/website-source/eda.html#analysis-summary",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "",
    "text": "Each visualization provides unique insights into how U.S. states are discussed on Reddit:\n\nEvent-driven activity: Peaks in mentions are often tied to cultural or regional events, underscoring the importance of temporal context.\nSubreddit diversity: A mix of niche and general-interest subreddits contributes to state mentions, reflecting the varied interests of Reddit users.\nEngagement disparities: Larger states dominate engagement, but smaller states excel in normalized metrics, highlighting unique aspects like tourism.\nCross-state mentions: Regional relationships and mobility trends are evident in subreddit-specific data.\n\nThe next steps involve applying sentiment analysis to measure the tone of discussions and exploring how these online perceptions align with real-world data.",
    "crumbs": [
      "EDA"
    ]
  },
  {
    "objectID": "6000-website/code/pull_subreddit_data.html",
    "href": "6000-website/code/pull_subreddit_data.html",
    "title": "Pull subreddit data for relevant subreddits",
    "section": "",
    "text": "We need an available Java installation to run pyspark. The easiest way to do this is to install JDK and set the proper paths using conda\n\n# Setup - Run only once per Kernel App\n%conda install https://anaconda.org/conda-forge/openjdk/11.0.1/download/linux-64/openjdk-11.0.1-hacce0ff_1021.tar.bz2\n\n# install PySpark\n%pip install pyspark==3.4.0\n\n# restart kernel\nfrom IPython.core.display import HTML\nHTML(\"&lt;script&gt;Jupyter.notebook.kernel.restart()&lt;/script&gt;\")\n\nIOStream.flush timed out\nRetrieving notices: ...working... done\n\nDownloading and Extracting Packages:\n\n\n## Package Plan ##\n\n  environment location: /opt/conda\n\n\n\nPreparing transaction: done\nVerifying transaction: done\nExecuting transaction: done\n\nNote: you may need to restart the kernel to use updated packages.\nIOStream.flush timed out\nRequirement already satisfied: pyspark==3.4.0 in /opt/conda/lib/python3.11/site-packages (3.4.0)\nRequirement already satisfied: py4j==0.10.9.7 in /opt/conda/lib/python3.11/site-packages (from pyspark==3.4.0) (0.10.9.7)\nNote: you may need to restart the kernel to use updated packages."
  },
  {
    "objectID": "6000-website/code/pull_subreddit_data.html#setup",
    "href": "6000-website/code/pull_subreddit_data.html#setup",
    "title": "Pull subreddit data for relevant subreddits",
    "section": "",
    "text": "We need an available Java installation to run pyspark. The easiest way to do this is to install JDK and set the proper paths using conda\n\n# Setup - Run only once per Kernel App\n%conda install https://anaconda.org/conda-forge/openjdk/11.0.1/download/linux-64/openjdk-11.0.1-hacce0ff_1021.tar.bz2\n\n# install PySpark\n%pip install pyspark==3.4.0\n\n# restart kernel\nfrom IPython.core.display import HTML\nHTML(\"&lt;script&gt;Jupyter.notebook.kernel.restart()&lt;/script&gt;\")\n\nIOStream.flush timed out\nRetrieving notices: ...working... done\n\nDownloading and Extracting Packages:\n\n\n## Package Plan ##\n\n  environment location: /opt/conda\n\n\n\nPreparing transaction: done\nVerifying transaction: done\nExecuting transaction: done\n\nNote: you may need to restart the kernel to use updated packages.\nIOStream.flush timed out\nRequirement already satisfied: pyspark==3.4.0 in /opt/conda/lib/python3.11/site-packages (3.4.0)\nRequirement already satisfied: py4j==0.10.9.7 in /opt/conda/lib/python3.11/site-packages (from pyspark==3.4.0) (0.10.9.7)\nNote: you may need to restart the kernel to use updated packages."
  },
  {
    "objectID": "6000-website/code/pull_subreddit_data.html#utilize-s3-data-within-local-pyspark",
    "href": "6000-website/code/pull_subreddit_data.html#utilize-s3-data-within-local-pyspark",
    "title": "Pull subreddit data for relevant subreddits",
    "section": "Utilize S3 Data within local PySpark",
    "text": "Utilize S3 Data within local PySpark\n\nBy specifying the hadoop-aws jar in our Spark config we’re able to access S3 datasets using the s3a file prefix.\nSince we’ve already authenticated ourself to SageMaker Studio , we can use our assumed SageMaker ExecutionRole for any S3 reads/writes by setting the credential provider as ContainerCredentialsProvider\n\n\n# Import pyspark and build Spark session\nfrom pyspark.sql import SparkSession\n\nspark = (\n    SparkSession.builder.appName(\"PySparkApp\")\n    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.2.2\")\n    .config(\n        \"fs.s3a.aws.credentials.provider\",\n        \"com.amazonaws.auth.ContainerCredentialsProvider\",\n    )\n    .getOrCreate()\n)\n\nprint(spark.version)\n\n3.4.0\n\n\nINFO:py4j.clientserver:Error while sending or receiving.\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.11/site-packages/py4j/clientserver.py\", line 503, in send_command\n    self.socket.sendall(command.encode(\"utf-8\"))\nConnectionResetError: [Errno 104] Connection reset by peer\nINFO:py4j.clientserver:Closing down clientserver connection\nINFO:root:Exception while sending command.\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.11/site-packages/py4j/clientserver.py\", line 503, in send_command\n    self.socket.sendall(command.encode(\"utf-8\"))\nConnectionResetError: [Errno 104] Connection reset by peer\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n    response = connection.send_command(command)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/lib/python3.11/site-packages/py4j/clientserver.py\", line 506, in send_command\n    raise Py4JNetworkError(\npy4j.protocol.Py4JNetworkError: Error while sending\nINFO:py4j.clientserver:Closing down clientserver connection"
  },
  {
    "objectID": "6000-website/code/pull_subreddit_data.html#process-s3-data-with-sagemaker-processing-job-pysparkprocessor",
    "href": "6000-website/code/pull_subreddit_data.html#process-s3-data-with-sagemaker-processing-job-pysparkprocessor",
    "title": "Pull subreddit data for relevant subreddits",
    "section": "Process S3 data with SageMaker Processing Job PySparkProcessor",
    "text": "Process S3 data with SageMaker Processing Job PySparkProcessor\nWe are going to move the above processing code in a Python file and then submit that file to SageMaker Processing Job’s PySparkProcessor.\n\n!mkdir -p ./pull_data\n\n\n%%writefile ./pull_data/process.py\n\nimport os\nimport logging\nimport argparse\n\n# Import pyspark and build Spark session\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import (\n    DoubleType,\n    IntegerType,\n    StringType,\n    StructField,\n    StructType,\n)\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col\n\nlogging.basicConfig(format='%(asctime)s,%(levelname)s,%(module)s,%(filename)s,%(lineno)d,%(message)s', level=logging.DEBUG)\nlogger = logging.getLogger()\nlogger.setLevel(logging.DEBUG)\nlogger.addHandler(logging.StreamHandler(sys.stdout))\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"app inputs and outputs\")\n    parser.add_argument(\"--s3_dataset_path_commments\", type=str, help=\"Path of dataset in S3 for reddit comments\")\n    parser.add_argument(\"--s3_dataset_path_submissions\", type=str, help=\"Path of dataset in S3 for reddit submissions\")\n    parser.add_argument(\"--s3_output_bucket\", type=str, help=\"s3 output bucket\")\n    parser.add_argument(\"--s3_output_prefix\", type=str, help=\"s3 output prefix\")\n    parser.add_argument(\"--subreddits\", type=str, help=\"comma separate list of subreddits of interest\")\n    args = parser.parse_args()\n\n    spark = SparkSession.builder.appName(\"PySparkApp\").getOrCreate()\n    logger.info(f\"spark version = {spark.version}\")\n    \n    # This is needed to save RDDs which is the only way to write nested Dataframes into CSV format\n    sc = spark.sparkContext\n    sc._jsc.hadoopConfiguration().set(\n        \"mapred.output.committer.class\", \"org.apache.hadoop.mapred.FileOutputCommitter\"\n    )\n\n   \n    # Downloading the data from S3 into a Dataframe\n    logger.info(f\"going to read {args.s3_dataset_path_commments}\")\n    comments = spark.read.parquet(args.s3_dataset_path_commments, header=True)\n    logger.info(f\"finished reading files...\")\n    \n    logger.info(f\"going to read {args.s3_dataset_path_submissions}\")\n    submissions = spark.read.parquet(args.s3_dataset_path_submissions, header=True)\n    logger.info(f\"finished reading files...\")\n    \n    # filter the dataframe to only keep the subreddits of interest\n    subreddits = [s.strip() for s in args.subreddits.split(\",\")]\n    excluded_cols = [] # ['edited', 'created_utc', 'retrieved_on']\n    \n    comments_included_cols = [c for c in comments.columns if c not in excluded_cols]\n    logger.info(f\"comments included_cols={comments_included_cols}\")\n\n    submissions_included_cols = [c for c in submissions.columns if c not in excluded_cols]\n    logger.info(f\"submissions included_cols={submissions_included_cols}\")\n\n    # subset the dataframes because \"edited\" and \"created_utc\" have data type problems\n    # sometimes they occur as int some time as float and since schema is encoded in the \n    # parquet files therefore different files have different data tpyes for these fields (float in some cases, int in some cases)\n    # and spark enforces strict type checking on read so the only option we have is to either\n    # fix this outside of spark or delete these columns.\n    comments = comments.select(comments_included_cols)\n    submissions = submissions.select(submissions_included_cols)\n    \n    submissions_filtered = submissions.where(lower(col(\"subreddit\")).isin(subreddits))\n    comments_filtered = comments.where(lower(col(\"subreddit\")).isin(subreddits))\n    \n    # save the filtered dataframes so that these files can now be used for future analysis\n    s3_path = f\"s3://{args.s3_output_bucket}/{args.s3_output_prefix}/comments\"\n    logger.info(f\"going to write comments for {subreddits} in {s3_path}\")\n    logger.info(f\"shape of the comments_filtered dataframe is {comments_filtered.count():,}x{len(comments_filtered.columns)}\")\n    comments_filtered.write.mode(\"overwrite\").parquet(s3_path)\n    \n    s3_path = f\"s3://{args.s3_output_bucket}/{args.s3_output_prefix}/submissions\"\n    logger.info(f\"going to write submissions for {subreddits} in {s3_path}\")\n    logger.info(f\"shape of the submissions_filtered dataframe is {submissions_filtered.count():,}x{len(submissions_filtered.columns)}\")\n    submissions_filtered.write.mode(\"overwrite\").parquet(s3_path)\n\nif __name__ == \"__main__\":\n    main()\n\nWriting ./pull_data/process.py\n\n\n\nNow submit this code to SageMaker Processing Job.\nFor the r/travel, r/usatravel, and r/AskAnAmerican subreddits\n\n%%time\nimport sagemaker\nfrom sagemaker.spark.processing import PySparkProcessor\n\n# Setup the PySpark processor to run the job. Note the instance type and instance count parameters. SageMaker will create these many instances of this type for the spark job.\nrole = sagemaker.get_execution_role()\nspark_processor = PySparkProcessor(\n    base_job_name=\"sm-spark-project\",\n    framework_version=\"3.3\",\n    role=role,\n    instance_count=4,\n    instance_type=\"ml.m5.xlarge\",\n    max_runtime_in_seconds=3600,\n)\n\n# s3 paths\nsession = sagemaker.Session()\nbucket = session.default_bucket()\ns3_dataset_path_commments = \"s3://bigdatateaching/reddit-project/reddit/parquet/comments/yyyy=*/mm=*/*.parquet\"\ns3_dataset_path_submissions = \"s3://bigdatateaching/reddit-project/reddit/parquet/submissions/yyyy=*/mm=*/*.parquet\"\noutput_prefix_data = \"project\"\noutput_prefix_logs = f\"spark_logs\"\n\n# modify this comma separated list to choose the subreddits of interest\n#subreddits = \"technology,chatgpt\"\nsubreddits = \"travel,usatravel,askanamerican\"\n    \n# run the job now, the arguments array is provided as command line to the Python script (Spark code in this case).\nspark_processor.run(\n    submit_app=\"./pull_data/process.py\",\n    arguments=[\n        \"--s3_dataset_path_commments\",\n        s3_dataset_path_commments,\n        \"--s3_dataset_path_submissions\",\n        s3_dataset_path_submissions,\n        \"--s3_output_bucket\",\n              bucket,\n        \"--s3_output_prefix\",\n        output_prefix_data,\n        \"--subreddits\",\n        subreddits,\n    ],\n    spark_event_logs_s3_uri=\"s3://{}/{}/spark_event_logs\".format(bucket, output_prefix_logs),\n    logs=False,\n)\n\nsagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\nsagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n...............................................................................................................................................................................................................................................................................................................................!CPU times: user 4.22 s, sys: 418 ms, total: 4.64 s\nWall time: 37min 42s\n\n\nINFO:sagemaker:Creating processing-job with name sm-spark-project-2024-11-08-16-16-49-511\n\n\n\n\nRe-write process.py for another pull\nFor the r/alabama, alaska, arizona, arkansas, california, colorado, connecticut, delaware, florida, georgia, hawaii, idaho, illinois, indiana, iowa, kansas, kentucky, louisiana, maine, maryland, massachusetts, michigan, minnesota, mississippi, missouri, montana, nebraska, nevada subreddits\n\n%%writefile ./pull_data/process.py\n\nimport os\nimport logging\nimport argparse\n\n# Import pyspark and build Spark session\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import (\n    DoubleType,\n    IntegerType,\n    StringType,\n    StructField,\n    StructType,\n)\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col\n\nlogging.basicConfig(format='%(asctime)s,%(levelname)s,%(module)s,%(filename)s,%(lineno)d,%(message)s', level=logging.DEBUG)\nlogger = logging.getLogger()\nlogger.setLevel(logging.DEBUG)\nlogger.addHandler(logging.StreamHandler(sys.stdout))\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"app inputs and outputs\")\n    parser.add_argument(\"--s3_dataset_path_commments\", type=str, help=\"Path of dataset in S3 for reddit comments\")\n    parser.add_argument(\"--s3_dataset_path_submissions\", type=str, help=\"Path of dataset in S3 for reddit submissions\")\n    parser.add_argument(\"--s3_output_bucket\", type=str, help=\"s3 output bucket\")\n    parser.add_argument(\"--s3_output_prefix\", type=str, help=\"s3 output prefix\")\n    parser.add_argument(\"--subreddits\", type=str, help=\"comma separate list of subreddits of interest\")\n    args = parser.parse_args()\n\n    spark = SparkSession.builder.appName(\"PySparkApp\").getOrCreate()\n    logger.info(f\"spark version = {spark.version}\")\n    \n    # This is needed to save RDDs which is the only way to write nested Dataframes into CSV format\n    sc = spark.sparkContext\n    sc._jsc.hadoopConfiguration().set(\n        \"mapred.output.committer.class\", \"org.apache.hadoop.mapred.FileOutputCommitter\"\n    )\n\n   \n    # Downloading the data from S3 into a Dataframe\n    logger.info(f\"going to read {args.s3_dataset_path_commments}\")\n    comments = spark.read.parquet(args.s3_dataset_path_commments, header=True)\n    logger.info(f\"finished reading files...\")\n    \n    logger.info(f\"going to read {args.s3_dataset_path_submissions}\")\n    submissions = spark.read.parquet(args.s3_dataset_path_submissions, header=True)\n    logger.info(f\"finished reading files...\")\n    \n    # filter the dataframe to only keep the subreddits of interest\n    subreddits = [s.strip() for s in args.subreddits.split(\",\")]\n    excluded_cols = [] # ['edited', 'created_utc', 'retrieved_on']\n    \n    comments_included_cols = [c for c in comments.columns if c not in excluded_cols]\n    logger.info(f\"comments included_cols={comments_included_cols}\")\n\n    submissions_included_cols = [c for c in submissions.columns if c not in excluded_cols]\n    logger.info(f\"submissions included_cols={submissions_included_cols}\")\n\n    # subset the dataframes because \"edited\" and \"created_utc\" have data type problems\n    # sometimes they occur as int some time as float and since schema is encoded in the \n    # parquet files therefore different files have different data tpyes for these fields (float in some cases, int in some cases)\n    # and spark enforces strict type checking on read so the only option we have is to either\n    # fix this outside of spark or delete these columns.\n    comments = comments.select(comments_included_cols)\n    submissions = submissions.select(submissions_included_cols)\n    \n    submissions_filtered = submissions.where(lower(col(\"subreddit\")).isin(subreddits))\n    comments_filtered = comments.where(lower(col(\"subreddit\")).isin(subreddits))\n    \n    # save the filtered dataframes so that these files can now be used for future analysis\n    s3_path = f\"s3://{args.s3_output_bucket}/{args.s3_output_prefix}/comments_2\"\n    logger.info(f\"going to write comments for {subreddits} in {s3_path}\")\n    logger.info(f\"shape of the comments_filtered dataframe is {comments_filtered.count():,}x{len(comments_filtered.columns)}\")\n    comments_filtered.write.mode(\"overwrite\").parquet(s3_path)\n    \n    s3_path = f\"s3://{args.s3_output_bucket}/{args.s3_output_prefix}/submissions_2\"\n    logger.info(f\"going to write submissions for {subreddits} in {s3_path}\")\n    logger.info(f\"shape of the submissions_filtered dataframe is {submissions_filtered.count():,}x{len(submissions_filtered.columns)}\")\n    submissions_filtered.write.mode(\"overwrite\").parquet(s3_path)\n\nif __name__ == \"__main__\":\n    main()\n\nOverwriting ./pull_data/process.py\n\n\n\n%%time\nimport sagemaker\nfrom sagemaker.spark.processing import PySparkProcessor\n\n# Setup the PySpark processor to run the job. Note the instance type and instance count parameters. SageMaker will create these many instances of this type for the spark job.\nrole = sagemaker.get_execution_role()\nspark_processor = PySparkProcessor(\n    base_job_name=\"sm-spark-project\",\n    framework_version=\"3.3\",\n    role=role,\n    instance_count=4,\n    instance_type=\"ml.m5.xlarge\",\n    max_runtime_in_seconds=3600,\n)\n\n# s3 paths\nsession = sagemaker.Session()\nbucket = session.default_bucket()\ns3_dataset_path_commments = \"s3://bigdatateaching/reddit-project/reddit/parquet/comments/yyyy=*/mm=*/*.parquet\"\ns3_dataset_path_submissions = \"s3://bigdatateaching/reddit-project/reddit/parquet/submissions/yyyy=*/mm=*/*.parquet\"\noutput_prefix_data = \"project\"\noutput_prefix_logs = f\"spark_logs\"\n\n# modify this comma separated list to choose the subreddits of interest\n#subreddits = \"technology,chatgpt\"\nsubreddits = \"alabama,alaska,arizona,arkansas,california,colorado,connecticut,delaware,florida,georgia,hawaii,idaho,illinois,indiana,iowa,kansas,kentucky,louisiana,maine,maryland,massachusetts,michigan,minnesota,mississippi,missouri,montana,nebraska,nevada\"\n    \n# run the job now, the arguments array is provided as command line to the Python script (Spark code in this case).\nspark_processor.run(\n    submit_app=\"./pull_data/process.py\",\n    arguments=[\n        \"--s3_dataset_path_commments\",\n        s3_dataset_path_commments,\n        \"--s3_dataset_path_submissions\",\n        s3_dataset_path_submissions,\n        \"--s3_output_bucket\",\n              bucket,\n        \"--s3_output_prefix\",\n        output_prefix_data,\n        \"--subreddits\",\n        subreddits,\n    ],\n    spark_event_logs_s3_uri=\"s3://{}/{}/spark_event_logs\".format(bucket, output_prefix_logs),\n    logs=False,\n)\n\nINFO:sagemaker:Creating processing-job with name sm-spark-project-2024-11-08-16-56-59-922\n\n\n.......................................................................................................................................................................................................................................................................................................................................................................!CPU times: user 2.32 s, sys: 256 ms, total: 2.58 s\nWall time: 35min 7s\n\n\n\n\nRe-write process.py for another pull\nFor the r/newhampshire, newjersey, newmexico, newyork, northcarolina, northdakota, ohio, oklahoma, oregon, pennsylvania, rhodeisland, southcarolina, southdakota, tennessee, texas, utah, vermont, virginia, washington, westvirginia, wisconsin, wyoming subreddits\n\n%%writefile ./pull_data/process.py\n\nimport os\nimport logging\nimport argparse\n\n# Import pyspark and build Spark session\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import (\n    DoubleType,\n    IntegerType,\n    StringType,\n    StructField,\n    StructType,\n)\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col\n\nlogging.basicConfig(format='%(asctime)s,%(levelname)s,%(module)s,%(filename)s,%(lineno)d,%(message)s', level=logging.DEBUG)\nlogger = logging.getLogger()\nlogger.setLevel(logging.DEBUG)\nlogger.addHandler(logging.StreamHandler(sys.stdout))\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"app inputs and outputs\")\n    parser.add_argument(\"--s3_dataset_path_commments\", type=str, help=\"Path of dataset in S3 for reddit comments\")\n    parser.add_argument(\"--s3_dataset_path_submissions\", type=str, help=\"Path of dataset in S3 for reddit submissions\")\n    parser.add_argument(\"--s3_output_bucket\", type=str, help=\"s3 output bucket\")\n    parser.add_argument(\"--s3_output_prefix\", type=str, help=\"s3 output prefix\")\n    parser.add_argument(\"--subreddits\", type=str, help=\"comma separate list of subreddits of interest\")\n    args = parser.parse_args()\n\n    spark = SparkSession.builder.appName(\"PySparkApp\").getOrCreate()\n    logger.info(f\"spark version = {spark.version}\")\n    \n    # This is needed to save RDDs which is the only way to write nested Dataframes into CSV format\n    sc = spark.sparkContext\n    sc._jsc.hadoopConfiguration().set(\n        \"mapred.output.committer.class\", \"org.apache.hadoop.mapred.FileOutputCommitter\"\n    )\n\n   \n    # Downloading the data from S3 into a Dataframe\n    logger.info(f\"going to read {args.s3_dataset_path_commments}\")\n    comments = spark.read.parquet(args.s3_dataset_path_commments, header=True)\n    logger.info(f\"finished reading files...\")\n    \n    logger.info(f\"going to read {args.s3_dataset_path_submissions}\")\n    submissions = spark.read.parquet(args.s3_dataset_path_submissions, header=True)\n    logger.info(f\"finished reading files...\")\n    \n    # filter the dataframe to only keep the subreddits of interest\n    subreddits = [s.strip() for s in args.subreddits.split(\",\")]\n    excluded_cols = [] # ['edited', 'created_utc', 'retrieved_on']\n    \n    comments_included_cols = [c for c in comments.columns if c not in excluded_cols]\n    logger.info(f\"comments included_cols={comments_included_cols}\")\n\n    submissions_included_cols = [c for c in submissions.columns if c not in excluded_cols]\n    logger.info(f\"submissions included_cols={submissions_included_cols}\")\n\n    # subset the dataframes because \"edited\" and \"created_utc\" have data type problems\n    # sometimes they occur as int some time as float and since schema is encoded in the \n    # parquet files therefore different files have different data tpyes for these fields (float in some cases, int in some cases)\n    # and spark enforces strict type checking on read so the only option we have is to either\n    # fix this outside of spark or delete these columns.\n    comments = comments.select(comments_included_cols)\n    submissions = submissions.select(submissions_included_cols)\n    \n    submissions_filtered = submissions.where(lower(col(\"subreddit\")).isin(subreddits))\n    comments_filtered = comments.where(lower(col(\"subreddit\")).isin(subreddits))\n    \n    # save the filtered dataframes so that these files can now be used for future analysis\n    s3_path = f\"s3://{args.s3_output_bucket}/{args.s3_output_prefix}/comments_3\"\n    logger.info(f\"going to write comments for {subreddits} in {s3_path}\")\n    logger.info(f\"shape of the comments_filtered dataframe is {comments_filtered.count():,}x{len(comments_filtered.columns)}\")\n    comments_filtered.write.mode(\"overwrite\").parquet(s3_path)\n    \n    s3_path = f\"s3://{args.s3_output_bucket}/{args.s3_output_prefix}/submissions_3\"\n    logger.info(f\"going to write submissions for {subreddits} in {s3_path}\")\n    logger.info(f\"shape of the submissions_filtered dataframe is {submissions_filtered.count():,}x{len(submissions_filtered.columns)}\")\n    submissions_filtered.write.mode(\"overwrite\").parquet(s3_path)\n\nif __name__ == \"__main__\":\n    main()\n\nOverwriting ./pull_data/process.py\n\n\n\n%%time\nimport sagemaker\nfrom sagemaker.spark.processing import PySparkProcessor\n\n# Setup the PySpark processor to run the job. Note the instance type and instance count parameters. SageMaker will create these many instances of this type for the spark job.\nrole = sagemaker.get_execution_role()\nspark_processor = PySparkProcessor(\n    base_job_name=\"sm-spark-project\",\n    framework_version=\"3.3\",\n    role=role,\n    instance_count=4,\n    instance_type=\"ml.m5.xlarge\",\n    max_runtime_in_seconds=3600,\n)\n\n# s3 paths\nsession = sagemaker.Session()\nbucket = session.default_bucket()\ns3_dataset_path_commments = \"s3://bigdatateaching/reddit-project/reddit/parquet/comments/yyyy=*/mm=*/*.parquet\"\ns3_dataset_path_submissions = \"s3://bigdatateaching/reddit-project/reddit/parquet/submissions/yyyy=*/mm=*/*.parquet\"\noutput_prefix_data = \"project\"\noutput_prefix_logs = f\"spark_logs\"\n\n# modify this comma separated list to choose the subreddits of interest\n#subreddits = \"technology,chatgpt\"\nsubreddits = \"newhampshire,newjersey,newmexico,newyork,northcarolina,northdakota,ohio,oklahoma,oregon,pennsylvania,rhodeisland,southcarolina,southdakota,tennessee,texas,utah,vermont,virginia,washington,westvirginia,wisconsin,wyoming\"\n    \n# run the job now, the arguments array is provided as command line to the Python script (Spark code in this case).\nspark_processor.run(\n    submit_app=\"./pull_data/process.py\",\n    arguments=[\n        \"--s3_dataset_path_commments\",\n        s3_dataset_path_commments,\n        \"--s3_dataset_path_submissions\",\n        s3_dataset_path_submissions,\n        \"--s3_output_bucket\",\n              bucket,\n        \"--s3_output_prefix\",\n        output_prefix_data,\n        \"--subreddits\",\n        subreddits,\n    ],\n    spark_event_logs_s3_uri=\"s3://{}/{}/spark_event_logs\".format(bucket, output_prefix_logs),\n    logs=False,\n)\n\nINFO:sagemaker:Creating processing-job with name sm-spark-project-2024-11-08-17-33-24-773\n\n\n.................................................................................................................................................................................................................................................................................................................................................................................................................................!CPU times: user 2.18 s, sys: 210 ms, total: 2.39 s\nWall time: 35min 7s"
  },
  {
    "objectID": "6000-website/code/pull_subreddit_data.html#read-and-combine-the-filtered-data",
    "href": "6000-website/code/pull_subreddit_data.html#read-and-combine-the-filtered-data",
    "title": "Pull subreddit data for relevant subreddits",
    "section": "Read and combine the filtered data",
    "text": "Read and combine the filtered data\nNow that we have filtered the data to only keep submissions and comments from subreddits of interest. Let us read data from the s3 path where we saved the filtered data.\n\n%%time\n# Read comments data\ns3_path_1 = f\"s3a://{bucket}/{output_prefix_data}/comments\"\ns3_path_2 = f\"s3a://{bucket}/{output_prefix_data}/comments_2\"\ns3_path_3 = f\"s3a://{bucket}/{output_prefix_data}/comments_3\"\n\nprint(f\"reading comments from {s3_path_1}\")\ncomments_1 = spark.read.parquet(s3_path_1, header=True)\nprint(f\"reading comments from {s3_path_2}\")\ncomments_2 = spark.read.parquet(s3_path_2, header=True)\nprint(f\"reading comments from {s3_path_3}\")\ncomments_3 = spark.read.parquet(s3_path_3, header=True)\n\n# Append comments data\ncomments = comments_1.union(comments_2)\ncomments = comments.union(comments_3)\nprint(f\"shape of the comments dataframe is {comments.count():,}x{len(comments.columns)}\")\n\n# Print schema\nprint(comments.printSchema)\ncomments.groupBy('subreddit').count().show()\n\nreading comments from s3a://sagemaker-us-east-1-562201516459/project/comments\nreading comments from s3a://sagemaker-us-east-1-562201516459/project/comments_2\nreading comments from s3a://sagemaker-us-east-1-562201516459/project/comments_3\n+-------------+-------+\n|    subreddit|  count|\n+-------------+-------+\n|       travel|1692634|\n|    usatravel|   3541|\n|AskAnAmerican|1484279|\n|       Hawaii| 229058|\n|     Arkansas| 206619|\n|       alaska| 155213|\n|  Connecticut| 596502|\n|     maryland| 374793|\n|     Nebraska|  92045|\n|massachusetts| 532337|\n|       Nevada|  43471|\n|  mississippi| 120069|\n|     Delaware| 121224|\n|      arizona| 212745|\n|     illinois| 151436|\n|      florida| 983754|\n|      Georgia| 289066|\n|      Montana|  94978|\n|     Michigan| 417908|\n|      Alabama| 134993|\n+-------------+-------+\nonly showing top 20 rows\n\nCPU times: user 821 ms, sys: 179 ms, total: 1e+03 ms\nWall time: 50min 29s\n\n\n[Stage 18:=====================================================&gt;(361 + 1) / 362]                                                                                \n\n\n\n%%time\n# Read submissions data\ns3_path_1 = f\"s3a://{bucket}/{output_prefix_data}/submissions\"\ns3_path_2 = f\"s3a://{bucket}/{output_prefix_data}/submissions_2\"\ns3_path_3 = f\"s3a://{bucket}/{output_prefix_data}/submissions_3\"\n\nprint(f\"reading submissions from {s3_path_1}\")\nsubmissions_1 = spark.read.parquet(s3_path_1, header=True)\nprint(f\"reading comments from {s3_path_2}\")\nsubmissions_2 = spark.read.parquet(s3_path_2, header=True)\nprint(f\"reading comments from {s3_path_3}\")\nsubmissions_3 = spark.read.parquet(s3_path_3, header=True)\n\n# Append submissions data\nsubmissions = submissions_1.union(submissions_2)\nsubmissions = submissions.union(submissions_3)\nprint(f\"shape of the submissions dataframe is {submissions.count():,}x{len(submissions.columns)}\")\n\n# Print schema\nprint(submissions.printSchema)\nsubmissions.groupBy('subreddit').count().show()\n\nreading submissions from s3a://sagemaker-us-east-1-562201516459/project/submissions\nreading comments from s3a://sagemaker-us-east-1-562201516459/project/submissions_2\nreading comments from s3a://sagemaker-us-east-1-562201516459/project/submissions_3\nshape of the submissions dataframe is 697,043x21\n&lt;bound method DataFrame.printSchema of DataFrame[author: string, author_flair_css_class: string, author_flair_text: string, created_utc: bigint, distinguished: string, domain: string, edited: double, id: string, is_self: boolean, locked: boolean, num_comments: bigint, over_18: boolean, quarantine: boolean, retrieved_on: bigint, score: bigint, selftext: string, stickied: boolean, subreddit: string, subreddit_id: string, title: string, url: string]&gt;\n+-------------+------+\n|    subreddit| count|\n+-------------+------+\n|       travel|169797|\n|    usatravel|   880|\n|AskAnAmerican| 30216|\n|       Hawaii| 11515|\n|     Arkansas|  7360|\n|       alaska|  7925|\n|  Connecticut| 21102|\n|     maryland| 15686|\n|     Nebraska|  2991|\n|massachusetts| 16189|\n|       Nevada|  2084|\n|  mississippi|  4362|\n|     Delaware|  6207|\n|      arizona| 10510|\n|     illinois|  6594|\n|      florida| 29461|\n|      Georgia| 10063|\n|      Montana|  4392|\n|     Michigan| 13109|\n|      Alabama|  5505|\n+-------------+------+\nonly showing top 20 rows\n\nCPU times: user 133 ms, sys: 20.8 ms, total: 154 ms\nWall time: 6min 56s\n\n\n                                                                                [Stage 33:======================================================&gt; (54 + 1) / 55]                                                                                \n\n\n\nSave data to S3\n\noutput_path_1 = f\"s3a://{bucket}/{output_prefix_data}/comments_filt\"\noutput_path_2 = f\"s3a://{bucket}/{output_prefix_data}/submissions_filt\"\ncomments.write.mode(\"overwrite\").parquet(output_path_1)\nsubmissions.write.mode(\"overwrite\").parquet(output_path_2)\n\n                                                                                \n\n\n\n# Test that data was saved correctly\ns3_path_1 = f\"s3a://{bucket}/{output_prefix_data}/comments_filt\"\nprint(f\"reading submissions from {s3_path_1}\")\ncomments = spark.read.parquet(s3_path_1, header=True)\nprint(f\"shape of the comments dataframe is {comments.count():,}x{len(comments.columns)}\")\n\nreading submissions from s3a://sagemaker-us-east-1-562201516459/project/comments_filt\nshape of the comments dataframe is 17,667,099x17\n\n\n[Stage 41:======================================================&gt; (32 + 1) / 33]                                                                                \n\n\n\n# Test that data was saved correctly\ns3_path_1 = f\"s3a://{bucket}/{output_prefix_data}/submissions_filt\"\nprint(f\"reading submissions from {s3_path_1}\")\ncomments = spark.read.parquet(s3_path_1, header=True)\nprint(f\"shape of the comments dataframe is {comments.count():,}x{len(comments.columns)}\")\n\nreading submissions from s3a://sagemaker-us-east-1-562201516459/project/submissions_filt\nshape of the comments dataframe is 697,043x21\n\n\n[Stage 45:===========================================&gt;              (3 + 1) / 4]"
  },
  {
    "objectID": "6000-website/website-source/nlp.html",
    "href": "6000-website/website-source/nlp.html",
    "title": "Natural Language Processing (NLP)",
    "section": "",
    "text": "In this section, we dive into Natural Language Processing (NLP) to explore the themes and topics associated with different U.S. states on Reddit. Using techniques like PCA and TF-IDF, we can start identifying patterns and clustering states based on the keywords that frequently come up in discussions. This analysis gives us a deeper look at how people talk about each state, setting the stage for more detailed sentiment and topic analysis in the next milestones.\n\n\n\n\nThis analysis tracks the sentiment of Reddit discussions mentioning U.S. states over time. Sentiment scores, ranging from -1 (negative) to 1 (positive), are averaged for each state and visualized using a 14-day moving average. This approach highlights long-term sentiment trends while smoothing out short-term fluctuations.\n\n\n\n\n\n\n\n\n\n\n\nSentiment Scores: Sentiment analysis was applied to Reddit posts, generating scores for each post mentioning a state.\nDate Aggregation: Posts were grouped by date, and average sentiment scores were calculated for each state.\nSmoothing: A 14-day rolling mean was applied to smooth the data and highlight long-term trends.\n\n\n\n\n\nEach state is displayed in a separate panel, showing how sentiment changes over time.\nThe y-axis represents the average sentiment score, ranging from -1 (negative sentiment) to 1 (positive sentiment).\n\n\n\n\n\n\n\n\n\nStates like Hawaii and Florida consistently exhibit positive sentiment, reflecting discussions related to travel and vacations.\nSeasonal spikes align with holidays and popular travel periods.\n\n\n\n\n\nStates such as Colorado and Indiana show relatively stable and neutral sentiment trends, indicating balanced or less emotionally charged discussions.\n\n\n\n\n\nStates like California and Texas show occasional dips into negative sentiment, often correlating with controversial topics or political discussions.\n\nHere we highlighted Louisiana and Maine because of notable sentiment shifts tied to significant events in each state. Let’s take a closer look: In Louisiana, a sharp rise in sentiment occurred in July 2024, coinciding with the state’s mandate requiring public classrooms to display the Ten Commandments. Meanwhile, in Maine, sentiment plummeted in October 2023 following mass shootings in Lewiston, where 22 people were killed, leading to a widespread manhunt. These examples illustrate how local events can strongly influence public sentiment over time.\n\n\n\nLouisiana and Maine\n\n\n\n\n\n\n\n\nEvent Overlay: Correlate sentiment trends with major events, such as elections, natural disasters, or cultural festivals.\nTopic Sentiment: Break down sentiment trends by topic (e.g., politics, tourism) for more nuanced insights.\nSubreddit Analysis: Examine how sentiment trends differ across various Reddit communities.\n\n\n\n\n\n\nWith our dataset of all the comments that mentioned a full state name, we calculated a state-level average to generate the full rankings of all the states and territories. This analysis was performed using the Hugging Face twitter-dl sentiment analysis model and processed in a Spark job. Below is the total ranking of all the states and territories:\n\n\n\nSentiment by State Ranking\n\n\nAt the very top we have a few territories, American Samoaa and the Virgin Islands, which have high scores but are also not mentioned as often as the the states which leads to their ranks being not as significant. Same with DC, since we had to use the full names DC was mentioned the second to least and it seems that if people are writing out the full name of the city then they are fairly unhappy with it.\nOtherwise, Montana and New Mexico were some of the more popular states with Ohio, Florida, and Iowa being some of the least popular. Too look at the states more generally though lets visualize the whole country:\n\n\n\nSentiment by State Map\n\n\nWe can see here the more popular states from the less popular ones although the sentiment tended to swing positive, meaning that states were not seen very negatively in the aggregate. A few do stand out though, the ones mentioned as well as California, Maine, and even Arizona don’t seem to have the best impressions. There are not a ton of regional blocks, most states seem to have a varying opinions rather than being grouped by location.\nWith this sentiment average and ranking we decided to compare to the US News & World Report rankings which had an aggregate score and subgroups which the states were ranked by. We wanted to see if the sentiment that we found had any interaction with those results and if so which and how. We did compare them all together but it ended up looking like alot of noise. Why don’t you see for yourself:\n\n\n\n\n\nThis sentiment matrix visualizes how Reddit discussions about U.S. states vary across different subreddits. Each cell represents the sentiment score for a particular state mentioned within a specific subreddit, with sentiment scores ranging from -1 (negative) to 1 (positive).\n\n\n\n\n\n\n\n\nSubreddit Diversity: States like California and Texas exhibit diverse sentiment across subreddits, reflecting the variety of topics and emotional tones associated with these states.\nConsistent Sentiment: Smaller states such as Wyoming show more uniform sentiment across subreddits, indicating focused or limited discussions.\nPolarized Discussions: Subreddits addressing politically or culturally charged states often display strong variations in sentiment, highlighting their contentious nature.\nRegional Overlaps: States in the same geographic region (e.g., southern or northeastern states) may share sentiment trends across certain subreddits, possibly due to shared cultural or regional topics.\n\n\n\n\n\nSentiment Scoring: Sentiment was calculated for each Reddit post mentioning a state using an NLP sentiment analysis model. Scores were aggregated at the subreddit level for each state.\nData Normalization: Scores were rounded and normalized for clarity in the visualization.\nVisualization: A heatmap was created to display sentiment scores, with a diverging color scale to indicate positive, neutral, and negative sentiment.\n\n\n\n\n\n\nDeep Dive into Subreddits: Investigate specific subreddits (e.g., politics, travel) to understand the drivers of sentiment for states like California and Florida.\nTemporal Analysis: Explore how sentiment across subreddits evolves over time, particularly for states with highly polarized discussions.\nTopic Alignment: Combine this analysis with topic modeling to identify dominant themes driving sentiment variations within subreddits.\n\n\n\n\nGif of Sentiment rank compared to State Rankings\n\n\nAcross all categories there did not seem to be any relationship between the sentiment average of a state and its ranking. This leads us to believe that the sentiment we are capturing is more around the vibes and feelings of a state which are more cultural and perspective driven than practical.\nOverall, we were able to look at a broad range of sentiment information around the states and found that these rankings were not tied to reality and thus may be more likely to shift to culutral rather than practical intervention.\n\n\n\n\nIn addition to looking at sentiment we wanted to look at word importance and hopefully importance by state. In order to do this we first attempted to use a TF-IDF model to ranking the different documents based on scores around subjects. While we were able to fit the model, we were unable to pull out relevant insights on an aggregate state by state level. In leiu of this though we went with the TF aspect of the model and compared those values. The wordcloud of the top 100 words can be seen below:\n\n\n\nWordcloud of the top 100 words\n\n\nThe top 5 words were one, people, new, also, and even in that order. We did remove stop words as a part of the process using pyspark’s StopWordsRemover() but most of what was left still wasn’t incredibly interesting and relevant. You can see a few states did have higher mentions like Texas, Florida, and California which all made it into the top 30. We did run this on state level subsets of the data but we ended up getting one as the topword for all of them and other familiar words to the overall list. We think that this is due to the size of the data and the differences between the values being too big for subsets to change it.\n\n\n\nIn this section, we demonstrated that sentiment can be pulled from reddit discussions to evaluate public opinions about states. This is evident given the drastic changes in sentiment that occurred in states like Maine and Louisiana during national, consuming news stories that drove discourse. Potential biases were more pronounced in this analysis, where we saw a territory like Washington DC exhibit extreme negative sentiment. Perhaps if other states were not contaminated with views and opinions about topics such as sports teams, a more accurate score of sentiment could be calculated. Finally, the sentiment being evaluated from these online discussions did not align with more traditional sources of state perception like state rankings on education, health, or economic performance. The sentiment of states in these posts more likely reflected topics like news, local culture, or the stochastic and ever-changing nature of public opinion. For future work, the sentiment of a state’s news could be compared to the sentiment of a state’s Reddit posts to examine similarities. In summary, while the sentiment we capture from Reddit is not traditional, it reflects a complicated phenomenon that could be useful in teasing out public opinion around marketing campaigns, state-level policy changes, or national events/news stories.",
    "crumbs": [
      "NLP"
    ]
  },
  {
    "objectID": "6000-website/website-source/nlp.html#sentiment-analysis-over-time",
    "href": "6000-website/website-source/nlp.html#sentiment-analysis-over-time",
    "title": "Natural Language Processing (NLP)",
    "section": "",
    "text": "This analysis tracks the sentiment of Reddit discussions mentioning U.S. states over time. Sentiment scores, ranging from -1 (negative) to 1 (positive), are averaged for each state and visualized using a 14-day moving average. This approach highlights long-term sentiment trends while smoothing out short-term fluctuations.\n\n\n\n\n\n\n\n\n\n\n\nSentiment Scores: Sentiment analysis was applied to Reddit posts, generating scores for each post mentioning a state.\nDate Aggregation: Posts were grouped by date, and average sentiment scores were calculated for each state.\nSmoothing: A 14-day rolling mean was applied to smooth the data and highlight long-term trends.\n\n\n\n\n\nEach state is displayed in a separate panel, showing how sentiment changes over time.\nThe y-axis represents the average sentiment score, ranging from -1 (negative sentiment) to 1 (positive sentiment).\n\n\n\n\n\n\n\n\n\nStates like Hawaii and Florida consistently exhibit positive sentiment, reflecting discussions related to travel and vacations.\nSeasonal spikes align with holidays and popular travel periods.\n\n\n\n\n\nStates such as Colorado and Indiana show relatively stable and neutral sentiment trends, indicating balanced or less emotionally charged discussions.\n\n\n\n\n\nStates like California and Texas show occasional dips into negative sentiment, often correlating with controversial topics or political discussions.\n\nHere we highlighted Louisiana and Maine because of notable sentiment shifts tied to significant events in each state. Let’s take a closer look: In Louisiana, a sharp rise in sentiment occurred in July 2024, coinciding with the state’s mandate requiring public classrooms to display the Ten Commandments. Meanwhile, in Maine, sentiment plummeted in October 2023 following mass shootings in Lewiston, where 22 people were killed, leading to a widespread manhunt. These examples illustrate how local events can strongly influence public sentiment over time.\n\n\n\nLouisiana and Maine\n\n\n\n\n\n\n\n\nEvent Overlay: Correlate sentiment trends with major events, such as elections, natural disasters, or cultural festivals.\nTopic Sentiment: Break down sentiment trends by topic (e.g., politics, tourism) for more nuanced insights.\nSubreddit Analysis: Examine how sentiment trends differ across various Reddit communities.",
    "crumbs": [
      "NLP"
    ]
  },
  {
    "objectID": "6000-website/website-source/nlp.html#state-level-sentiment-from-overall-mentions",
    "href": "6000-website/website-source/nlp.html#state-level-sentiment-from-overall-mentions",
    "title": "Natural Language Processing (NLP)",
    "section": "",
    "text": "With our dataset of all the comments that mentioned a full state name, we calculated a state-level average to generate the full rankings of all the states and territories. This analysis was performed using the Hugging Face twitter-dl sentiment analysis model and processed in a Spark job. Below is the total ranking of all the states and territories:\n\n\n\nSentiment by State Ranking\n\n\nAt the very top we have a few territories, American Samoaa and the Virgin Islands, which have high scores but are also not mentioned as often as the the states which leads to their ranks being not as significant. Same with DC, since we had to use the full names DC was mentioned the second to least and it seems that if people are writing out the full name of the city then they are fairly unhappy with it.\nOtherwise, Montana and New Mexico were some of the more popular states with Ohio, Florida, and Iowa being some of the least popular. Too look at the states more generally though lets visualize the whole country:\n\n\n\nSentiment by State Map\n\n\nWe can see here the more popular states from the less popular ones although the sentiment tended to swing positive, meaning that states were not seen very negatively in the aggregate. A few do stand out though, the ones mentioned as well as California, Maine, and even Arizona don’t seem to have the best impressions. There are not a ton of regional blocks, most states seem to have a varying opinions rather than being grouped by location.\nWith this sentiment average and ranking we decided to compare to the US News & World Report rankings which had an aggregate score and subgroups which the states were ranked by. We wanted to see if the sentiment that we found had any interaction with those results and if so which and how. We did compare them all together but it ended up looking like alot of noise. Why don’t you see for yourself:",
    "crumbs": [
      "NLP"
    ]
  },
  {
    "objectID": "6000-website/website-source/nlp.html#sentiment-matrix-by-subreddit",
    "href": "6000-website/website-source/nlp.html#sentiment-matrix-by-subreddit",
    "title": "Natural Language Processing (NLP)",
    "section": "",
    "text": "This sentiment matrix visualizes how Reddit discussions about U.S. states vary across different subreddits. Each cell represents the sentiment score for a particular state mentioned within a specific subreddit, with sentiment scores ranging from -1 (negative) to 1 (positive).\n\n\n\n\n\n\n\n\nSubreddit Diversity: States like California and Texas exhibit diverse sentiment across subreddits, reflecting the variety of topics and emotional tones associated with these states.\nConsistent Sentiment: Smaller states such as Wyoming show more uniform sentiment across subreddits, indicating focused or limited discussions.\nPolarized Discussions: Subreddits addressing politically or culturally charged states often display strong variations in sentiment, highlighting their contentious nature.\nRegional Overlaps: States in the same geographic region (e.g., southern or northeastern states) may share sentiment trends across certain subreddits, possibly due to shared cultural or regional topics.\n\n\n\n\n\nSentiment Scoring: Sentiment was calculated for each Reddit post mentioning a state using an NLP sentiment analysis model. Scores were aggregated at the subreddit level for each state.\nData Normalization: Scores were rounded and normalized for clarity in the visualization.\nVisualization: A heatmap was created to display sentiment scores, with a diverging color scale to indicate positive, neutral, and negative sentiment.\n\n\n\n\n\n\nDeep Dive into Subreddits: Investigate specific subreddits (e.g., politics, travel) to understand the drivers of sentiment for states like California and Florida.\nTemporal Analysis: Explore how sentiment across subreddits evolves over time, particularly for states with highly polarized discussions.\nTopic Alignment: Combine this analysis with topic modeling to identify dominant themes driving sentiment variations within subreddits.\n\n\n\n\nGif of Sentiment rank compared to State Rankings\n\n\nAcross all categories there did not seem to be any relationship between the sentiment average of a state and its ranking. This leads us to believe that the sentiment we are capturing is more around the vibes and feelings of a state which are more cultural and perspective driven than practical.\nOverall, we were able to look at a broad range of sentiment information around the states and found that these rankings were not tied to reality and thus may be more likely to shift to culutral rather than practical intervention.",
    "crumbs": [
      "NLP"
    ]
  },
  {
    "objectID": "6000-website/website-source/nlp.html#word-frequency-from-overall-mentions",
    "href": "6000-website/website-source/nlp.html#word-frequency-from-overall-mentions",
    "title": "Natural Language Processing (NLP)",
    "section": "",
    "text": "In addition to looking at sentiment we wanted to look at word importance and hopefully importance by state. In order to do this we first attempted to use a TF-IDF model to ranking the different documents based on scores around subjects. While we were able to fit the model, we were unable to pull out relevant insights on an aggregate state by state level. In leiu of this though we went with the TF aspect of the model and compared those values. The wordcloud of the top 100 words can be seen below:\n\n\n\nWordcloud of the top 100 words\n\n\nThe top 5 words were one, people, new, also, and even in that order. We did remove stop words as a part of the process using pyspark’s StopWordsRemover() but most of what was left still wasn’t incredibly interesting and relevant. You can see a few states did have higher mentions like Texas, Florida, and California which all made it into the top 30. We did run this on state level subsets of the data but we ended up getting one as the topword for all of them and other familiar words to the overall list. We think that this is due to the size of the data and the differences between the values being too big for subsets to change it.",
    "crumbs": [
      "NLP"
    ]
  },
  {
    "objectID": "6000-website/website-source/nlp.html#conclusion",
    "href": "6000-website/website-source/nlp.html#conclusion",
    "title": "Natural Language Processing (NLP)",
    "section": "",
    "text": "In this section, we demonstrated that sentiment can be pulled from reddit discussions to evaluate public opinions about states. This is evident given the drastic changes in sentiment that occurred in states like Maine and Louisiana during national, consuming news stories that drove discourse. Potential biases were more pronounced in this analysis, where we saw a territory like Washington DC exhibit extreme negative sentiment. Perhaps if other states were not contaminated with views and opinions about topics such as sports teams, a more accurate score of sentiment could be calculated. Finally, the sentiment being evaluated from these online discussions did not align with more traditional sources of state perception like state rankings on education, health, or economic performance. The sentiment of states in these posts more likely reflected topics like news, local culture, or the stochastic and ever-changing nature of public opinion. For future work, the sentiment of a state’s news could be compared to the sentiment of a state’s Reddit posts to examine similarities. In summary, while the sentiment we capture from Reddit is not traditional, it reflects a complicated phenomenon that could be useful in teasing out public opinion around marketing campaigns, state-level policy changes, or national events/news stories.",
    "crumbs": [
      "NLP"
    ]
  },
  {
    "objectID": "6000-website/website-source/summary.html",
    "href": "6000-website/website-source/summary.html",
    "title": "Summary",
    "section": "",
    "text": "In Conclusion!\n\n\n\n\nThrough our project, we sought to figure out how online conversations reflect our perceptions of U.S. states. To facilitate this, we used two different subsets of Reddit data to first visualize and explore the relationships and information in the data itself before beginning a modeling process to answer our questions. This included natural language processing for sentiment analysis, training models to predict features, and exploring the context of text posts across the site. Afterwards, we broke down our results to find connections between online discourse and real-world problems, uncovering insights and identifying opportunities for growth.\n\n\nSome of the insights we would like to highlight are:\n\n\n\n\n\nSentiment by State Map\n\n\nThis sentiment map visualizes the average sentiment of posts mentioning each U.S. state, derived from the full Reddit data. The color scale ranges from blue (indicating more positive sentiment) to red (indicating more negative sentiment), with yellow representing neutral sentiment.\n\nBlue States: States like Montana and Maine show positive sentiment, suggesting discussions around these states are generally favorable.\nRed States: States like Kansas and Mississippi lean toward negative sentiment, indicating less favorable discussions in the dataset.\nNeutral States: Most states hover around neutral sentiment (yellow-green), reflecting a balance of positive and negative discussions.\n\nThis visualization highlights how public sentiment on Reddit varies across states, offering insights into regional perceptions and how discussions align with or diverge from real-world reputations.\n\n\n\n\n\n\nLouisiana and Maine Mentions\n\n\nThis chart visualizes the daily mentions of Louisiana and Maine, normalized using Z-scores and smoothed over a 7-day rolling average. Key spikes in mentions correspond to major events:\n\nLouisiana: In July 2024, a sharp increase in mentions occurred when the state mandated the display of the Ten Commandments in public classrooms.\nMaine: In October 2023, mentions peaked dramatically during the manhunt in Lewiston after mass shootings that tragically claimed 22 lives.\n\nThese spikes underline the strong correlation between major state-specific events and public discourse, as captured through Reddit data.\n\n\n\n\n\n\n\nLouisiana and Maine Mentions\n\n\nThe comparative chart highlights how public attention varies between states:\n\nLouisiana’s mentions feature a distinct, isolated peak tied to its event.\nMaine’s mentions display a more prolonged period of heightened activity, reflecting extended discussions surrounding the tragic events in Lewiston.\n\nThese stood out to us as some of the more interesting results from our analysis. While there is always more to uncover, we are excited to share these findings.\n\n\n\n\n\n\nWhile we have made significant progress on this subject, there are additional steps we could take to further explore our questions and obtain more accurate results:\n\nAddress issues in the data caused by our regex search, which left some out-of-context words. Instead, we could use a context-specific model to focus on mentions where people are specifically talking about the state.\nExperiment with other sentiment models to observe if there are noticeable changes in state sentiment rankings.\n\n\n\n\n\nWhen we first started this project, we hoped to return a spatial (and special) element to the data, and we were successful in doing so while analyzing U.S. states. While this was our primary focus, we also see possibilities for applying our methods to other spatial aspects of the data, such as regions, countries, oceans, and other geographic entities.\nOur work demonstrates that sentiments often do not relate to the realities of a location but are instead based more on reputation. This insight could be the start of organizations tracking their popularity through looser metrics that are not bound to traditional methods. We hope that organizations explore this further in the future, and we also hope that the work we’ve done sets the stage for others to continue in a similar direction.",
    "crumbs": [
      "Summary"
    ]
  },
  {
    "objectID": "6000-website/website-source/summary.html#overview",
    "href": "6000-website/website-source/summary.html#overview",
    "title": "Summary",
    "section": "",
    "text": "Through our project, we sought to figure out how online conversations reflect our perceptions of U.S. states. To facilitate this, we used two different subsets of Reddit data to first visualize and explore the relationships and information in the data itself before beginning a modeling process to answer our questions. This included natural language processing for sentiment analysis, training models to predict features, and exploring the context of text posts across the site. Afterwards, we broke down our results to find connections between online discourse and real-world problems, uncovering insights and identifying opportunities for growth.\n\n\nSome of the insights we would like to highlight are:\n\n\n\n\n\nSentiment by State Map\n\n\nThis sentiment map visualizes the average sentiment of posts mentioning each U.S. state, derived from the full Reddit data. The color scale ranges from blue (indicating more positive sentiment) to red (indicating more negative sentiment), with yellow representing neutral sentiment.\n\nBlue States: States like Montana and Maine show positive sentiment, suggesting discussions around these states are generally favorable.\nRed States: States like Kansas and Mississippi lean toward negative sentiment, indicating less favorable discussions in the dataset.\nNeutral States: Most states hover around neutral sentiment (yellow-green), reflecting a balance of positive and negative discussions.\n\nThis visualization highlights how public sentiment on Reddit varies across states, offering insights into regional perceptions and how discussions align with or diverge from real-world reputations.\n\n\n\n\n\n\nLouisiana and Maine Mentions\n\n\nThis chart visualizes the daily mentions of Louisiana and Maine, normalized using Z-scores and smoothed over a 7-day rolling average. Key spikes in mentions correspond to major events:\n\nLouisiana: In July 2024, a sharp increase in mentions occurred when the state mandated the display of the Ten Commandments in public classrooms.\nMaine: In October 2023, mentions peaked dramatically during the manhunt in Lewiston after mass shootings that tragically claimed 22 lives.\n\nThese spikes underline the strong correlation between major state-specific events and public discourse, as captured through Reddit data.\n\n\n\n\n\n\n\nLouisiana and Maine Mentions\n\n\nThe comparative chart highlights how public attention varies between states:\n\nLouisiana’s mentions feature a distinct, isolated peak tied to its event.\nMaine’s mentions display a more prolonged period of heightened activity, reflecting extended discussions surrounding the tragic events in Lewiston.\n\nThese stood out to us as some of the more interesting results from our analysis. While there is always more to uncover, we are excited to share these findings.",
    "crumbs": [
      "Summary"
    ]
  },
  {
    "objectID": "6000-website/website-source/summary.html#next-steps",
    "href": "6000-website/website-source/summary.html#next-steps",
    "title": "Summary",
    "section": "",
    "text": "While we have made significant progress on this subject, there are additional steps we could take to further explore our questions and obtain more accurate results:\n\nAddress issues in the data caused by our regex search, which left some out-of-context words. Instead, we could use a context-specific model to focus on mentions where people are specifically talking about the state.\nExperiment with other sentiment models to observe if there are noticeable changes in state sentiment rankings.",
    "crumbs": [
      "Summary"
    ]
  },
  {
    "objectID": "6000-website/website-source/summary.html#conclusions",
    "href": "6000-website/website-source/summary.html#conclusions",
    "title": "Summary",
    "section": "",
    "text": "When we first started this project, we hoped to return a spatial (and special) element to the data, and we were successful in doing so while analyzing U.S. states. While this was our primary focus, we also see possibilities for applying our methods to other spatial aspects of the data, such as regions, countries, oceans, and other geographic entities.\nOur work demonstrates that sentiments often do not relate to the realities of a location but are instead based more on reputation. This insight could be the start of organizations tracking their popularity through looser metrics that are not bound to traditional methods. We hope that organizations explore this further in the future, and we also hope that the work we’ve done sets the stage for others to continue in a similar direction.",
    "crumbs": [
      "Summary"
    ]
  }
]