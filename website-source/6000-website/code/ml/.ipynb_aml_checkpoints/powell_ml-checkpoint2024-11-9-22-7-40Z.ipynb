{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "spark"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "09c8ba2d-49fa-498b-9d76-d59b947bd032",
              "statement_id": 6,
              "statement_ids": [
                6
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 0,
                  "RUNNING": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "session_id": "55",
              "normalized_state": "finished",
              "queued_time": "2024-12-09T19:32:05.5065523Z",
              "session_start_time": "2024-12-09T19:32:05.6212861Z",
              "execution_start_time": "2024-12-09T19:33:57.0122374Z",
              "execution_finish_time": "2024-12-09T19:33:57.4170234Z",
              "parent_msg_id": "fa23cd55-2ca3-48d2-8d11-71380650fc4f"
            },
            "text/plain": "StatementMeta(09c8ba2d-49fa-498b-9d76-d59b947bd032, 55, 6, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 11,
          "data": {
            "text/plain": "<pyspark.sql.session.SparkSession at 0x728ef580b9d0>",
            "text/html": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://vm-f6932184:39043\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.4.3.5.3.20241016.1</code></dd>\n              <dt>Master</dt>\n                <dd><code>yarn</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Azure ML Experiment</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "
          },
          "metadata": {}
        }
      ],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1733772801361
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Workspace, Dataset, Datastore\n",
        "\n",
        "subscription_id = '21ff0fc0-dd2c-450d-93b7-96eeb3699b22'\n",
        "resource_group = 'project-group-29'\n",
        "workspace_name = 'project-group-29'\n",
        "\n",
        "workspace = Workspace(subscription_id, resource_group, workspace_name)\n",
        "\n",
        "datastore = Datastore.get(workspace, \"workspaceblobstore\")\n",
        "dataset = Dataset.Tabular.from_parquet_files(path=(datastore, 'states/states_sentiment'))\n",
        "df = dataset.to_spark_dataframe()\n",
        "df.head()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "09c8ba2d-49fa-498b-9d76-d59b947bd032",
              "statement_id": 7,
              "statement_ids": [
                7
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 1,
                  "RUNNING": 0
                },
                "jobs": [
                  {
                    "displayName": "head at /tmp/ipykernel_26019/2394195642.py:12",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 19,
                    "usageDescription": "",
                    "jobId": 0,
                    "name": "head at /tmp/ipykernel_26019/2394195642.py:12",
                    "description": "Job group for statement 7:\nfrom azureml.core import Workspace, Dataset, Datastore\n\nsubscription_id = '21ff0fc0-dd2c-450d-93b7-96eeb3699b22'\nresource_group = 'project-group-29'\nworkspace_name = 'project-group-29'\n\nworkspace = Workspace(subscription_id, resource_group, workspace_name)\n\ndatastore = Datastore.get(workspace, \"workspaceblobstore\")\ndataset = Dataset.Tabular.from_parquet_files(path=(datastore, 'states/states_sentiment'))\ndf = dataset.to_spark_dataframe()\ndf.head()",
                    "submissionTime": "2024-12-09T19:38:03.301GMT",
                    "completionTime": "2024-12-09T19:39:28.335GMT",
                    "stageIds": [
                      0
                    ],
                    "jobGroup": "7",
                    "jobTags": [],
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "session_id": "55",
              "normalized_state": "finished",
              "queued_time": "2024-12-09T19:32:05.5077527Z",
              "session_start_time": null,
              "execution_start_time": "2024-12-09T19:33:57.5944994Z",
              "execution_finish_time": "2024-12-09T19:39:30.7885572Z",
              "parent_msg_id": "dddfa8ab-35b2-4d53-a949-5310f3023401"
            },
            "text/plain": "StatementMeta(09c8ba2d-49fa-498b-9d76-d59b947bd032, 55, 7, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Performing interactive authentication. Please follow the instructions on the terminal.\nTo sign in, use a web browser to open the page https://microsoft.com/devicelogin and enter the code AUDVU2D92 to authenticate.\nInteractive authentication successfully completed.\n{'infer_column_types': 'False', 'activity': 'to_spark_dataframe'}\n{'infer_column_types': 'False', 'activity': 'to_spark_dataframe', 'activityApp': 'TabularDataset'}\n{'infer_column_types': 'False', 'activity': 'to_spark_dataframe', 'activityApp': 'TabularDataset', 'runId': '23137735-185a-436e-b332-97b921c8fadf'}\n{'infer_column_types': 'False', 'activity': 'to_spark_dataframe', 'activityApp': 'TabularDataset', 'runId': '23137735-185a-436e-b332-97b921c8fadf', 'run_id': '23137735-185a-436e-b332-97b921c8fadf'}\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 13,
          "data": {
            "text/plain": "Row(author='fireweedflowers', text='I live in Colorado these days but I was born and raised in Wyoming and was emphatically a socialist.', controversiality=0, created_utc=1701619261, distinguished=None, edited=None, gilded=0, id='kbtqz1h', parent_id='t3_187jmr5', score=1, subreddit='wyoming', subreddit_id='t5_2r53d', alaska=False, alabama=False, arkansas=False, american samoa=False, arizona=False, california=False, colorado=True, connecticut=False, district of columbia=False, delaware=False, florida=False, georgia=False, guam=False, hawaii=False, iowa=False, idaho=False, illinois=False, indiana=False, kansas=False, kentucky=False, louisiana=False, massachusetts=False, maryland=False, maine=False, michigan=False, minnesota=False, missouri=False, mississippi=False, montana=False, north carolina=False, north dakota=False, nebraska=False, new hampshire=False, new jersey=False, new mexico=False, nevada=False, new york=False, ohio=False, oklahoma=False, oregon=False, pennsylvania=False, puerto rico=False, rhode island=False, south carolina=False, south dakota=False, tennessee=False, texas=False, utah=False, virginia=False, virgin islands=False, vermont=False, washington=False, wisconsin=False, west virginia=False, wyoming=True, sentiment_num=1)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733773134711
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#df.printSchema()\n",
        "df.show()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "09c8ba2d-49fa-498b-9d76-d59b947bd032",
              "statement_id": 8,
              "statement_ids": [
                8
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 1,
                  "RUNNING": 0
                },
                "jobs": [
                  {
                    "displayName": "showString at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 19,
                    "usageDescription": "",
                    "jobId": 1,
                    "name": "showString at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 8:\n#df.printSchema()\ndf.show()",
                    "submissionTime": "2024-12-09T19:39:31.841GMT",
                    "completionTime": "2024-12-09T19:39:47.182GMT",
                    "stageIds": [
                      1
                    ],
                    "jobGroup": "8",
                    "jobTags": [],
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "session_id": "55",
              "normalized_state": "finished",
              "queued_time": "2024-12-09T19:32:05.5088928Z",
              "session_start_time": null,
              "execution_start_time": "2024-12-09T19:39:30.9520027Z",
              "execution_finish_time": "2024-12-09T19:39:49.7881596Z",
              "parent_msg_id": "4ea49da5-12a5-4901-9a67-f489d9acb217"
            },
            "text/plain": "StatementMeta(09c8ba2d-49fa-498b-9d76-d59b947bd032, 55, 8, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "+--------------------+--------------------+----------------+-----------+-------------+------+------+-------+----------+-----+-------------------+------------+------+-------+--------+--------------+-------+----------+--------+-----------+--------------------+--------+-------+-------+-----+------+-----+-----+--------+-------+------+--------+---------+-------------+--------+-----+--------+---------+--------+-----------+-------+--------------+------------+--------+-------------+----------+----------+------+--------+-----+--------+------+------------+-----------+------------+--------------+------------+---------+-----+-----+--------+--------------+-------+----------+---------+-------------+-------+-------------+\n|              author|                text|controversiality|created_utc|distinguished|edited|gilded|     id| parent_id|score|          subreddit|subreddit_id|alaska|alabama|arkansas|american samoa|arizona|california|colorado|connecticut|district of columbia|delaware|florida|georgia| guam|hawaii| iowa|idaho|illinois|indiana|kansas|kentucky|louisiana|massachusetts|maryland|maine|michigan|minnesota|missouri|mississippi|montana|north carolina|north dakota|nebraska|new hampshire|new jersey|new mexico|nevada|new york| ohio|oklahoma|oregon|pennsylvania|puerto rico|rhode island|south carolina|south dakota|tennessee|texas| utah|virginia|virgin islands|vermont|washington|wisconsin|west virginia|wyoming|sentiment_num|\n+--------------------+--------------------+----------------+-----------+-------------+------+------+-------+----------+-----+-------------------+------------+------+-------+--------+--------------+-------+----------+--------+-----------+--------------------+--------+-------+-------+-----+------+-----+-----+--------+-------+------+--------+---------+-------------+--------+-----+--------+---------+--------+-----------+-------+--------------+------------+--------+-------------+----------+----------+------+--------+-----+--------+------+------------+-----------+------------+--------------+------------+---------+-----+-----+--------+--------------+-------+----------+---------+-------------+-------+-------------+\n|     fireweedflowers|I live in Colorad...|               0| 1701619261|         null|  null|     0|kbtqz1h|t3_187jmr5|    1|            wyoming|    t5_2r53d| false|  false|   false|         false|  false|     false|    true|      false|               false|   false|  false|  false|false| false|false|false|   false|  false| false|   false|    false|        false|   false|false|   false|    false|   false|      false|  false|         false|       false|   false|        false|     false|     false| false|   false|false|   false| false|       false|      false|       false|         false|       false|    false|false|false|   false|         false|  false|     false|    false|        false|   true|            1|\n|               IHB31|If Iowa was 12-1 ...|               0| 1701619262|         null|  null|     0|kbtqz3t|t1_kbtqbur|    2|                CFB|    t5_2qm9d| false|  false|   false|         false|  false|     false|   false|      false|               false|   false|  false|  false|false| false| true|false|   false|  false| false|   false|    false|        false|   false|false|    true|    false|   false|      false|  false|         false|       false|   false|        false|     false|     false| false|   false|false|   false| false|       false|      false|       false|         false|       false|    false|false|false|   false|         false|  false|     false|    false|        false|  false|            1|\n|      Moonlight12350|On FD participant...|               0| 1701619262|         null|  null|     0|kbtqz49|t3_189lmgf|    1|         sportsbook|    t5_2s3v4| false|   true|   false|         false|  false|     false|   false|      false|               false|   false|  false|  false|false| false|false|false|   false|  false| false|   false|    false|        false|   false|false|    true|    false|   false|      false|  false|         false|       false|   false|        false|     false|     false| false|   false|false|   false| false|       false|      false|       false|         false|       false|    false| true|false|   false|         false|  false|      true|    false|        false|  false|            1|\n|    fuzzypetiolesguy|Wrecked by who? U...|               0| 1701619263|         null|  null|     0|kbtqz6w|t1_kbrzgkq|    1|                CFB|    t5_2qm9d| false|  false|   false|         false|  false|     false|   false|      false|               false|   false|  false|  false|false| false|false|false|   false|  false| false|   false|    false|        false|   false|false|   false|    false|   false|      false|  false|         false|       false|   false|        false|     false|     false| false|   false|false|   false| false|       false|      false|       false|         false|       false|    false| true|false|   false|         false|  false|     false|    false|        false|  false|           -1|\n|      Suck_Me_Dry666|I think you naile...|               0| 1701619264|         null|  null|     0|kbtqz90|t3_189k238|   -1|          geography|    t5_2qnms| false|  false|   false|         false|  false|     false|   false|      false|               false|   false|  false|  false|false| false|false|false|   false|   true| false|   false|    false|        false|   false|false|   false|    false|   false|      false|  false|         false|       false|   false|        false|     false|     false| false|   false|false|   false| false|       false|      false|       false|         false|       false|    false|false|false|   false|         false|  false|     false|    false|        false|  false|            1|\n|             crc2993|Man, I’d honestly...|               0| 1701619264|         null|  null|     0|kbtqzal|t1_kbtqb3v|    1|                CFB|    t5_2qm9d| false|  false|   false|         false|  false|     false|   false|      false|               false|   false|  false|  false|false| false|false|false|   false|  false| false|   false|    false|        false|   false|false|   false|    false|   false|      false|  false|         false|       false|   false|        false|     false|     false| false|   false|false|   false| false|       false|      false|       false|         false|       false|    false| true|false|   false|         false|  false|     false|    false|        false|  false|            0|\n|         BurstAgentX|The latter, and i...|               0| 1701619265|         null|  null|     0|kbtqzdm|t1_kbtj7lx|    1|KitchenConfidential|    t5_2sa8b| false|  false|   false|         false|  false|     false|   false|      false|               false|   false|  false|  false|false| false|false|false|   false|  false| false|   false|    false|        false|   false|false|   false|    false|   false|      false|  false|         false|       false|   false|        false|     false|     false| false|   false|false|   false| false|        true|      false|       false|         false|       false|    false|false|false|   false|         false|  false|     false|    false|        false|  false|           -1|\n|       onesneakymofo|You're one of the...|               0| 1701619266|         null|  null|     0|kbtqze2|t1_kbtq2e4|    1|                CFB|    t5_2qm9d| false|  false|   false|         false|  false|     false|   false|      false|               false|   false|  false|  false|false| false|false|false|   false|  false| false|   false|    false|        false|   false|false|   false|    false|   false|      false|  false|         false|       false|   false|        false|     false|     false| false|   false|false|   false| false|       false|      false|       false|         false|       false|    false| true|false|   false|         false|  false|     false|    false|        false|  false|            0|\n|Justacouplemoreholes|Don't lose to Tex...|               0| 1701619268|         null|  null|     0|kbtqzmg|t1_kbt963r|  -11|           rolltide|    t5_2sanc| false|  false|   false|         false|  false|     false|   false|      false|               false|   false|  false|  false|false| false|false|false|   false|  false| false|   false|    false|        false|   false|false|   false|    false|   false|      false|  false|         false|       false|   false|        false|     false|     false| false|   false|false|   false| false|       false|      false|       false|         false|       false|    false| true|false|   false|         false|  false|     false|    false|        false|  false|            1|\n|         poonjouster|Cardale and the B...|               0| 1701619268|         null|  null|     0|kbtqznf|t1_kbt3xq6|    0|                CFB|    t5_2qm9d| false|  false|   false|         false|  false|     false|   false|      false|               false|   false|  false|  false|false| false|false|false|   false|  false| false|    true|    false|        false|   false|false|   false|    false|   false|      false|  false|         false|       false|   false|        false|     false|     false| false|   false|false|   false| false|       false|      false|       false|         false|       false|    false|false|false|   false|         false|  false|     false|     true|        false|  false|           -1|\n|     SpartansStandUp|No, Texas is a on...|               0| 1701619269|         null|  null|     0|kbtqzo1|t3_189tmhm|   -2|                CFB|    t5_2qm9d| false|   true|   false|         false|  false|     false|   false|      false|               false|   false|  false|   true|false| false|false|false|   false|  false| false|   false|    false|        false|   false|false|   false|    false|   false|      false|  false|         false|       false|   false|        false|     false|     false| false|   false|false|    true| false|       false|      false|       false|         false|       false|    false| true|false|   false|         false|  false|     false|    false|        false|  false|            1|\n|         pericles123|it comes down to ...|               0| 1701619269|         null|  null|     0|kbtqzog|t3_189vp4v|    2|                CFB|    t5_2qm9d| false|   true|   false|         false|  false|     false|   false|      false|               false|   false|  false|   true|false| false|false|false|   false|  false| false|   false|    false|        false|   false|false|   false|    false|   false|      false|  false|         false|       false|   false|        false|     false|     false| false|   false| true|   false|  true|       false|      false|       false|         false|       false|    false|false|false|   false|         false|  false|     false|    false|        false|  false|            1|\n|              _cdlc_|I was in complete...|               0| 1701619269|         null|  null|     0|kbtqzq4|t3_189ohq3|   27|       greysanatomy|    t5_2t2vo| false|  false|   false|         false|   true|     false|   false|      false|               false|   false|  false|  false|false| false|false|false|   false|  false| false|   false|    false|        false|   false|false|   false|    false|   false|      false|  false|         false|       false|   false|        false|     false|     false| false|   false|false|   false| false|       false|      false|       false|         false|       false|    false|false|false|   false|         false|  false|     false|    false|        false|  false|           -1|\n|     toomuchfrosting|Breaking: NCAA ha...|               0| 1701619270|         null|  null|     0|kbtqzrq|t3_189wh00|    5|                CFB|    t5_2qm9d| false|  false|   false|         false|  false|     false|   false|      false|               false|   false|  false|  false|false| false|false|false|   false|  false| false|   false|    false|        false|   false|false|    true|    false|   false|      false|  false|         false|       false|   false|        false|     false|     false| false|   false|false|   false| false|       false|      false|       false|         false|       false|    false|false|false|   false|         false|  false|     false|    false|        false|  false|           -1|\n|         Smokey19mom|If you like the J...|               0| 1701619270|         null|  null|     0|kbtqztk|t3_189w9we|    2|            bourbon|    t5_2rgos| false|  false|   false|         false|  false|     false|   false|      false|               false|   false|  false|  false|false| false|false|false|   false|  false| false|    true|    false|        false|   false|false|   false|    false|   false|      false|  false|         false|       false|   false|        false|     false|     false| false|   false|false|   false| false|       false|      false|       false|         false|       false|    false|false|false|   false|         false|  false|     false|    false|        false|  false|            1|\n|          Langland88|I guess that migh...|               0| 1701619271|         null|  null|     0|kbtqzvu|t3_189i18m|    2|          wisconsin|    t5_2qrc2| false|  false|   false|         false|  false|     false|   false|      false|               false|   false|  false|  false|false| false|false|false|   false|  false| false|   false|    false|        false|   false|false|   false|    false|   false|      false|  false|         false|       false|   false|        false|     false|     false| false|   false|false|   false| false|       false|      false|       false|         false|       false|    false|false|false|   false|         false|  false|     false|     true|        false|  false|            1|\n|     flaminhotcheeto|They just got luc...|               0| 1701619271|         null|  null|     0|kbtqzwj|t1_kbrpgw1|    0|                CFB|    t5_2qm9d| false|  false|   false|         false|  false|     false|   false|      false|               false|   false|  false|  false|false| false|false|false|   false|  false| false|   false|    false|        false|   false|false|   false|    false|   false|      false|  false|         false|       false|   false|        false|     false|     false| false|   false|false|   false|  true|       false|      false|       false|         false|       false|    false|false|false|   false|         false|  false|     false|    false|        false|  false|           -1|\n|      SamOfTheTetons|This is how it sh...|               1| 1701619273|         null|  null|     0|kbtr01m|t1_kbs64nm|    0|       PardonMyTake|    t5_3d389| false|  false|   false|         false|  false|     false|   false|      false|               false|   false|  false|  false|false| false|false|false|   false|  false| false|   false|    false|        false|   false|false|    true|    false|   false|      false|  false|         false|       false|   false|        false|     false|     false| false|   false|false|   false| false|       false|      false|       false|         false|       false|    false|false|false|   false|         false|  false|     false|    false|        false|  false|           -1|\n|            LordSahu|If you want a hig...|               0| 1701619273|         null|  null|     0|kbtr02e|t3_189ph2m|    3|       Pathfinder2e|    t5_gkc60| false|  false|   false|         false|  false|     false|   false|      false|               false|   false|  false|  false|false| false|false|false|   false|  false| false|   false|    false|        false|   false|false|   false|    false|   false|      false|  false|         false|       false|   false|        false|     false|     false| false|    true|false|   false| false|       false|      false|       false|         false|       false|    false|false|false|   false|         false|  false|     false|    false|        false|  false|            1|\n|  Acceptable-CatProf|I am not super fa...|               0| 1701619273|         null|  null|     0|kbtr03i|t3_189n826|    3|SameGrassButGreener|    t5_2u6j8| false|  false|   false|         false|   true|     false|   false|      false|               false|   false|  false|  false|false| false|false|false|   false|  false| false|   false|    false|        false|   false|false|   false|    false|   false|      false|  false|         false|       false|   false|        false|     false|     false| false|   false|false|   false| false|       false|      false|       false|         false|       false|    false|false|false|   false|         false|  false|     false|    false|        false|  false|            1|\n+--------------------+--------------------+----------------+-----------+-------------+------+------+-------+----------+-----+-------------------+------------+------+-------+--------+--------------+-------+----------+--------+-----------+--------------------+--------+-------+-------+-----+------+-----+-----+--------+-------+------+--------+---------+-------------+--------+-----+--------+---------+--------+-----------+-------+--------------+------------+--------+-------------+----------+----------+------+--------+-----+--------+------+------------+-----------+------------+--------------+------------+---------+-----+-----+--------+--------------+-------+----------+---------+-------------+-------+-------------+\nonly showing top 20 rows\n\n"
        }
      ],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": true
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733773153868
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, VectorAssembler\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "\n",
        "state_names = [\"Alaska\", \"Alabama\", \"Arkansas\", \"American Samoa\", \"Arizona\", \"California\", \n",
        "\"Colorado\", \"Connecticut\", \"District of Columbia\", \"Delaware\", \"Florida\", \"Georgia\", \"Guam\", \n",
        "\"Hawaii\", \"Iowa\", \"Idaho\", \"Illinois\", \"Indiana\", \"Kansas\", \"Kentucky\", \"Louisiana\", \n",
        "\"Massachusetts\", \"Maryland\", \"Maine\", \"Michigan\", \"Minnesota\", \"Missouri\", \"Mississippi\", \n",
        "\"Montana\", \"North Carolina\", \"North Dakota\", \"Nebraska\", \"New Hampshire\", \"New Jersey\", \n",
        "\"New Mexico\", \"Nevada\", \"New York\", \"Ohio\", \"Oklahoma\", \"Oregon\", \"Pennsylvania\", \"Puerto Rico\", \n",
        "\"Rhode Island\", \"South Carolina\", \"South Dakota\", \"Tennessee\", \"Texas\", \"Utah\", \"Virginia\", \n",
        "\"Virgin Islands\", \"Vermont\", \"Washington\", \"Wisconsin\", \"West Virginia\", \"Wyoming\"]\n",
        "state_names = [x.lower() for x in state_names]\n",
        "\n",
        "state_names.append(\"sentiment_num\")\n",
        "state_names.append(\"rawFeatures\")\n",
        "\n",
        "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
        "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=1000)\n",
        "idf = VectorAssembler(inputCols= state_names, outputCol=\"features\")\n",
        "\n",
        "lr = LinearRegression(featuresCol=\"features\", labelCol=\"score\")\n",
        "\n",
        "pipeline_linear = Pipeline(stages=[tokenizer, hashingTF, idf, lr])"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "09c8ba2d-49fa-498b-9d76-d59b947bd032",
              "statement_id": 13,
              "statement_ids": [
                13
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 0,
                  "RUNNING": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "session_id": "55",
              "normalized_state": "finished",
              "queued_time": "2024-12-09T20:16:18.0456034Z",
              "session_start_time": null,
              "execution_start_time": "2024-12-09T20:16:18.1836454Z",
              "execution_finish_time": "2024-12-09T20:16:18.5202705Z",
              "parent_msg_id": "5b6f7a3d-4268-46d7-8586-18d1e63e443a"
            },
            "text/plain": "StatementMeta(09c8ba2d-49fa-498b-9d76-d59b947bd032, 55, 13, Finished, Available, Finished)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733775342423
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(trainingData, testData) = df.randomSplit([0.7, 0.3])\n",
        "\n",
        "model_linear = pipeline_linear.fit(trainingData)\n",
        "predictions_linear = model_linear.transform(testData)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "09c8ba2d-49fa-498b-9d76-d59b947bd032",
              "statement_id": 15,
              "statement_ids": [
                15
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 2,
                  "RUNNING": 0
                },
                "jobs": [
                  {
                    "displayName": "treeAggregate at Statistics.scala:58",
                    "dataWritten": 2942,
                    "dataRead": 2942,
                    "rowCount": 170,
                    "usageDescription": "",
                    "jobId": 7,
                    "name": "treeAggregate at Statistics.scala:58",
                    "description": "Job group for statement 15:\n(trainingData, testData) = df.randomSplit([0.7, 0.3])\n\nmodel_linear = pipeline_linear.fit(trainingData)\npredictions_linear = model_linear.transform(testData)",
                    "submissionTime": "2024-12-09T20:30:19.913GMT",
                    "completionTime": "2024-12-09T20:42:45.148GMT",
                    "stageIds": [
                      12,
                      13
                    ],
                    "jobGroup": "15",
                    "jobTags": [],
                    "status": "SUCCEEDED",
                    "numTasks": 10,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 10,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 10,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "treeAggregate at WeightedLeastSquares.scala:107",
                    "dataWritten": 20582196,
                    "dataRead": 20582196,
                    "rowCount": 170,
                    "usageDescription": "",
                    "jobId": 6,
                    "name": "treeAggregate at WeightedLeastSquares.scala:107",
                    "description": "Job group for statement 15:\n(trainingData, testData) = df.randomSplit([0.7, 0.3])\n\nmodel_linear = pipeline_linear.fit(trainingData)\npredictions_linear = model_linear.transform(testData)",
                    "submissionTime": "2024-12-09T20:17:22.902GMT",
                    "completionTime": "2024-12-09T20:30:18.750GMT",
                    "stageIds": [
                      10,
                      11
                    ],
                    "jobGroup": "15",
                    "jobTags": [],
                    "status": "SUCCEEDED",
                    "numTasks": 10,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 10,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 10,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "session_id": "55",
              "normalized_state": "finished",
              "queued_time": "2024-12-09T20:17:21.6586219Z",
              "session_start_time": null,
              "execution_start_time": "2024-12-09T20:17:21.8045611Z",
              "execution_finish_time": "2024-12-09T20:42:50.1132692Z",
              "parent_msg_id": "130020c9-4d24-4e97-ba8a-5256400e7061"
            },
            "text/plain": "StatementMeta(09c8ba2d-49fa-498b-9d76-d59b947bd032, 55, 15, Finished, Available, Finished)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 10,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733776934195
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datastore = 'azureml://datastores/workspaceblobstore/paths'\n",
        "model_linear.write().overwrite().save(f\"{datastore}/models/ml_model_linear_score_prediction\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "09c8ba2d-49fa-498b-9d76-d59b947bd032",
              "statement_id": 23,
              "statement_ids": [
                23
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 7,
                  "RUNNING": 0
                },
                "jobs": [
                  {
                    "displayName": "parquet at LinearRegression.scala:790",
                    "dataWritten": 10569,
                    "dataRead": 8584,
                    "rowCount": 2,
                    "usageDescription": "",
                    "jobId": 34,
                    "name": "parquet at LinearRegression.scala:790",
                    "description": "Job group for statement 23:\ndatastore = 'azureml://datastores/workspaceblobstore/paths'\nmodel_linear.write().overwrite().save(f\"{datastore}/models/ml_model_linear_score_prediction\")",
                    "submissionTime": "2024-12-09T20:54:41.141GMT",
                    "completionTime": "2024-12-09T20:54:42.685GMT",
                    "stageIds": [
                      42,
                      43
                    ],
                    "jobGroup": "23",
                    "jobTags": [],
                    "status": "SUCCEEDED",
                    "numTasks": 2,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 1,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 1,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "parquet at LinearRegression.scala:790",
                    "dataWritten": 8584,
                    "dataRead": 0,
                    "rowCount": 1,
                    "usageDescription": "",
                    "jobId": 33,
                    "name": "parquet at LinearRegression.scala:790",
                    "description": "Job group for statement 23:\ndatastore = 'azureml://datastores/workspaceblobstore/paths'\nmodel_linear.write().overwrite().save(f\"{datastore}/models/ml_model_linear_score_prediction\")",
                    "submissionTime": "2024-12-09T20:54:40.789GMT",
                    "completionTime": "2024-12-09T20:54:40.916GMT",
                    "stageIds": [
                      41
                    ],
                    "jobGroup": "23",
                    "jobTags": [],
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "runJob at SparkHadoopWriter.scala:85",
                    "dataWritten": 521,
                    "dataRead": 0,
                    "rowCount": 1,
                    "usageDescription": "",
                    "jobId": 32,
                    "name": "runJob at SparkHadoopWriter.scala:85",
                    "description": "Job group for statement 23:\ndatastore = 'azureml://datastores/workspaceblobstore/paths'\nmodel_linear.write().overwrite().save(f\"{datastore}/models/ml_model_linear_score_prediction\")",
                    "submissionTime": "2024-12-09T20:54:39.889GMT",
                    "completionTime": "2024-12-09T20:54:40.419GMT",
                    "stageIds": [
                      40
                    ],
                    "jobGroup": "23",
                    "jobTags": [],
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "runJob at SparkHadoopWriter.scala:85",
                    "dataWritten": 982,
                    "dataRead": 0,
                    "rowCount": 1,
                    "usageDescription": "",
                    "jobId": 31,
                    "name": "runJob at SparkHadoopWriter.scala:85",
                    "description": "Job group for statement 23:\ndatastore = 'azureml://datastores/workspaceblobstore/paths'\nmodel_linear.write().overwrite().save(f\"{datastore}/models/ml_model_linear_score_prediction\")",
                    "submissionTime": "2024-12-09T20:54:38.873GMT",
                    "completionTime": "2024-12-09T20:54:39.431GMT",
                    "stageIds": [
                      39
                    ],
                    "jobGroup": "23",
                    "jobTags": [],
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "runJob at SparkHadoopWriter.scala:85",
                    "dataWritten": 323,
                    "dataRead": 0,
                    "rowCount": 1,
                    "usageDescription": "",
                    "jobId": 30,
                    "name": "runJob at SparkHadoopWriter.scala:85",
                    "description": "Job group for statement 23:\ndatastore = 'azureml://datastores/workspaceblobstore/paths'\nmodel_linear.write().overwrite().save(f\"{datastore}/models/ml_model_linear_score_prediction\")",
                    "submissionTime": "2024-12-09T20:54:37.573GMT",
                    "completionTime": "2024-12-09T20:54:38.263GMT",
                    "stageIds": [
                      38
                    ],
                    "jobGroup": "23",
                    "jobTags": [],
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "runJob at SparkHadoopWriter.scala:85",
                    "dataWritten": 261,
                    "dataRead": 0,
                    "rowCount": 1,
                    "usageDescription": "",
                    "jobId": 29,
                    "name": "runJob at SparkHadoopWriter.scala:85",
                    "description": "Job group for statement 23:\ndatastore = 'azureml://datastores/workspaceblobstore/paths'\nmodel_linear.write().overwrite().save(f\"{datastore}/models/ml_model_linear_score_prediction\")",
                    "submissionTime": "2024-12-09T20:54:36.466GMT",
                    "completionTime": "2024-12-09T20:54:37.099GMT",
                    "stageIds": [
                      37
                    ],
                    "jobGroup": "23",
                    "jobTags": [],
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "runJob at SparkHadoopWriter.scala:85",
                    "dataWritten": 306,
                    "dataRead": 0,
                    "rowCount": 1,
                    "usageDescription": "",
                    "jobId": 28,
                    "name": "runJob at SparkHadoopWriter.scala:85",
                    "description": "Job group for statement 23:\ndatastore = 'azureml://datastores/workspaceblobstore/paths'\nmodel_linear.write().overwrite().save(f\"{datastore}/models/ml_model_linear_score_prediction\")",
                    "submissionTime": "2024-12-09T20:54:35.145GMT",
                    "completionTime": "2024-12-09T20:54:35.904GMT",
                    "stageIds": [
                      36
                    ],
                    "jobGroup": "23",
                    "jobTags": [],
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "session_id": "55",
              "normalized_state": "finished",
              "queued_time": "2024-12-09T20:54:34.4029418Z",
              "session_start_time": null,
              "execution_start_time": "2024-12-09T20:54:34.5169234Z",
              "execution_finish_time": "2024-12-09T20:54:43.8023342Z",
              "parent_msg_id": "3f5ecc20-462a-4efb-8f0d-4f963b1411a5"
            },
            "text/plain": "StatementMeta(09c8ba2d-49fa-498b-9d76-d59b947bd032, 55, 23, Finished, Available, Finished)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 18,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733777647703
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml import PipelineModel\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        "PipelineModel.load(f\"{datastore}/models/ml_model_score_prediction\") \n",
        "evaluator = RegressionEvaluator(labelCol=\"score\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
        "rmse = evaluator.evaluate(predictions_linear)\n",
        "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "09c8ba2d-49fa-498b-9d76-d59b947bd032",
              "statement_id": 17,
              "statement_ids": [
                17
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 13,
                  "RUNNING": 0
                },
                "jobs": [
                  {
                    "displayName": "treeAggregate at Statistics.scala:58",
                    "dataWritten": 2942,
                    "dataRead": 2942,
                    "rowCount": 170,
                    "usageDescription": "",
                    "jobId": 27,
                    "name": "treeAggregate at Statistics.scala:58",
                    "description": "Job group for statement 17:\nfrom pyspark.ml import PipelineModel\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\nPipelineModel.load(f\"{datastore}/models/ml_model_score_prediction\") \nevaluator = RegressionEvaluator(labelCol=\"score\", predictionCol=\"prediction\", metricName=\"rmse\")\nrmse = evaluator.evaluate(predictions_linear)\nprint(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)",
                    "submissionTime": "2024-12-09T20:43:13.456GMT",
                    "completionTime": "2024-12-09T20:50:02.031GMT",
                    "stageIds": [
                      34,
                      35
                    ],
                    "jobGroup": "17",
                    "jobTags": [],
                    "status": "SUCCEEDED",
                    "numTasks": 10,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 10,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 10,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "head at LinearRegression.scala:845",
                    "dataWritten": 0,
                    "dataRead": 13951,
                    "rowCount": 1,
                    "usageDescription": "",
                    "jobId": 26,
                    "name": "head at LinearRegression.scala:845",
                    "description": "Job group for statement 17:\nfrom pyspark.ml import PipelineModel\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\nPipelineModel.load(f\"{datastore}/models/ml_model_score_prediction\") \nevaluator = RegressionEvaluator(labelCol=\"score\", predictionCol=\"prediction\", metricName=\"rmse\")\nrmse = evaluator.evaluate(predictions_linear)\nprint(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)",
                    "submissionTime": "2024-12-09T20:43:12.504GMT",
                    "completionTime": "2024-12-09T20:43:13.075GMT",
                    "stageIds": [
                      33
                    ],
                    "jobGroup": "17",
                    "jobTags": [],
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "load at LinearRegression.scala:833",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 25,
                    "name": "load at LinearRegression.scala:833",
                    "description": "Job group for statement 17:\nfrom pyspark.ml import PipelineModel\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\nPipelineModel.load(f\"{datastore}/models/ml_model_score_prediction\") \nevaluator = RegressionEvaluator(labelCol=\"score\", predictionCol=\"prediction\", metricName=\"rmse\")\nrmse = evaluator.evaluate(predictions_linear)\nprint(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)",
                    "submissionTime": "2024-12-09T20:43:11.126GMT",
                    "completionTime": "2024-12-09T20:43:12.211GMT",
                    "stageIds": [
                      32
                    ],
                    "jobGroup": "17",
                    "jobTags": [],
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "first at ReadWrite.scala:587",
                    "dataWritten": 0,
                    "dataRead": 521,
                    "rowCount": 1,
                    "usageDescription": "",
                    "jobId": 24,
                    "name": "first at ReadWrite.scala:587",
                    "description": "Job group for statement 17:\nfrom pyspark.ml import PipelineModel\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\nPipelineModel.load(f\"{datastore}/models/ml_model_score_prediction\") \nevaluator = RegressionEvaluator(labelCol=\"score\", predictionCol=\"prediction\", metricName=\"rmse\")\nrmse = evaluator.evaluate(predictions_linear)\nprint(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)",
                    "submissionTime": "2024-12-09T20:43:10.676GMT",
                    "completionTime": "2024-12-09T20:43:10.755GMT",
                    "stageIds": [
                      31
                    ],
                    "jobGroup": "17",
                    "jobTags": [],
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "first at ReadWrite.scala:587",
                    "dataWritten": 0,
                    "dataRead": 521,
                    "rowCount": 1,
                    "usageDescription": "",
                    "jobId": 23,
                    "name": "first at ReadWrite.scala:587",
                    "description": "Job group for statement 17:\nfrom pyspark.ml import PipelineModel\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\nPipelineModel.load(f\"{datastore}/models/ml_model_score_prediction\") \nevaluator = RegressionEvaluator(labelCol=\"score\", predictionCol=\"prediction\", metricName=\"rmse\")\nrmse = evaluator.evaluate(predictions_linear)\nprint(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)",
                    "submissionTime": "2024-12-09T20:43:10.506GMT",
                    "completionTime": "2024-12-09T20:43:10.589GMT",
                    "stageIds": [
                      30
                    ],
                    "jobGroup": "17",
                    "jobTags": [],
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "first at ReadWrite.scala:587",
                    "dataWritten": 0,
                    "dataRead": 982,
                    "rowCount": 1,
                    "usageDescription": "",
                    "jobId": 22,
                    "name": "first at ReadWrite.scala:587",
                    "description": "Job group for statement 17:\nfrom pyspark.ml import PipelineModel\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\nPipelineModel.load(f\"{datastore}/models/ml_model_score_prediction\") \nevaluator = RegressionEvaluator(labelCol=\"score\", predictionCol=\"prediction\", metricName=\"rmse\")\nrmse = evaluator.evaluate(predictions_linear)\nprint(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)",
                    "submissionTime": "2024-12-09T20:43:10.286GMT",
                    "completionTime": "2024-12-09T20:43:10.390GMT",
                    "stageIds": [
                      29
                    ],
                    "jobGroup": "17",
                    "jobTags": [],
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "first at ReadWrite.scala:587",
                    "dataWritten": 0,
                    "dataRead": 982,
                    "rowCount": 1,
                    "usageDescription": "",
                    "jobId": 21,
                    "name": "first at ReadWrite.scala:587",
                    "description": "Job group for statement 17:\nfrom pyspark.ml import PipelineModel\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\nPipelineModel.load(f\"{datastore}/models/ml_model_score_prediction\") \nevaluator = RegressionEvaluator(labelCol=\"score\", predictionCol=\"prediction\", metricName=\"rmse\")\nrmse = evaluator.evaluate(predictions_linear)\nprint(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)",
                    "submissionTime": "2024-12-09T20:43:10.080GMT",
                    "completionTime": "2024-12-09T20:43:10.166GMT",
                    "stageIds": [
                      28
                    ],
                    "jobGroup": "17",
                    "jobTags": [],
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "first at ReadWrite.scala:587",
                    "dataWritten": 0,
                    "dataRead": 323,
                    "rowCount": 1,
                    "usageDescription": "",
                    "jobId": 20,
                    "name": "first at ReadWrite.scala:587",
                    "description": "Job group for statement 17:\nfrom pyspark.ml import PipelineModel\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\nPipelineModel.load(f\"{datastore}/models/ml_model_score_prediction\") \nevaluator = RegressionEvaluator(labelCol=\"score\", predictionCol=\"prediction\", metricName=\"rmse\")\nrmse = evaluator.evaluate(predictions_linear)\nprint(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)",
                    "submissionTime": "2024-12-09T20:43:09.903GMT",
                    "completionTime": "2024-12-09T20:43:09.995GMT",
                    "stageIds": [
                      27
                    ],
                    "jobGroup": "17",
                    "jobTags": [],
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "first at ReadWrite.scala:587",
                    "dataWritten": 0,
                    "dataRead": 323,
                    "rowCount": 1,
                    "usageDescription": "",
                    "jobId": 19,
                    "name": "first at ReadWrite.scala:587",
                    "description": "Job group for statement 17:\nfrom pyspark.ml import PipelineModel\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\nPipelineModel.load(f\"{datastore}/models/ml_model_score_prediction\") \nevaluator = RegressionEvaluator(labelCol=\"score\", predictionCol=\"prediction\", metricName=\"rmse\")\nrmse = evaluator.evaluate(predictions_linear)\nprint(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)",
                    "submissionTime": "2024-12-09T20:43:09.711GMT",
                    "completionTime": "2024-12-09T20:43:09.788GMT",
                    "stageIds": [
                      26
                    ],
                    "jobGroup": "17",
                    "jobTags": [],
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "first at ReadWrite.scala:587",
                    "dataWritten": 0,
                    "dataRead": 261,
                    "rowCount": 1,
                    "usageDescription": "",
                    "jobId": 18,
                    "name": "first at ReadWrite.scala:587",
                    "description": "Job group for statement 17:\nfrom pyspark.ml import PipelineModel\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\nPipelineModel.load(f\"{datastore}/models/ml_model_score_prediction\") \nevaluator = RegressionEvaluator(labelCol=\"score\", predictionCol=\"prediction\", metricName=\"rmse\")\nrmse = evaluator.evaluate(predictions_linear)\nprint(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)",
                    "submissionTime": "2024-12-09T20:43:09.514GMT",
                    "completionTime": "2024-12-09T20:43:09.612GMT",
                    "stageIds": [
                      25
                    ],
                    "jobGroup": "17",
                    "jobTags": [],
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "first at ReadWrite.scala:587",
                    "dataWritten": 0,
                    "dataRead": 261,
                    "rowCount": 1,
                    "usageDescription": "",
                    "jobId": 17,
                    "name": "first at ReadWrite.scala:587",
                    "description": "Job group for statement 17:\nfrom pyspark.ml import PipelineModel\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\nPipelineModel.load(f\"{datastore}/models/ml_model_score_prediction\") \nevaluator = RegressionEvaluator(labelCol=\"score\", predictionCol=\"prediction\", metricName=\"rmse\")\nrmse = evaluator.evaluate(predictions_linear)\nprint(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)",
                    "submissionTime": "2024-12-09T20:43:09.262GMT",
                    "completionTime": "2024-12-09T20:43:09.389GMT",
                    "stageIds": [
                      24
                    ],
                    "jobGroup": "17",
                    "jobTags": [],
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "first at ReadWrite.scala:587",
                    "dataWritten": 0,
                    "dataRead": 306,
                    "rowCount": 1,
                    "usageDescription": "",
                    "jobId": 16,
                    "name": "first at ReadWrite.scala:587",
                    "description": "Job group for statement 17:\nfrom pyspark.ml import PipelineModel\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\nPipelineModel.load(f\"{datastore}/models/ml_model_score_prediction\") \nevaluator = RegressionEvaluator(labelCol=\"score\", predictionCol=\"prediction\", metricName=\"rmse\")\nrmse = evaluator.evaluate(predictions_linear)\nprint(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)",
                    "submissionTime": "2024-12-09T20:43:08.894GMT",
                    "completionTime": "2024-12-09T20:43:09.156GMT",
                    "stageIds": [
                      23
                    ],
                    "jobGroup": "17",
                    "jobTags": [],
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "runJob at PythonRDD.scala:185",
                    "dataWritten": 0,
                    "dataRead": 306,
                    "rowCount": 1,
                    "usageDescription": "",
                    "jobId": 15,
                    "name": "runJob at PythonRDD.scala:185",
                    "description": "Job group for statement 17:\nfrom pyspark.ml import PipelineModel\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\nPipelineModel.load(f\"{datastore}/models/ml_model_score_prediction\") \nevaluator = RegressionEvaluator(labelCol=\"score\", predictionCol=\"prediction\", metricName=\"rmse\")\nrmse = evaluator.evaluate(predictions_linear)\nprint(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)",
                    "submissionTime": "2024-12-09T20:43:07.166GMT",
                    "completionTime": "2024-12-09T20:43:08.760GMT",
                    "stageIds": [
                      22
                    ],
                    "jobGroup": "17",
                    "jobTags": [],
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "session_id": "55",
              "normalized_state": "finished",
              "queued_time": "2024-12-09T20:41:23.9420398Z",
              "session_start_time": null,
              "execution_start_time": "2024-12-09T20:43:06.9317794Z",
              "execution_finish_time": "2024-12-09T20:50:04.9443957Z",
              "parent_msg_id": "674ccf99-2eb4-4b59-bdd7-23c7bc870af9"
            },
            "text/plain": "StatementMeta(09c8ba2d-49fa-498b-9d76-d59b947bd032, 55, 17, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Root Mean Squared Error (RMSE) on test data = 68.5895\n"
        }
      ],
      "execution_count": 12,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733777368905
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.ml import PipelineModel\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, VectorAssembler\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "from pyspark.ml.regression import RandomForestRegressor"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "09c8ba2d-49fa-498b-9d76-d59b947bd032",
              "statement_id": 25,
              "statement_ids": [
                25
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 0,
                  "RUNNING": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "session_id": "55",
              "normalized_state": "finished",
              "queued_time": "2024-12-09T20:57:45.4657625Z",
              "session_start_time": null,
              "execution_start_time": "2024-12-09T20:57:45.6434557Z",
              "execution_finish_time": "2024-12-09T20:57:45.9648314Z",
              "parent_msg_id": "7a094486-9e1e-4a26-b29e-55f5945759e8"
            },
            "text/plain": "StatementMeta(09c8ba2d-49fa-498b-9d76-d59b947bd032, 55, 25, Finished, Available, Finished)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 20,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733777830193
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
        "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=1000)\n",
        "idf = VectorAssembler(inputCols= state_names, outputCol=\"features\")\n",
        "\n",
        "rf = RandomForestRegressor(featuresCol=\"indexedFeatures\", labelCol = \"score\")\n",
        "\n",
        "pipeline_forest = Pipeline(stages=[tokenizer, hashingTF, idf, rf])"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "09c8ba2d-49fa-498b-9d76-d59b947bd032",
              "statement_id": 26,
              "statement_ids": [
                26
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 0,
                  "RUNNING": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "session_id": "55",
              "normalized_state": "finished",
              "queued_time": "2024-12-09T20:57:47.4259239Z",
              "session_start_time": null,
              "execution_start_time": "2024-12-09T20:57:47.5426967Z",
              "execution_finish_time": "2024-12-09T20:57:47.8399094Z",
              "parent_msg_id": "f69ec5eb-2700-4649-b87e-4d9c3914a96c"
            },
            "text/plain": "StatementMeta(09c8ba2d-49fa-498b-9d76-d59b947bd032, 55, 26, Finished, Available, Finished)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 21,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733777831857
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(trainingData, testData) = df.randomSplit([0.7, 0.3])\n",
        "\n",
        "model_forest = pipeline_forest.fit(trainingData)\n",
        "predictions_forest = model_forest.transform(testData)\n",
        "\n",
        "datastore = 'azureml://datastores/workspaceblobstore/paths'\n",
        "model_forest.write().overwrite().save(f\"{datastore}/models/ml_model_forest_score_prediction\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "09c8ba2d-49fa-498b-9d76-d59b947bd032",
              "statement_id": 27,
              "statement_ids": [
                27
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 0,
                  "RUNNING": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "session_id": "55",
              "normalized_state": "finished",
              "queued_time": "2024-12-09T20:57:50.8846077Z",
              "session_start_time": null,
              "execution_start_time": "2024-12-09T20:57:50.9976024Z",
              "execution_finish_time": "2024-12-09T20:57:51.9059287Z",
              "parent_msg_id": "d712b50d-7448-4ca3-b842-5fc159be238b"
            },
            "text/plain": "StatementMeta(09c8ba2d-49fa-498b-9d76-d59b947bd032, 55, 27, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "IllegalArgumentException",
          "evalue": "indexedFeatures does not exist. Available: author, text, controversiality, created_utc, distinguished, edited, gilded, id, parent_id, score, subreddit, subreddit_id, alaska, alabama, arkansas, american samoa, arizona, california, colorado, connecticut, district of columbia, delaware, florida, georgia, guam, hawaii, iowa, idaho, illinois, indiana, kansas, kentucky, louisiana, massachusetts, maryland, maine, michigan, minnesota, missouri, mississippi, montana, north carolina, north dakota, nebraska, new hampshire, new jersey, new mexico, nevada, new york, ohio, oklahoma, oregon, pennsylvania, puerto rico, rhode island, south carolina, south dakota, tennessee, texas, utah, virginia, virgin islands, vermont, washington, wisconsin, west virginia, wyoming, sentiment_num, words, rawFeatures, features",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[53], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m (trainingData, testData) \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mrandomSplit([\u001b[38;5;241m0.7\u001b[39m, \u001b[38;5;241m0.3\u001b[39m])\n\u001b[0;32m----> 3\u001b[0m model_forest \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline_forest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainingData\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m predictions_forest \u001b[38;5;241m=\u001b[39m model_forest\u001b[38;5;241m.\u001b[39mtransform(testData)\n\u001b[1;32m      6\u001b[0m datastore \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mazureml://datastores/workspaceblobstore/paths\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
            "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
            "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/ml/pipeline.py:134\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    132\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m stage\u001b[38;5;241m.\u001b[39mtransform(dataset)\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# must be an Estimator\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mstage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m     transformers\u001b[38;5;241m.\u001b[39mappend(model)\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m indexOfLastEstimator:\n",
            "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
            "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/ml/wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[0;32m--> 381\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    382\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
            "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/ml/wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/cluster-env/env/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
            "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    171\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
            "\u001b[0;31mIllegalArgumentException\u001b[0m: indexedFeatures does not exist. Available: author, text, controversiality, created_utc, distinguished, edited, gilded, id, parent_id, score, subreddit, subreddit_id, alaska, alabama, arkansas, american samoa, arizona, california, colorado, connecticut, district of columbia, delaware, florida, georgia, guam, hawaii, iowa, idaho, illinois, indiana, kansas, kentucky, louisiana, massachusetts, maryland, maine, michigan, minnesota, missouri, mississippi, montana, north carolina, north dakota, nebraska, new hampshire, new jersey, new mexico, nevada, new york, ohio, oklahoma, oregon, pennsylvania, puerto rico, rhode island, south carolina, south dakota, tennessee, texas, utah, virginia, virgin islands, vermont, washington, wisconsin, west virginia, wyoming, sentiment_num, words, rawFeatures, features"
          ]
        }
      ],
      "execution_count": 22,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733777835843
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#PipelineModel.load(f\"{datastore}/models/ml_model_score_prediction\") \n",
        "evaluator = RegressionEvaluator(labelCol=\"score\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
        "rmse = evaluator.evaluate(predictions_forest)\n",
        "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0",
      "mimetype": "text/x-python",
      "file_extension": ".py",
      "pygments_lexer": "ipython",
      "codemirror_mode": "ipython",
      "nbconvert_exporter": "python"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}