{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "spark"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "09c8ba2d-49fa-498b-9d76-d59b947bd032",
              "statement_id": 6,
              "statement_ids": [
                6
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 0,
                  "RUNNING": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "session_id": "55",
              "normalized_state": "finished",
              "queued_time": "2024-12-09T19:32:05.5065523Z",
              "session_start_time": "2024-12-09T19:32:05.6212861Z",
              "execution_start_time": "2024-12-09T19:33:57.0122374Z",
              "execution_finish_time": "2024-12-09T19:33:57.4170234Z",
              "parent_msg_id": "fa23cd55-2ca3-48d2-8d11-71380650fc4f"
            },
            "text/plain": "StatementMeta(09c8ba2d-49fa-498b-9d76-d59b947bd032, 55, 6, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 11,
          "data": {
            "text/plain": "<pyspark.sql.session.SparkSession at 0x728ef580b9d0>",
            "text/html": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://vm-f6932184:39043\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.4.3.5.3.20241016.1</code></dd>\n              <dt>Master</dt>\n                <dd><code>yarn</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Azure ML Experiment</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "
          },
          "metadata": {}
        }
      ],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1733772801361
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Workspace, Dataset, Datastore\n",
        "\n",
        "subscription_id = '21ff0fc0-dd2c-450d-93b7-96eeb3699b22'\n",
        "resource_group = 'project-group-29'\n",
        "workspace_name = 'project-group-29'\n",
        "\n",
        "workspace = Workspace(subscription_id, resource_group, workspace_name)\n",
        "\n",
        "datastore = Datastore.get(workspace, \"workspaceblobstore\")\n",
        "dataset = Dataset.Tabular.from_parquet_files(path=(datastore, 'states/states_sentiment'))\n",
        "df = dataset.to_spark_dataframe()\n",
        "df.head()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "09c8ba2d-49fa-498b-9d76-d59b947bd032",
              "statement_id": 7,
              "statement_ids": [
                7
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 1,
                  "RUNNING": 0
                },
                "jobs": [
                  {
                    "displayName": "head at /tmp/ipykernel_26019/2394195642.py:12",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 19,
                    "usageDescription": "",
                    "jobId": 0,
                    "name": "head at /tmp/ipykernel_26019/2394195642.py:12",
                    "description": "Job group for statement 7:\nfrom azureml.core import Workspace, Dataset, Datastore\n\nsubscription_id = '21ff0fc0-dd2c-450d-93b7-96eeb3699b22'\nresource_group = 'project-group-29'\nworkspace_name = 'project-group-29'\n\nworkspace = Workspace(subscription_id, resource_group, workspace_name)\n\ndatastore = Datastore.get(workspace, \"workspaceblobstore\")\ndataset = Dataset.Tabular.from_parquet_files(path=(datastore, 'states/states_sentiment'))\ndf = dataset.to_spark_dataframe()\ndf.head()",
                    "submissionTime": "2024-12-09T19:38:03.301GMT",
                    "completionTime": "2024-12-09T19:39:28.335GMT",
                    "stageIds": [
                      0
                    ],
                    "jobGroup": "7",
                    "jobTags": [],
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "session_id": "55",
              "normalized_state": "finished",
              "queued_time": "2024-12-09T19:32:05.5077527Z",
              "session_start_time": null,
              "execution_start_time": "2024-12-09T19:33:57.5944994Z",
              "execution_finish_time": "2024-12-09T19:39:30.7885572Z",
              "parent_msg_id": "dddfa8ab-35b2-4d53-a949-5310f3023401"
            },
            "text/plain": "StatementMeta(09c8ba2d-49fa-498b-9d76-d59b947bd032, 55, 7, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Performing interactive authentication. Please follow the instructions on the terminal.\nTo sign in, use a web browser to open the page https://microsoft.com/devicelogin and enter the code AUDVU2D92 to authenticate.\nInteractive authentication successfully completed.\n{'infer_column_types': 'False', 'activity': 'to_spark_dataframe'}\n{'infer_column_types': 'False', 'activity': 'to_spark_dataframe', 'activityApp': 'TabularDataset'}\n{'infer_column_types': 'False', 'activity': 'to_spark_dataframe', 'activityApp': 'TabularDataset', 'runId': '23137735-185a-436e-b332-97b921c8fadf'}\n{'infer_column_types': 'False', 'activity': 'to_spark_dataframe', 'activityApp': 'TabularDataset', 'runId': '23137735-185a-436e-b332-97b921c8fadf', 'run_id': '23137735-185a-436e-b332-97b921c8fadf'}\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 13,
          "data": {
            "text/plain": "Row(author='fireweedflowers', text='I live in Colorado these days but I was born and raised in Wyoming and was emphatically a socialist.', controversiality=0, created_utc=1701619261, distinguished=None, edited=None, gilded=0, id='kbtqz1h', parent_id='t3_187jmr5', score=1, subreddit='wyoming', subreddit_id='t5_2r53d', alaska=False, alabama=False, arkansas=False, american samoa=False, arizona=False, california=False, colorado=True, connecticut=False, district of columbia=False, delaware=False, florida=False, georgia=False, guam=False, hawaii=False, iowa=False, idaho=False, illinois=False, indiana=False, kansas=False, kentucky=False, louisiana=False, massachusetts=False, maryland=False, maine=False, michigan=False, minnesota=False, missouri=False, mississippi=False, montana=False, north carolina=False, north dakota=False, nebraska=False, new hampshire=False, new jersey=False, new mexico=False, nevada=False, new york=False, ohio=False, oklahoma=False, oregon=False, pennsylvania=False, puerto rico=False, rhode island=False, south carolina=False, south dakota=False, tennessee=False, texas=False, utah=False, virginia=False, virgin islands=False, vermont=False, washington=False, wisconsin=False, west virginia=False, wyoming=True, sentiment_num=1)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733773134711
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#df.printSchema()\n",
        "df.show()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "09c8ba2d-49fa-498b-9d76-d59b947bd032",
              "statement_id": 8,
              "statement_ids": [
                8
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 1,
                  "RUNNING": 0
                },
                "jobs": [
                  {
                    "displayName": "showString at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 19,
                    "usageDescription": "",
                    "jobId": 1,
                    "name": "showString at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 8:\n#df.printSchema()\ndf.show()",
                    "submissionTime": "2024-12-09T19:39:31.841GMT",
                    "completionTime": "2024-12-09T19:39:47.182GMT",
                    "stageIds": [
                      1
                    ],
                    "jobGroup": "8",
                    "jobTags": [],
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "session_id": "55",
              "normalized_state": "finished",
              "queued_time": "2024-12-09T19:32:05.5088928Z",
              "session_start_time": null,
              "execution_start_time": "2024-12-09T19:39:30.9520027Z",
              "execution_finish_time": "2024-12-09T19:39:49.7881596Z",
              "parent_msg_id": "4ea49da5-12a5-4901-9a67-f489d9acb217"
            },
            "text/plain": "StatementMeta(09c8ba2d-49fa-498b-9d76-d59b947bd032, 55, 8, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "+--------------------+--------------------+----------------+-----------+-------------+------+------+-------+----------+-----+-------------------+------------+------+-------+--------+--------------+-------+----------+--------+-----------+--------------------+--------+-------+-------+-----+------+-----+-----+--------+-------+------+--------+---------+-------------+--------+-----+--------+---------+--------+-----------+-------+--------------+------------+--------+-------------+----------+----------+------+--------+-----+--------+------+------------+-----------+------------+--------------+------------+---------+-----+-----+--------+--------------+-------+----------+---------+-------------+-------+-------------+\n|              author|                text|controversiality|created_utc|distinguished|edited|gilded|     id| parent_id|score|          subreddit|subreddit_id|alaska|alabama|arkansas|american samoa|arizona|california|colorado|connecticut|district of columbia|delaware|florida|georgia| guam|hawaii| iowa|idaho|illinois|indiana|kansas|kentucky|louisiana|massachusetts|maryland|maine|michigan|minnesota|missouri|mississippi|montana|north carolina|north dakota|nebraska|new hampshire|new jersey|new mexico|nevada|new york| ohio|oklahoma|oregon|pennsylvania|puerto rico|rhode island|south carolina|south dakota|tennessee|texas| utah|virginia|virgin islands|vermont|washington|wisconsin|west virginia|wyoming|sentiment_num|\n+--------------------+--------------------+----------------+-----------+-------------+------+------+-------+----------+-----+-------------------+------------+------+-------+--------+--------------+-------+----------+--------+-----------+--------------------+--------+-------+-------+-----+------+-----+-----+--------+-------+------+--------+---------+-------------+--------+-----+--------+---------+--------+-----------+-------+--------------+------------+--------+-------------+----------+----------+------+--------+-----+--------+------+------------+-----------+------------+--------------+------------+---------+-----+-----+--------+--------------+-------+----------+---------+-------------+-------+-------------+\n|     fireweedflowers|I live in Colorad...|               0| 1701619261|         null|  null|     0|kbtqz1h|t3_187jmr5|    1|            wyoming|    t5_2r53d| false|  false|   false|         false|  false|     false|    true|      false|               false|   false|  false|  false|false| false|false|false|   false|  false| false|   false|    false|        false|   false|false|   false|    false|   false|      false|  false|         false|       false|   false|        false|     false|     false| false|   false|false|   false| false|       false|      false|       false|         false|       false|    false|false|false|   false|         false|  false|     false|    false|        false|   true|            1|\n|               IHB31|If Iowa was 12-1 ...|               0| 1701619262|         null|  null|     0|kbtqz3t|t1_kbtqbur|    2|                CFB|    t5_2qm9d| false|  false|   false|         false|  false|     false|   false|      false|               false|   false|  false|  false|false| false| true|false|   false|  false| false|   false|    false|        false|   false|false|    true|    false|   false|      false|  false|         false|       false|   false|        false|     false|     false| false|   false|false|   false| false|       false|      false|       false|         false|       false|    false|false|false|   false|         false|  false|     false|    false|        false|  false|            1|\n|      Moonlight12350|On FD participant...|               0| 1701619262|         null|  null|     0|kbtqz49|t3_189lmgf|    1|         sportsbook|    t5_2s3v4| false|   true|   false|         false|  false|     false|   false|      false|               false|   false|  false|  false|false| false|false|false|   false|  false| false|   false|    false|        false|   false|false|    true|    false|   false|      false|  false|         false|       false|   false|        false|     false|     false| false|   false|false|   false| false|       false|      false|       false|         false|       false|    false| true|false|   false|         false|  false|      true|    false|        false|  false|            1|\n|    fuzzypetiolesguy|Wrecked by who? U...|               0| 1701619263|         null|  null|     0|kbtqz6w|t1_kbrzgkq|    1|                CFB|    t5_2qm9d| false|  false|   false|         false|  false|     false|   false|      false|               false|   false|  false|  false|false| false|false|false|   false|  false| false|   false|    false|        false|   false|false|   false|    false|   false|      false|  false|         false|       false|   false|        false|     false|     false| false|   false|false|   false| false|       false|      false|       false|         false|       false|    false| true|false|   false|         false|  false|     false|    false|        false|  false|           -1|\n|      Suck_Me_Dry666|I think you naile...|               0| 1701619264|         null|  null|     0|kbtqz90|t3_189k238|   -1|          geography|    t5_2qnms| false|  false|   false|         false|  false|     false|   false|      false|               false|   false|  false|  false|false| false|false|false|   false|   true| false|   false|    false|        false|   false|false|   false|    false|   false|      false|  false|         false|       false|   false|        false|     false|     false| false|   false|false|   false| false|       false|      false|       false|         false|       false|    false|false|false|   false|         false|  false|     false|    false|        false|  false|            1|\n|             crc2993|Man, I’d honestly...|               0| 1701619264|         null|  null|     0|kbtqzal|t1_kbtqb3v|    1|                CFB|    t5_2qm9d| false|  false|   false|         false|  false|     false|   false|      false|               false|   false|  false|  false|false| false|false|false|   false|  false| false|   false|    false|        false|   false|false|   false|    false|   false|      false|  false|         false|       false|   false|        false|     false|     false| false|   false|false|   false| false|       false|      false|       false|         false|       false|    false| true|false|   false|         false|  false|     false|    false|        false|  false|            0|\n|         BurstAgentX|The latter, and i...|               0| 1701619265|         null|  null|     0|kbtqzdm|t1_kbtj7lx|    1|KitchenConfidential|    t5_2sa8b| false|  false|   false|         false|  false|     false|   false|      false|               false|   false|  false|  false|false| false|false|false|   false|  false| false|   false|    false|        false|   false|false|   false|    false|   false|      false|  false|         false|       false|   false|        false|     false|     false| false|   false|false|   false| false|        true|      false|       false|         false|       false|    false|false|false|   false|         false|  false|     false|    false|        false|  false|           -1|\n|       onesneakymofo|You're one of the...|               0| 1701619266|         null|  null|     0|kbtqze2|t1_kbtq2e4|    1|                CFB|    t5_2qm9d| false|  false|   false|         false|  false|     false|   false|      false|               false|   false|  false|  false|false| false|false|false|   false|  false| false|   false|    false|        false|   false|false|   false|    false|   false|      false|  false|         false|       false|   false|        false|     false|     false| false|   false|false|   false| false|       false|      false|       false|         false|       false|    false| true|false|   false|         false|  false|     false|    false|        false|  false|            0|\n|Justacouplemoreholes|Don't lose to Tex...|               0| 1701619268|         null|  null|     0|kbtqzmg|t1_kbt963r|  -11|           rolltide|    t5_2sanc| false|  false|   false|         false|  false|     false|   false|      false|               false|   false|  false|  false|false| false|false|false|   false|  false| false|   false|    false|        false|   false|false|   false|    false|   false|      false|  false|         false|       false|   false|        false|     false|     false| false|   false|false|   false| false|       false|      false|       false|         false|       false|    false| true|false|   false|         false|  false|     false|    false|        false|  false|            1|\n|         poonjouster|Cardale and the B...|               0| 1701619268|         null|  null|     0|kbtqznf|t1_kbt3xq6|    0|                CFB|    t5_2qm9d| false|  false|   false|         false|  false|     false|   false|      false|               false|   false|  false|  false|false| false|false|false|   false|  false| false|    true|    false|        false|   false|false|   false|    false|   false|      false|  false|         false|       false|   false|        false|     false|     false| false|   false|false|   false| false|       false|      false|       false|         false|       false|    false|false|false|   false|         false|  false|     false|     true|        false|  false|           -1|\n|     SpartansStandUp|No, Texas is a on...|               0| 1701619269|         null|  null|     0|kbtqzo1|t3_189tmhm|   -2|                CFB|    t5_2qm9d| false|   true|   false|         false|  false|     false|   false|      false|               false|   false|  false|   true|false| false|false|false|   false|  false| false|   false|    false|        false|   false|false|   false|    false|   false|      false|  false|         false|       false|   false|        false|     false|     false| false|   false|false|    true| false|       false|      false|       false|         false|       false|    false| true|false|   false|         false|  false|     false|    false|        false|  false|            1|\n|         pericles123|it comes down to ...|               0| 1701619269|         null|  null|     0|kbtqzog|t3_189vp4v|    2|                CFB|    t5_2qm9d| false|   true|   false|         false|  false|     false|   false|      false|               false|   false|  false|   true|false| false|false|false|   false|  false| false|   false|    false|        false|   false|false|   false|    false|   false|      false|  false|         false|       false|   false|        false|     false|     false| false|   false| true|   false|  true|       false|      false|       false|         false|       false|    false|false|false|   false|         false|  false|     false|    false|        false|  false|            1|\n|              _cdlc_|I was in complete...|               0| 1701619269|         null|  null|     0|kbtqzq4|t3_189ohq3|   27|       greysanatomy|    t5_2t2vo| false|  false|   false|         false|   true|     false|   false|      false|               false|   false|  false|  false|false| false|false|false|   false|  false| false|   false|    false|        false|   false|false|   false|    false|   false|      false|  false|         false|       false|   false|        false|     false|     false| false|   false|false|   false| false|       false|      false|       false|         false|       false|    false|false|false|   false|         false|  false|     false|    false|        false|  false|           -1|\n|     toomuchfrosting|Breaking: NCAA ha...|               0| 1701619270|         null|  null|     0|kbtqzrq|t3_189wh00|    5|                CFB|    t5_2qm9d| false|  false|   false|         false|  false|     false|   false|      false|               false|   false|  false|  false|false| false|false|false|   false|  false| false|   false|    false|        false|   false|false|    true|    false|   false|      false|  false|         false|       false|   false|        false|     false|     false| false|   false|false|   false| false|       false|      false|       false|         false|       false|    false|false|false|   false|         false|  false|     false|    false|        false|  false|           -1|\n|         Smokey19mom|If you like the J...|               0| 1701619270|         null|  null|     0|kbtqztk|t3_189w9we|    2|            bourbon|    t5_2rgos| false|  false|   false|         false|  false|     false|   false|      false|               false|   false|  false|  false|false| false|false|false|   false|  false| false|    true|    false|        false|   false|false|   false|    false|   false|      false|  false|         false|       false|   false|        false|     false|     false| false|   false|false|   false| false|       false|      false|       false|         false|       false|    false|false|false|   false|         false|  false|     false|    false|        false|  false|            1|\n|          Langland88|I guess that migh...|               0| 1701619271|         null|  null|     0|kbtqzvu|t3_189i18m|    2|          wisconsin|    t5_2qrc2| false|  false|   false|         false|  false|     false|   false|      false|               false|   false|  false|  false|false| false|false|false|   false|  false| false|   false|    false|        false|   false|false|   false|    false|   false|      false|  false|         false|       false|   false|        false|     false|     false| false|   false|false|   false| false|       false|      false|       false|         false|       false|    false|false|false|   false|         false|  false|     false|     true|        false|  false|            1|\n|     flaminhotcheeto|They just got luc...|               0| 1701619271|         null|  null|     0|kbtqzwj|t1_kbrpgw1|    0|                CFB|    t5_2qm9d| false|  false|   false|         false|  false|     false|   false|      false|               false|   false|  false|  false|false| false|false|false|   false|  false| false|   false|    false|        false|   false|false|   false|    false|   false|      false|  false|         false|       false|   false|        false|     false|     false| false|   false|false|   false|  true|       false|      false|       false|         false|       false|    false|false|false|   false|         false|  false|     false|    false|        false|  false|           -1|\n|      SamOfTheTetons|This is how it sh...|               1| 1701619273|         null|  null|     0|kbtr01m|t1_kbs64nm|    0|       PardonMyTake|    t5_3d389| false|  false|   false|         false|  false|     false|   false|      false|               false|   false|  false|  false|false| false|false|false|   false|  false| false|   false|    false|        false|   false|false|    true|    false|   false|      false|  false|         false|       false|   false|        false|     false|     false| false|   false|false|   false| false|       false|      false|       false|         false|       false|    false|false|false|   false|         false|  false|     false|    false|        false|  false|           -1|\n|            LordSahu|If you want a hig...|               0| 1701619273|         null|  null|     0|kbtr02e|t3_189ph2m|    3|       Pathfinder2e|    t5_gkc60| false|  false|   false|         false|  false|     false|   false|      false|               false|   false|  false|  false|false| false|false|false|   false|  false| false|   false|    false|        false|   false|false|   false|    false|   false|      false|  false|         false|       false|   false|        false|     false|     false| false|    true|false|   false| false|       false|      false|       false|         false|       false|    false|false|false|   false|         false|  false|     false|    false|        false|  false|            1|\n|  Acceptable-CatProf|I am not super fa...|               0| 1701619273|         null|  null|     0|kbtr03i|t3_189n826|    3|SameGrassButGreener|    t5_2u6j8| false|  false|   false|         false|   true|     false|   false|      false|               false|   false|  false|  false|false| false|false|false|   false|  false| false|   false|    false|        false|   false|false|   false|    false|   false|      false|  false|         false|       false|   false|        false|     false|     false| false|   false|false|   false| false|       false|      false|       false|         false|       false|    false|false|false|   false|         false|  false|     false|    false|        false|  false|            1|\n+--------------------+--------------------+----------------+-----------+-------------+------+------+-------+----------+-----+-------------------+------------+------+-------+--------+--------------+-------+----------+--------+-----------+--------------------+--------+-------+-------+-----+------+-----+-----+--------+-------+------+--------+---------+-------------+--------+-----+--------+---------+--------+-----------+-------+--------------+------------+--------+-------------+----------+----------+------+--------+-----+--------+------+------------+-----------+------------+--------------+------------+---------+-----+-----+--------+--------------+-------+----------+---------+-------------+-------+-------------+\nonly showing top 20 rows\n\n"
        }
      ],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": true
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733773153868
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, VectorAssembler\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "\n",
        "state_names = [\"Alaska\", \"Alabama\", \"Arkansas\", \"American Samoa\", \"Arizona\", \"California\", \n",
        "\"Colorado\", \"Connecticut\", \"District of Columbia\", \"Delaware\", \"Florida\", \"Georgia\", \"Guam\", \n",
        "\"Hawaii\", \"Iowa\", \"Idaho\", \"Illinois\", \"Indiana\", \"Kansas\", \"Kentucky\", \"Louisiana\", \n",
        "\"Massachusetts\", \"Maryland\", \"Maine\", \"Michigan\", \"Minnesota\", \"Missouri\", \"Mississippi\", \n",
        "\"Montana\", \"North Carolina\", \"North Dakota\", \"Nebraska\", \"New Hampshire\", \"New Jersey\", \n",
        "\"New Mexico\", \"Nevada\", \"New York\", \"Ohio\", \"Oklahoma\", \"Oregon\", \"Pennsylvania\", \"Puerto Rico\", \n",
        "\"Rhode Island\", \"South Carolina\", \"South Dakota\", \"Tennessee\", \"Texas\", \"Utah\", \"Virginia\", \n",
        "\"Virgin Islands\", \"Vermont\", \"Washington\", \"Wisconsin\", \"West Virginia\", \"Wyoming\"]\n",
        "state_names = [x.lower() for x in state_names]\n",
        "\n",
        "state_names.append(\"sentiment_num\")\n",
        "state_names.append(\"rawFeatures\")\n",
        "\n",
        "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
        "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=1000)\n",
        "idf = VectorAssembler(inputCols= state_names, outputCol=\"features\")\n",
        "\n",
        "lr = LinearRegression(featuresCol=\"features\", labelCol=\"score\")\n",
        "\n",
        "pipeline_linear = Pipeline(stages=[tokenizer, hashingTF, idf, lr])"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "09c8ba2d-49fa-498b-9d76-d59b947bd032",
              "statement_id": 13,
              "statement_ids": [
                13
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 0,
                  "RUNNING": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "session_id": "55",
              "normalized_state": "finished",
              "queued_time": "2024-12-09T20:16:18.0456034Z",
              "session_start_time": null,
              "execution_start_time": "2024-12-09T20:16:18.1836454Z",
              "execution_finish_time": "2024-12-09T20:16:18.5202705Z",
              "parent_msg_id": "5b6f7a3d-4268-46d7-8586-18d1e63e443a"
            },
            "text/plain": "StatementMeta(09c8ba2d-49fa-498b-9d76-d59b947bd032, 55, 13, Finished, Available, Finished)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733775342423
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(trainingData, testData) = df.randomSplit([0.7, 0.3])\n",
        "\n",
        "model_linear = pipeline_linear.fit(trainingData)\n",
        "predictions_linear = model_linear.transform(testData)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "09c8ba2d-49fa-498b-9d76-d59b947bd032",
              "statement_id": 15,
              "statement_ids": [
                15
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 2,
                  "RUNNING": 0
                },
                "jobs": [
                  {
                    "displayName": "treeAggregate at Statistics.scala:58",
                    "dataWritten": 2942,
                    "dataRead": 2942,
                    "rowCount": 170,
                    "usageDescription": "",
                    "jobId": 7,
                    "name": "treeAggregate at Statistics.scala:58",
                    "description": "Job group for statement 15:\n(trainingData, testData) = df.randomSplit([0.7, 0.3])\n\nmodel_linear = pipeline_linear.fit(trainingData)\npredictions_linear = model_linear.transform(testData)",
                    "submissionTime": "2024-12-09T20:30:19.913GMT",
                    "completionTime": "2024-12-09T20:42:45.148GMT",
                    "stageIds": [
                      12,
                      13
                    ],
                    "jobGroup": "15",
                    "jobTags": [],
                    "status": "SUCCEEDED",
                    "numTasks": 10,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 10,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 10,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "treeAggregate at WeightedLeastSquares.scala:107",
                    "dataWritten": 20582196,
                    "dataRead": 20582196,
                    "rowCount": 170,
                    "usageDescription": "",
                    "jobId": 6,
                    "name": "treeAggregate at WeightedLeastSquares.scala:107",
                    "description": "Job group for statement 15:\n(trainingData, testData) = df.randomSplit([0.7, 0.3])\n\nmodel_linear = pipeline_linear.fit(trainingData)\npredictions_linear = model_linear.transform(testData)",
                    "submissionTime": "2024-12-09T20:17:22.902GMT",
                    "completionTime": "2024-12-09T20:30:18.750GMT",
                    "stageIds": [
                      10,
                      11
                    ],
                    "jobGroup": "15",
                    "jobTags": [],
                    "status": "SUCCEEDED",
                    "numTasks": 10,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 10,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 10,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "session_id": "55",
              "normalized_state": "finished",
              "queued_time": "2024-12-09T20:17:21.6586219Z",
              "session_start_time": null,
              "execution_start_time": "2024-12-09T20:17:21.8045611Z",
              "execution_finish_time": "2024-12-09T20:42:50.1132692Z",
              "parent_msg_id": "130020c9-4d24-4e97-ba8a-5256400e7061"
            },
            "text/plain": "StatementMeta(09c8ba2d-49fa-498b-9d76-d59b947bd032, 55, 15, Finished, Available, Finished)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 10,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733776934195
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datastore = 'azureml://datastores/workspaceblobstore/paths'\n",
        "model_linear.save(f\"{datastore}/models/ml_model_linear_score_prediction\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "09c8ba2d-49fa-498b-9d76-d59b947bd032",
              "statement_id": 18,
              "statement_ids": [
                18
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 0,
                  "RUNNING": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "session_id": "55",
              "normalized_state": "finished",
              "queued_time": "2024-12-09T20:45:13.802373Z",
              "session_start_time": null,
              "execution_start_time": "2024-12-09T20:50:05.1641051Z",
              "execution_finish_time": "2024-12-09T20:50:06.0587007Z",
              "parent_msg_id": "5c48fe07-a1b2-4d12-b763-df9c7b76e4aa"
            },
            "text/plain": "StatementMeta(09c8ba2d-49fa-498b-9d76-d59b947bd032, 55, 18, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling o6977.save.\n: java.io.IOException: Path azureml://datastores/workspaceblobstore/paths/models/ml_model_linear_score_prediction already exists. To overwrite it, please use write.overwrite().save(path) for Scala and use write().overwrite().save(path) for Java and Python.\n\tat org.apache.spark.ml.util.FileSystemOverwrite.handleOverwrite(ReadWrite.scala:683)\n\tat org.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:167)\n\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.super$save(Pipeline.scala:344)\n\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$4(Pipeline.scala:344)\n\tat org.apache.spark.ml.MLEvents.withSaveInstanceEvent(events.scala:174)\n\tat org.apache.spark.ml.MLEvents.withSaveInstanceEvent$(events.scala:169)\n\tat org.apache.spark.ml.util.Instrumentation.withSaveInstanceEvent(Instrumentation.scala:42)\n\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$3(Pipeline.scala:344)\n\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$3$adapted(Pipeline.scala:344)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.save(Pipeline.scala:344)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[35], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m datastore \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mazureml://datastores/workspaceblobstore/paths\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel_linear\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdatastore\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/models/ml_model_linear_score_prediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/ml/util.py:246\u001b[0m, in \u001b[0;36mMLWritable.save\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave\u001b[39m(\u001b[38;5;28mself\u001b[39m, path: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    245\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Save this ML instance to the given path, a shortcut of 'write().save(path)'.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 246\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/ml/util.py:197\u001b[0m, in \u001b[0;36mJavaMLWriter.save\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath should be a string, got type \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(path))\n\u001b[0;32m--> 197\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/cluster-env/env/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
            "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    171\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
            "File \u001b[0;32m~/cluster-env/env/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o6977.save.\n: java.io.IOException: Path azureml://datastores/workspaceblobstore/paths/models/ml_model_linear_score_prediction already exists. To overwrite it, please use write.overwrite().save(path) for Scala and use write().overwrite().save(path) for Java and Python.\n\tat org.apache.spark.ml.util.FileSystemOverwrite.handleOverwrite(ReadWrite.scala:683)\n\tat org.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:167)\n\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.super$save(Pipeline.scala:344)\n\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$4(Pipeline.scala:344)\n\tat org.apache.spark.ml.MLEvents.withSaveInstanceEvent(events.scala:174)\n\tat org.apache.spark.ml.MLEvents.withSaveInstanceEvent$(events.scala:169)\n\tat org.apache.spark.ml.util.Instrumentation.withSaveInstanceEvent(Instrumentation.scala:42)\n\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$3(Pipeline.scala:344)\n\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$3$adapted(Pipeline.scala:344)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.save(Pipeline.scala:344)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
          ]
        }
      ],
      "execution_count": 13,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733777369946
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml import PipelineModel\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        "PipelineModel.load(f\"{datastore}/models/ml_model_score_prediction\") \n",
        "evaluator = RegressionEvaluator(labelCol=\"score\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
        "rmse = evaluator.evaluate(predictions_linear)\n",
        "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "09c8ba2d-49fa-498b-9d76-d59b947bd032",
              "statement_id": 17,
              "statement_ids": [
                17
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 13,
                  "RUNNING": 0
                },
                "jobs": [
                  {
                    "displayName": "treeAggregate at Statistics.scala:58",
                    "dataWritten": 2942,
                    "dataRead": 2942,
                    "rowCount": 170,
                    "usageDescription": "",
                    "jobId": 27,
                    "name": "treeAggregate at Statistics.scala:58",
                    "description": "Job group for statement 17:\nfrom pyspark.ml import PipelineModel\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\nPipelineModel.load(f\"{datastore}/models/ml_model_score_prediction\") \nevaluator = RegressionEvaluator(labelCol=\"score\", predictionCol=\"prediction\", metricName=\"rmse\")\nrmse = evaluator.evaluate(predictions_linear)\nprint(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)",
                    "submissionTime": "2024-12-09T20:43:13.456GMT",
                    "completionTime": "2024-12-09T20:50:02.031GMT",
                    "stageIds": [
                      34,
                      35
                    ],
                    "jobGroup": "17",
                    "jobTags": [],
                    "status": "SUCCEEDED",
                    "numTasks": 10,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 10,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 10,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "head at LinearRegression.scala:845",
                    "dataWritten": 0,
                    "dataRead": 13951,
                    "rowCount": 1,
                    "usageDescription": "",
                    "jobId": 26,
                    "name": "head at LinearRegression.scala:845",
                    "description": "Job group for statement 17:\nfrom pyspark.ml import PipelineModel\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\nPipelineModel.load(f\"{datastore}/models/ml_model_score_prediction\") \nevaluator = RegressionEvaluator(labelCol=\"score\", predictionCol=\"prediction\", metricName=\"rmse\")\nrmse = evaluator.evaluate(predictions_linear)\nprint(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)",
                    "submissionTime": "2024-12-09T20:43:12.504GMT",
                    "completionTime": "2024-12-09T20:43:13.075GMT",
                    "stageIds": [
                      33
                    ],
                    "jobGroup": "17",
                    "jobTags": [],
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "load at LinearRegression.scala:833",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 25,
                    "name": "load at LinearRegression.scala:833",
                    "description": "Job group for statement 17:\nfrom pyspark.ml import PipelineModel\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\nPipelineModel.load(f\"{datastore}/models/ml_model_score_prediction\") \nevaluator = RegressionEvaluator(labelCol=\"score\", predictionCol=\"prediction\", metricName=\"rmse\")\nrmse = evaluator.evaluate(predictions_linear)\nprint(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)",
                    "submissionTime": "2024-12-09T20:43:11.126GMT",
                    "completionTime": "2024-12-09T20:43:12.211GMT",
                    "stageIds": [
                      32
                    ],
                    "jobGroup": "17",
                    "jobTags": [],
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "first at ReadWrite.scala:587",
                    "dataWritten": 0,
                    "dataRead": 521,
                    "rowCount": 1,
                    "usageDescription": "",
                    "jobId": 24,
                    "name": "first at ReadWrite.scala:587",
                    "description": "Job group for statement 17:\nfrom pyspark.ml import PipelineModel\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\nPipelineModel.load(f\"{datastore}/models/ml_model_score_prediction\") \nevaluator = RegressionEvaluator(labelCol=\"score\", predictionCol=\"prediction\", metricName=\"rmse\")\nrmse = evaluator.evaluate(predictions_linear)\nprint(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)",
                    "submissionTime": "2024-12-09T20:43:10.676GMT",
                    "completionTime": "2024-12-09T20:43:10.755GMT",
                    "stageIds": [
                      31
                    ],
                    "jobGroup": "17",
                    "jobTags": [],
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "first at ReadWrite.scala:587",
                    "dataWritten": 0,
                    "dataRead": 521,
                    "rowCount": 1,
                    "usageDescription": "",
                    "jobId": 23,
                    "name": "first at ReadWrite.scala:587",
                    "description": "Job group for statement 17:\nfrom pyspark.ml import PipelineModel\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\nPipelineModel.load(f\"{datastore}/models/ml_model_score_prediction\") \nevaluator = RegressionEvaluator(labelCol=\"score\", predictionCol=\"prediction\", metricName=\"rmse\")\nrmse = evaluator.evaluate(predictions_linear)\nprint(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)",
                    "submissionTime": "2024-12-09T20:43:10.506GMT",
                    "completionTime": "2024-12-09T20:43:10.589GMT",
                    "stageIds": [
                      30
                    ],
                    "jobGroup": "17",
                    "jobTags": [],
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "first at ReadWrite.scala:587",
                    "dataWritten": 0,
                    "dataRead": 982,
                    "rowCount": 1,
                    "usageDescription": "",
                    "jobId": 22,
                    "name": "first at ReadWrite.scala:587",
                    "description": "Job group for statement 17:\nfrom pyspark.ml import PipelineModel\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\nPipelineModel.load(f\"{datastore}/models/ml_model_score_prediction\") \nevaluator = RegressionEvaluator(labelCol=\"score\", predictionCol=\"prediction\", metricName=\"rmse\")\nrmse = evaluator.evaluate(predictions_linear)\nprint(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)",
                    "submissionTime": "2024-12-09T20:43:10.286GMT",
                    "completionTime": "2024-12-09T20:43:10.390GMT",
                    "stageIds": [
                      29
                    ],
                    "jobGroup": "17",
                    "jobTags": [],
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "first at ReadWrite.scala:587",
                    "dataWritten": 0,
                    "dataRead": 982,
                    "rowCount": 1,
                    "usageDescription": "",
                    "jobId": 21,
                    "name": "first at ReadWrite.scala:587",
                    "description": "Job group for statement 17:\nfrom pyspark.ml import PipelineModel\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\nPipelineModel.load(f\"{datastore}/models/ml_model_score_prediction\") \nevaluator = RegressionEvaluator(labelCol=\"score\", predictionCol=\"prediction\", metricName=\"rmse\")\nrmse = evaluator.evaluate(predictions_linear)\nprint(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)",
                    "submissionTime": "2024-12-09T20:43:10.080GMT",
                    "completionTime": "2024-12-09T20:43:10.166GMT",
                    "stageIds": [
                      28
                    ],
                    "jobGroup": "17",
                    "jobTags": [],
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "first at ReadWrite.scala:587",
                    "dataWritten": 0,
                    "dataRead": 323,
                    "rowCount": 1,
                    "usageDescription": "",
                    "jobId": 20,
                    "name": "first at ReadWrite.scala:587",
                    "description": "Job group for statement 17:\nfrom pyspark.ml import PipelineModel\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\nPipelineModel.load(f\"{datastore}/models/ml_model_score_prediction\") \nevaluator = RegressionEvaluator(labelCol=\"score\", predictionCol=\"prediction\", metricName=\"rmse\")\nrmse = evaluator.evaluate(predictions_linear)\nprint(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)",
                    "submissionTime": "2024-12-09T20:43:09.903GMT",
                    "completionTime": "2024-12-09T20:43:09.995GMT",
                    "stageIds": [
                      27
                    ],
                    "jobGroup": "17",
                    "jobTags": [],
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "first at ReadWrite.scala:587",
                    "dataWritten": 0,
                    "dataRead": 323,
                    "rowCount": 1,
                    "usageDescription": "",
                    "jobId": 19,
                    "name": "first at ReadWrite.scala:587",
                    "description": "Job group for statement 17:\nfrom pyspark.ml import PipelineModel\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\nPipelineModel.load(f\"{datastore}/models/ml_model_score_prediction\") \nevaluator = RegressionEvaluator(labelCol=\"score\", predictionCol=\"prediction\", metricName=\"rmse\")\nrmse = evaluator.evaluate(predictions_linear)\nprint(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)",
                    "submissionTime": "2024-12-09T20:43:09.711GMT",
                    "completionTime": "2024-12-09T20:43:09.788GMT",
                    "stageIds": [
                      26
                    ],
                    "jobGroup": "17",
                    "jobTags": [],
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "first at ReadWrite.scala:587",
                    "dataWritten": 0,
                    "dataRead": 261,
                    "rowCount": 1,
                    "usageDescription": "",
                    "jobId": 18,
                    "name": "first at ReadWrite.scala:587",
                    "description": "Job group for statement 17:\nfrom pyspark.ml import PipelineModel\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\nPipelineModel.load(f\"{datastore}/models/ml_model_score_prediction\") \nevaluator = RegressionEvaluator(labelCol=\"score\", predictionCol=\"prediction\", metricName=\"rmse\")\nrmse = evaluator.evaluate(predictions_linear)\nprint(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)",
                    "submissionTime": "2024-12-09T20:43:09.514GMT",
                    "completionTime": "2024-12-09T20:43:09.612GMT",
                    "stageIds": [
                      25
                    ],
                    "jobGroup": "17",
                    "jobTags": [],
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "first at ReadWrite.scala:587",
                    "dataWritten": 0,
                    "dataRead": 261,
                    "rowCount": 1,
                    "usageDescription": "",
                    "jobId": 17,
                    "name": "first at ReadWrite.scala:587",
                    "description": "Job group for statement 17:\nfrom pyspark.ml import PipelineModel\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\nPipelineModel.load(f\"{datastore}/models/ml_model_score_prediction\") \nevaluator = RegressionEvaluator(labelCol=\"score\", predictionCol=\"prediction\", metricName=\"rmse\")\nrmse = evaluator.evaluate(predictions_linear)\nprint(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)",
                    "submissionTime": "2024-12-09T20:43:09.262GMT",
                    "completionTime": "2024-12-09T20:43:09.389GMT",
                    "stageIds": [
                      24
                    ],
                    "jobGroup": "17",
                    "jobTags": [],
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "first at ReadWrite.scala:587",
                    "dataWritten": 0,
                    "dataRead": 306,
                    "rowCount": 1,
                    "usageDescription": "",
                    "jobId": 16,
                    "name": "first at ReadWrite.scala:587",
                    "description": "Job group for statement 17:\nfrom pyspark.ml import PipelineModel\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\nPipelineModel.load(f\"{datastore}/models/ml_model_score_prediction\") \nevaluator = RegressionEvaluator(labelCol=\"score\", predictionCol=\"prediction\", metricName=\"rmse\")\nrmse = evaluator.evaluate(predictions_linear)\nprint(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)",
                    "submissionTime": "2024-12-09T20:43:08.894GMT",
                    "completionTime": "2024-12-09T20:43:09.156GMT",
                    "stageIds": [
                      23
                    ],
                    "jobGroup": "17",
                    "jobTags": [],
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "runJob at PythonRDD.scala:185",
                    "dataWritten": 0,
                    "dataRead": 306,
                    "rowCount": 1,
                    "usageDescription": "",
                    "jobId": 15,
                    "name": "runJob at PythonRDD.scala:185",
                    "description": "Job group for statement 17:\nfrom pyspark.ml import PipelineModel\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\nPipelineModel.load(f\"{datastore}/models/ml_model_score_prediction\") \nevaluator = RegressionEvaluator(labelCol=\"score\", predictionCol=\"prediction\", metricName=\"rmse\")\nrmse = evaluator.evaluate(predictions_linear)\nprint(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)",
                    "submissionTime": "2024-12-09T20:43:07.166GMT",
                    "completionTime": "2024-12-09T20:43:08.760GMT",
                    "stageIds": [
                      22
                    ],
                    "jobGroup": "17",
                    "jobTags": [],
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "session_id": "55",
              "normalized_state": "finished",
              "queued_time": "2024-12-09T20:41:23.9420398Z",
              "session_start_time": null,
              "execution_start_time": "2024-12-09T20:43:06.9317794Z",
              "execution_finish_time": "2024-12-09T20:50:04.9443957Z",
              "parent_msg_id": "674ccf99-2eb4-4b59-bdd7-23c7bc870af9"
            },
            "text/plain": "StatementMeta(09c8ba2d-49fa-498b-9d76-d59b947bd032, 55, 17, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Root Mean Squared Error (RMSE) on test data = 68.5895\n"
        }
      ],
      "execution_count": 12,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733777368905
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.ml import PipelineModel\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, VectorAssembler\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "from pyspark.ml.regression import RandomForestRegressor\n",
        "\n",
        "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
        "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=1000)\n",
        "idf = VectorAssembler(inputCols= state_names, outputCol=\"features\")\n",
        "\n",
        "rf = RandomForestRegressor(featuresCol=\"indexedFeatures\", labelCol = \"score\")\n",
        "\n",
        "pipeline_forest = Pipeline(stages=[tokenizer, hashingTF, idf, rf])\n",
        "\n",
        "(trainingData, testData) = df.randomSplit([0.7, 0.3])\n",
        "\n",
        "model_forest = pipeline_forest.fit(trainingData)\n",
        "predictions_forest = model_forest.transform(testData)\n",
        "\n",
        "#PipelineModel.load(f\"{datastore}/models/ml_model_score_prediction\") \n",
        "evaluator = RegressionEvaluator(labelCol=\"score\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
        "rmse = evaluator.evaluate(predictions_forest)\n",
        "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "09c8ba2d-49fa-498b-9d76-d59b947bd032",
              "statement_id": 10,
              "statement_ids": [
                10
              ],
              "state": "submitted",
              "livy_statement_state": "running",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 0,
                  "RUNNING": 1
                },
                "jobs": [
                  {
                    "displayName": "treeAggregate at WeightedLeastSquares.scala:107",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 2,
                    "name": "treeAggregate at WeightedLeastSquares.scala:107",
                    "description": "Job group for statement 10:\nfrom pyspark.ml.evaluation import RegressionEvaluator\nfrom pyspark.ml import PipelineModel\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import HashingTF, IDF, Tokenizer, VectorAssembler\nfrom pyspark.ml.regression import LinearRegression\n\ntokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\nhashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=1000)\nidf = VectorAssembler(inputCols= state_names, outputCol=\"features\")\n\nlr = LinearRegression(featuresCol=\"features\", labelCol=\"score\")\n\npipeline = Pipeline(stages=[tokenizer, hashingTF, idf, lr])\n\n(trainingData, testData) = df.randomSplit([0.7, 0.3])\n\nmodel = pipeline.fit(trainingData)\npredictions = model.transform(testData)\n\nPipelineModel.load(f\"{datastore}/models/ml_model_score_prediction\") \nevaluator = RegressionEvaluator(labelCol=\"score\", predictionCol=\"prediction\", metricName=\"rmse\")\nrmse = evaluator.evaluate(predictions)\nprint(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)",
                    "submissionTime": "2024-12-09T19:41:41.414GMT",
                    "stageIds": [
                      2,
                      3
                    ],
                    "jobGroup": "10",
                    "jobTags": [],
                    "status": "RUNNING",
                    "numTasks": 10,
                    "numActiveTasks": 8,
                    "numCompletedTasks": 0,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 0,
                    "numActiveStages": 1,
                    "numCompletedStages": 0,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "session_id": "55",
              "normalized_state": "running",
              "queued_time": "2024-12-09T19:41:37.9924082Z",
              "session_start_time": null,
              "execution_start_time": "2024-12-09T19:41:38.133214Z",
              "execution_finish_time": null,
              "parent_msg_id": "f0a3fea2-8c98-4922-ad44-e890586f7afa"
            },
            "text/plain": "StatementMeta(09c8ba2d-49fa-498b-9d76-d59b947bd032, 55, 10, Submitted, Running, Running)"
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "IllegalArgumentException",
          "evalue": "java.net.URISyntaxException: Relative path in absolute URI: {\n  \"name\":%20%22workspaceblobstore%22,%0A%20%20%22container_name%22:%20%22azureml-blobstore-ab2acca4-77bf-4607-9fc6-941bb7fd6768%22,%0A%20%20%22account_name%22:%20%22projectgstorageef8131633%22,%0A%20%20%22protocol%22:%20%22https%22,%0A%20%20%22endpoint%22:%20%22core.windows.net%22%0A%7D/models/ml_model_score_prediction/metadata",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[19], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m model \u001b[38;5;241m=\u001b[39m pipeline\u001b[38;5;241m.\u001b[39mfit(trainingData)\n\u001b[1;32m     18\u001b[0m predictions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtransform(testData)\n\u001b[0;32m---> 20\u001b[0m \u001b[43mPipelineModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdatastore\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/models/ml_model_score_prediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \n\u001b[1;32m     21\u001b[0m evaluator \u001b[38;5;241m=\u001b[39m RegressionEvaluator(labelCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m\"\u001b[39m, predictionCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m, metricName\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrmse\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     22\u001b[0m rmse \u001b[38;5;241m=\u001b[39m evaluator\u001b[38;5;241m.\u001b[39mevaluate(predictions)\n",
            "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/ml/util.py:353\u001b[0m, in \u001b[0;36mMLReadable.load\u001b[0;34m(cls, path)\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mcls\u001b[39m, path: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RL:\n\u001b[1;32m    352\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Reads an ML instance from the input path, a shortcut of `read().load(path)`.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/ml/pipeline.py:282\u001b[0m, in \u001b[0;36mPipelineModelReader.load\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mself\u001b[39m, path: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipelineModel\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 282\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[43mDefaultParamsReader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloadMetadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlanguage\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m metadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparamMap\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m metadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparamMap\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlanguage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPython\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    284\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m JavaMLReader(cast(Type[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJavaMLReadable[PipelineModel]\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls))\u001b[38;5;241m.\u001b[39mload(path)\n",
            "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/ml/util.py:563\u001b[0m, in \u001b[0;36mDefaultParamsReader.loadMetadata\u001b[0;34m(path, sc, expectedClassName)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;124;03mLoad metadata saved using :py:meth:`DefaultParamsWriter.saveMetadata`\u001b[39;00m\n\u001b[1;32m    554\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    560\u001b[0m \u001b[38;5;124;03m    If non empty, this is checked against the loaded metadata.\u001b[39;00m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    562\u001b[0m metadataPath \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 563\u001b[0m metadataStr \u001b[38;5;241m=\u001b[39m \u001b[43msc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtextFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetadataPath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfirst\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    564\u001b[0m loadedVals \u001b[38;5;241m=\u001b[39m DefaultParamsReader\u001b[38;5;241m.\u001b[39m_parseMetaData(metadataStr, expectedClassName)\n\u001b[1;32m    565\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loadedVals\n",
            "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/rdd.py:2869\u001b[0m, in \u001b[0;36mRDD.first\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2843\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfirst\u001b[39m(\u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRDD[T]\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m   2844\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2845\u001b[0m \u001b[38;5;124;03m    Return the first element in this RDD.\u001b[39;00m\n\u001b[1;32m   2846\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2867\u001b[0m \u001b[38;5;124;03m    ValueError: RDD is empty\u001b[39;00m\n\u001b[1;32m   2868\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2869\u001b[0m     rs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2870\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m rs:\n\u001b[1;32m   2871\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m rs[\u001b[38;5;241m0\u001b[39m]\n",
            "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/rdd.py:2803\u001b[0m, in \u001b[0;36mRDD.take\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   2762\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2763\u001b[0m \u001b[38;5;124;03mTake the first num elements of the RDD.\u001b[39;00m\n\u001b[1;32m   2764\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2800\u001b[0m \u001b[38;5;124;03m[91, 92, 93]\u001b[39;00m\n\u001b[1;32m   2801\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2802\u001b[0m items: List[T] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m-> 2803\u001b[0m totalParts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetNumPartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2804\u001b[0m partsScanned \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   2806\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(items) \u001b[38;5;241m<\u001b[39m num \u001b[38;5;129;01mand\u001b[39;00m partsScanned \u001b[38;5;241m<\u001b[39m totalParts:\n\u001b[1;32m   2807\u001b[0m     \u001b[38;5;66;03m# The number of partitions to try in this iteration.\u001b[39;00m\n\u001b[1;32m   2808\u001b[0m     \u001b[38;5;66;03m# It is ok for this number to be greater than totalParts because\u001b[39;00m\n\u001b[1;32m   2809\u001b[0m     \u001b[38;5;66;03m# we actually cap it at totalParts in runJob.\u001b[39;00m\n",
            "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/rdd.py:938\u001b[0m, in \u001b[0;36mRDD.getNumPartitions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    921\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgetNumPartitions\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m    922\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    923\u001b[0m \u001b[38;5;124;03m    Returns the number of partitions in RDD\u001b[39;00m\n\u001b[1;32m    924\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    936\u001b[0m \u001b[38;5;124;03m    2\u001b[39;00m\n\u001b[1;32m    937\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 938\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msize()\n",
            "File \u001b[0;32m~/cluster-env/env/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
            "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    171\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
            "\u001b[0;31mIllegalArgumentException\u001b[0m: java.net.URISyntaxException: Relative path in absolute URI: {\n  \"name\":%20%22workspaceblobstore%22,%0A%20%20%22container_name%22:%20%22azureml-blobstore-ab2acca4-77bf-4607-9fc6-941bb7fd6768%22,%0A%20%20%22account_name%22:%20%22projectgstorageef8131633%22,%0A%20%20%22protocol%22:%20%22https%22,%0A%20%20%22endpoint%22:%20%22core.windows.net%22%0A%7D/models/ml_model_score_prediction/metadata"
          ]
        }
      ],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733774674533
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0",
      "mimetype": "text/x-python",
      "file_extension": ".py",
      "pygments_lexer": "ipython",
      "codemirror_mode": "ipython",
      "nbconvert_exporter": "python"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}