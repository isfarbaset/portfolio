{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3862d66e-ed2e-40e4-a14d-a89c1ec435fb",
   "metadata": {},
   "source": [
    "## Pull subreddit data for relevant subreddits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3e8b4a-96a0-4417-867f-b1d6d840297e",
   "metadata": {},
   "source": [
    "## Setup\n",
    "We need an available Java installation to run pyspark. The easiest way to do this is to install JDK and set the proper paths using conda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6077ac36-4656-48e2-b4ef-0c6a455917bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IOStream.flush timed out\n",
      "Retrieving notices: ...working... done\n",
      "\n",
      "Downloading and Extracting Packages:\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /opt/conda\n",
      "\n",
      "\n",
      "\n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "IOStream.flush timed out\n",
      "Requirement already satisfied: pyspark==3.4.0 in /opt/conda/lib/python3.11/site-packages (3.4.0)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /opt/conda/lib/python3.11/site-packages (from pyspark==3.4.0) (0.10.9.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<script>Jupyter.notebook.kernel.restart()</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup - Run only once per Kernel App\n",
    "%conda install https://anaconda.org/conda-forge/openjdk/11.0.1/download/linux-64/openjdk-11.0.1-hacce0ff_1021.tar.bz2\n",
    "\n",
    "# install PySpark\n",
    "%pip install pyspark==3.4.0\n",
    "\n",
    "# restart kernel\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a42c8b-0d67-4724-807b-403e788c9b7f",
   "metadata": {},
   "source": [
    "## Utilize S3 Data within local PySpark\n",
    "* By specifying the `hadoop-aws` jar in our Spark config we're able to access S3 datasets using the s3a file prefix. \n",
    "* Since we've already authenticated ourself to SageMaker Studio , we can use our assumed SageMaker ExecutionRole for any S3 reads/writes by setting the credential provider as `ContainerCredentialsProvider`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cd422206-005f-4161-b242-90ee3fa04434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Error while sending or receiving.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/py4j/clientserver.py\", line 503, in send_command\n",
      "    self.socket.sendall(command.encode(\"utf-8\"))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "INFO:py4j.clientserver:Closing down clientserver connection\n",
      "INFO:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/py4j/clientserver.py\", line 503, in send_command\n",
      "    self.socket.sendall(command.encode(\"utf-8\"))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/py4j/clientserver.py\", line 506, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending\n",
      "INFO:py4j.clientserver:Closing down clientserver connection\n"
     ]
    }
   ],
   "source": [
    "# Import pyspark and build Spark session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"PySparkApp\")\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.2.2\")\n",
    "    .config(\n",
    "        \"fs.s3a.aws.credentials.provider\",\n",
    "        \"com.amazonaws.auth.ContainerCredentialsProvider\",\n",
    "    )\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print(spark.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874b450b-b585-43ad-a4df-c01be948d7a6",
   "metadata": {},
   "source": [
    "## Process S3 data with SageMaker Processing Job `PySparkProcessor`\n",
    "\n",
    "We are going to move the above processing code in a Python file and then submit that file to SageMaker Processing Job's [`PySparkProcessor`](https://sagemaker.readthedocs.io/en/stable/amazon_sagemaker_processing.html#pysparkprocessor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f888993b-df9a-4b18-aa87-894e7f5bf3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ./pull_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9830aae-c934-4fd5-abdb-2d3ddab95f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./pull_data/process.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./pull_data/process.py\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import argparse\n",
    "\n",
    "# Import pyspark and build Spark session\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import (\n",
    "    DoubleType,\n",
    "    IntegerType,\n",
    "    StringType,\n",
    "    StructField,\n",
    "    StructType,\n",
    ")\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s,%(levelname)s,%(module)s,%(filename)s,%(lineno)d,%(message)s', level=logging.DEBUG)\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(logging.StreamHandler(sys.stdout))\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"app inputs and outputs\")\n",
    "    parser.add_argument(\"--s3_dataset_path_commments\", type=str, help=\"Path of dataset in S3 for reddit comments\")\n",
    "    parser.add_argument(\"--s3_dataset_path_submissions\", type=str, help=\"Path of dataset in S3 for reddit submissions\")\n",
    "    parser.add_argument(\"--s3_output_bucket\", type=str, help=\"s3 output bucket\")\n",
    "    parser.add_argument(\"--s3_output_prefix\", type=str, help=\"s3 output prefix\")\n",
    "    parser.add_argument(\"--subreddits\", type=str, help=\"comma separate list of subreddits of interest\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    spark = SparkSession.builder.appName(\"PySparkApp\").getOrCreate()\n",
    "    logger.info(f\"spark version = {spark.version}\")\n",
    "    \n",
    "    # This is needed to save RDDs which is the only way to write nested Dataframes into CSV format\n",
    "    sc = spark.sparkContext\n",
    "    sc._jsc.hadoopConfiguration().set(\n",
    "        \"mapred.output.committer.class\", \"org.apache.hadoop.mapred.FileOutputCommitter\"\n",
    "    )\n",
    "\n",
    "   \n",
    "    # Downloading the data from S3 into a Dataframe\n",
    "    logger.info(f\"going to read {args.s3_dataset_path_commments}\")\n",
    "    comments = spark.read.parquet(args.s3_dataset_path_commments, header=True)\n",
    "    logger.info(f\"finished reading files...\")\n",
    "    \n",
    "    logger.info(f\"going to read {args.s3_dataset_path_submissions}\")\n",
    "    submissions = spark.read.parquet(args.s3_dataset_path_submissions, header=True)\n",
    "    logger.info(f\"finished reading files...\")\n",
    "    \n",
    "    # filter the dataframe to only keep the subreddits of interest\n",
    "    subreddits = [s.strip() for s in args.subreddits.split(\",\")]\n",
    "    excluded_cols = [] # ['edited', 'created_utc', 'retrieved_on']\n",
    "    \n",
    "    comments_included_cols = [c for c in comments.columns if c not in excluded_cols]\n",
    "    logger.info(f\"comments included_cols={comments_included_cols}\")\n",
    "\n",
    "    submissions_included_cols = [c for c in submissions.columns if c not in excluded_cols]\n",
    "    logger.info(f\"submissions included_cols={submissions_included_cols}\")\n",
    "\n",
    "    # subset the dataframes because \"edited\" and \"created_utc\" have data type problems\n",
    "    # sometimes they occur as int some time as float and since schema is encoded in the \n",
    "    # parquet files therefore different files have different data tpyes for these fields (float in some cases, int in some cases)\n",
    "    # and spark enforces strict type checking on read so the only option we have is to either\n",
    "    # fix this outside of spark or delete these columns.\n",
    "    comments = comments.select(comments_included_cols)\n",
    "    submissions = submissions.select(submissions_included_cols)\n",
    "    \n",
    "    submissions_filtered = submissions.where(lower(col(\"subreddit\")).isin(subreddits))\n",
    "    comments_filtered = comments.where(lower(col(\"subreddit\")).isin(subreddits))\n",
    "    \n",
    "    # save the filtered dataframes so that these files can now be used for future analysis\n",
    "    s3_path = f\"s3://{args.s3_output_bucket}/{args.s3_output_prefix}/comments\"\n",
    "    logger.info(f\"going to write comments for {subreddits} in {s3_path}\")\n",
    "    logger.info(f\"shape of the comments_filtered dataframe is {comments_filtered.count():,}x{len(comments_filtered.columns)}\")\n",
    "    comments_filtered.write.mode(\"overwrite\").parquet(s3_path)\n",
    "    \n",
    "    s3_path = f\"s3://{args.s3_output_bucket}/{args.s3_output_prefix}/submissions\"\n",
    "    logger.info(f\"going to write submissions for {subreddits} in {s3_path}\")\n",
    "    logger.info(f\"shape of the submissions_filtered dataframe is {submissions_filtered.count():,}x{len(submissions_filtered.columns)}\")\n",
    "    submissions_filtered.write.mode(\"overwrite\").parquet(s3_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913a41e2-ba48-4cc5-9b8c-2da30e7901ca",
   "metadata": {},
   "source": [
    "### Now submit this code to SageMaker Processing Job.\n",
    "\n",
    "For the r/travel, r/usatravel, and r/AskAnAmerican subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815ac57c-6db3-420b-b9d4-901cdd94c923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating processing-job with name sm-spark-project-2024-11-08-16-16-49-511\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...............................................................................................................................................................................................................................................................................................................................!CPU times: user 4.22 s, sys: 418 ms, total: 4.64 s\n",
      "Wall time: 37min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import sagemaker\n",
    "from sagemaker.spark.processing import PySparkProcessor\n",
    "\n",
    "# Setup the PySpark processor to run the job. Note the instance type and instance count parameters. SageMaker will create these many instances of this type for the spark job.\n",
    "role = sagemaker.get_execution_role()\n",
    "spark_processor = PySparkProcessor(\n",
    "    base_job_name=\"sm-spark-project\",\n",
    "    framework_version=\"3.3\",\n",
    "    role=role,\n",
    "    instance_count=4,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    max_runtime_in_seconds=3600,\n",
    ")\n",
    "\n",
    "# s3 paths\n",
    "session = sagemaker.Session()\n",
    "bucket = session.default_bucket()\n",
    "s3_dataset_path_commments = \"s3://bigdatateaching/reddit-project/reddit/parquet/comments/yyyy=*/mm=*/*.parquet\"\n",
    "s3_dataset_path_submissions = \"s3://bigdatateaching/reddit-project/reddit/parquet/submissions/yyyy=*/mm=*/*.parquet\"\n",
    "output_prefix_data = \"project\"\n",
    "output_prefix_logs = f\"spark_logs\"\n",
    "\n",
    "# modify this comma separated list to choose the subreddits of interest\n",
    "#subreddits = \"technology,chatgpt\"\n",
    "subreddits = \"travel,usatravel,askanamerican\"\n",
    "    \n",
    "# run the job now, the arguments array is provided as command line to the Python script (Spark code in this case).\n",
    "spark_processor.run(\n",
    "    submit_app=\"./pull_data/process.py\",\n",
    "    arguments=[\n",
    "        \"--s3_dataset_path_commments\",\n",
    "        s3_dataset_path_commments,\n",
    "        \"--s3_dataset_path_submissions\",\n",
    "        s3_dataset_path_submissions,\n",
    "        \"--s3_output_bucket\",\n",
    "              bucket,\n",
    "        \"--s3_output_prefix\",\n",
    "        output_prefix_data,\n",
    "        \"--subreddits\",\n",
    "        subreddits,\n",
    "    ],\n",
    "    spark_event_logs_s3_uri=\"s3://{}/{}/spark_event_logs\".format(bucket, output_prefix_logs),\n",
    "    logs=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3a10dc-9e18-459d-b308-b8aec7eb25af",
   "metadata": {},
   "source": [
    "### Re-write process.py for another pull\n",
    "\n",
    "For the r/alabama, alaska, arizona, arkansas, california, colorado, connecticut, delaware, florida, georgia, hawaii, idaho, illinois, indiana, iowa, kansas, kentucky, louisiana, maine, maryland, massachusetts, michigan, minnesota, mississippi, missouri, montana, nebraska, nevada subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80698717-2ec6-465a-9013-8b32ca43eb41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./pull_data/process.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./pull_data/process.py\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import argparse\n",
    "\n",
    "# Import pyspark and build Spark session\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import (\n",
    "    DoubleType,\n",
    "    IntegerType,\n",
    "    StringType,\n",
    "    StructField,\n",
    "    StructType,\n",
    ")\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s,%(levelname)s,%(module)s,%(filename)s,%(lineno)d,%(message)s', level=logging.DEBUG)\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(logging.StreamHandler(sys.stdout))\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"app inputs and outputs\")\n",
    "    parser.add_argument(\"--s3_dataset_path_commments\", type=str, help=\"Path of dataset in S3 for reddit comments\")\n",
    "    parser.add_argument(\"--s3_dataset_path_submissions\", type=str, help=\"Path of dataset in S3 for reddit submissions\")\n",
    "    parser.add_argument(\"--s3_output_bucket\", type=str, help=\"s3 output bucket\")\n",
    "    parser.add_argument(\"--s3_output_prefix\", type=str, help=\"s3 output prefix\")\n",
    "    parser.add_argument(\"--subreddits\", type=str, help=\"comma separate list of subreddits of interest\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    spark = SparkSession.builder.appName(\"PySparkApp\").getOrCreate()\n",
    "    logger.info(f\"spark version = {spark.version}\")\n",
    "    \n",
    "    # This is needed to save RDDs which is the only way to write nested Dataframes into CSV format\n",
    "    sc = spark.sparkContext\n",
    "    sc._jsc.hadoopConfiguration().set(\n",
    "        \"mapred.output.committer.class\", \"org.apache.hadoop.mapred.FileOutputCommitter\"\n",
    "    )\n",
    "\n",
    "   \n",
    "    # Downloading the data from S3 into a Dataframe\n",
    "    logger.info(f\"going to read {args.s3_dataset_path_commments}\")\n",
    "    comments = spark.read.parquet(args.s3_dataset_path_commments, header=True)\n",
    "    logger.info(f\"finished reading files...\")\n",
    "    \n",
    "    logger.info(f\"going to read {args.s3_dataset_path_submissions}\")\n",
    "    submissions = spark.read.parquet(args.s3_dataset_path_submissions, header=True)\n",
    "    logger.info(f\"finished reading files...\")\n",
    "    \n",
    "    # filter the dataframe to only keep the subreddits of interest\n",
    "    subreddits = [s.strip() for s in args.subreddits.split(\",\")]\n",
    "    excluded_cols = [] # ['edited', 'created_utc', 'retrieved_on']\n",
    "    \n",
    "    comments_included_cols = [c for c in comments.columns if c not in excluded_cols]\n",
    "    logger.info(f\"comments included_cols={comments_included_cols}\")\n",
    "\n",
    "    submissions_included_cols = [c for c in submissions.columns if c not in excluded_cols]\n",
    "    logger.info(f\"submissions included_cols={submissions_included_cols}\")\n",
    "\n",
    "    # subset the dataframes because \"edited\" and \"created_utc\" have data type problems\n",
    "    # sometimes they occur as int some time as float and since schema is encoded in the \n",
    "    # parquet files therefore different files have different data tpyes for these fields (float in some cases, int in some cases)\n",
    "    # and spark enforces strict type checking on read so the only option we have is to either\n",
    "    # fix this outside of spark or delete these columns.\n",
    "    comments = comments.select(comments_included_cols)\n",
    "    submissions = submissions.select(submissions_included_cols)\n",
    "    \n",
    "    submissions_filtered = submissions.where(lower(col(\"subreddit\")).isin(subreddits))\n",
    "    comments_filtered = comments.where(lower(col(\"subreddit\")).isin(subreddits))\n",
    "    \n",
    "    # save the filtered dataframes so that these files can now be used for future analysis\n",
    "    s3_path = f\"s3://{args.s3_output_bucket}/{args.s3_output_prefix}/comments_2\"\n",
    "    logger.info(f\"going to write comments for {subreddits} in {s3_path}\")\n",
    "    logger.info(f\"shape of the comments_filtered dataframe is {comments_filtered.count():,}x{len(comments_filtered.columns)}\")\n",
    "    comments_filtered.write.mode(\"overwrite\").parquet(s3_path)\n",
    "    \n",
    "    s3_path = f\"s3://{args.s3_output_bucket}/{args.s3_output_prefix}/submissions_2\"\n",
    "    logger.info(f\"going to write submissions for {subreddits} in {s3_path}\")\n",
    "    logger.info(f\"shape of the submissions_filtered dataframe is {submissions_filtered.count():,}x{len(submissions_filtered.columns)}\")\n",
    "    submissions_filtered.write.mode(\"overwrite\").parquet(s3_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977b71c8-2c10-4179-b0d0-92ed1760e76c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating processing-job with name sm-spark-project-2024-11-08-16-56-59-922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".......................................................................................................................................................................................................................................................................................................................................................................!CPU times: user 2.32 s, sys: 256 ms, total: 2.58 s\n",
      "Wall time: 35min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import sagemaker\n",
    "from sagemaker.spark.processing import PySparkProcessor\n",
    "\n",
    "# Setup the PySpark processor to run the job. Note the instance type and instance count parameters. SageMaker will create these many instances of this type for the spark job.\n",
    "role = sagemaker.get_execution_role()\n",
    "spark_processor = PySparkProcessor(\n",
    "    base_job_name=\"sm-spark-project\",\n",
    "    framework_version=\"3.3\",\n",
    "    role=role,\n",
    "    instance_count=4,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    max_runtime_in_seconds=3600,\n",
    ")\n",
    "\n",
    "# s3 paths\n",
    "session = sagemaker.Session()\n",
    "bucket = session.default_bucket()\n",
    "s3_dataset_path_commments = \"s3://bigdatateaching/reddit-project/reddit/parquet/comments/yyyy=*/mm=*/*.parquet\"\n",
    "s3_dataset_path_submissions = \"s3://bigdatateaching/reddit-project/reddit/parquet/submissions/yyyy=*/mm=*/*.parquet\"\n",
    "output_prefix_data = \"project\"\n",
    "output_prefix_logs = f\"spark_logs\"\n",
    "\n",
    "# modify this comma separated list to choose the subreddits of interest\n",
    "#subreddits = \"technology,chatgpt\"\n",
    "subreddits = \"alabama,alaska,arizona,arkansas,california,colorado,connecticut,delaware,florida,georgia,hawaii,idaho,illinois,indiana,iowa,kansas,kentucky,louisiana,maine,maryland,massachusetts,michigan,minnesota,mississippi,missouri,montana,nebraska,nevada\"\n",
    "    \n",
    "# run the job now, the arguments array is provided as command line to the Python script (Spark code in this case).\n",
    "spark_processor.run(\n",
    "    submit_app=\"./pull_data/process.py\",\n",
    "    arguments=[\n",
    "        \"--s3_dataset_path_commments\",\n",
    "        s3_dataset_path_commments,\n",
    "        \"--s3_dataset_path_submissions\",\n",
    "        s3_dataset_path_submissions,\n",
    "        \"--s3_output_bucket\",\n",
    "              bucket,\n",
    "        \"--s3_output_prefix\",\n",
    "        output_prefix_data,\n",
    "        \"--subreddits\",\n",
    "        subreddits,\n",
    "    ],\n",
    "    spark_event_logs_s3_uri=\"s3://{}/{}/spark_event_logs\".format(bucket, output_prefix_logs),\n",
    "    logs=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd052ce-c7b8-4253-94d6-8267d778e735",
   "metadata": {},
   "source": [
    "### Re-write process.py for another pull\n",
    "\n",
    "For the r/newhampshire, newjersey, newmexico, newyork, northcarolina, northdakota, ohio, oklahoma, oregon, pennsylvania, rhodeisland, southcarolina, southdakota, tennessee, texas, utah, vermont, virginia, washington, westvirginia, wisconsin, wyoming subreddits\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b96dfb5-f547-4354-b27b-fbdc1d3c55a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./pull_data/process.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./pull_data/process.py\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import argparse\n",
    "\n",
    "# Import pyspark and build Spark session\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import (\n",
    "    DoubleType,\n",
    "    IntegerType,\n",
    "    StringType,\n",
    "    StructField,\n",
    "    StructType,\n",
    ")\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s,%(levelname)s,%(module)s,%(filename)s,%(lineno)d,%(message)s', level=logging.DEBUG)\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(logging.StreamHandler(sys.stdout))\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"app inputs and outputs\")\n",
    "    parser.add_argument(\"--s3_dataset_path_commments\", type=str, help=\"Path of dataset in S3 for reddit comments\")\n",
    "    parser.add_argument(\"--s3_dataset_path_submissions\", type=str, help=\"Path of dataset in S3 for reddit submissions\")\n",
    "    parser.add_argument(\"--s3_output_bucket\", type=str, help=\"s3 output bucket\")\n",
    "    parser.add_argument(\"--s3_output_prefix\", type=str, help=\"s3 output prefix\")\n",
    "    parser.add_argument(\"--subreddits\", type=str, help=\"comma separate list of subreddits of interest\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    spark = SparkSession.builder.appName(\"PySparkApp\").getOrCreate()\n",
    "    logger.info(f\"spark version = {spark.version}\")\n",
    "    \n",
    "    # This is needed to save RDDs which is the only way to write nested Dataframes into CSV format\n",
    "    sc = spark.sparkContext\n",
    "    sc._jsc.hadoopConfiguration().set(\n",
    "        \"mapred.output.committer.class\", \"org.apache.hadoop.mapred.FileOutputCommitter\"\n",
    "    )\n",
    "\n",
    "   \n",
    "    # Downloading the data from S3 into a Dataframe\n",
    "    logger.info(f\"going to read {args.s3_dataset_path_commments}\")\n",
    "    comments = spark.read.parquet(args.s3_dataset_path_commments, header=True)\n",
    "    logger.info(f\"finished reading files...\")\n",
    "    \n",
    "    logger.info(f\"going to read {args.s3_dataset_path_submissions}\")\n",
    "    submissions = spark.read.parquet(args.s3_dataset_path_submissions, header=True)\n",
    "    logger.info(f\"finished reading files...\")\n",
    "    \n",
    "    # filter the dataframe to only keep the subreddits of interest\n",
    "    subreddits = [s.strip() for s in args.subreddits.split(\",\")]\n",
    "    excluded_cols = [] # ['edited', 'created_utc', 'retrieved_on']\n",
    "    \n",
    "    comments_included_cols = [c for c in comments.columns if c not in excluded_cols]\n",
    "    logger.info(f\"comments included_cols={comments_included_cols}\")\n",
    "\n",
    "    submissions_included_cols = [c for c in submissions.columns if c not in excluded_cols]\n",
    "    logger.info(f\"submissions included_cols={submissions_included_cols}\")\n",
    "\n",
    "    # subset the dataframes because \"edited\" and \"created_utc\" have data type problems\n",
    "    # sometimes they occur as int some time as float and since schema is encoded in the \n",
    "    # parquet files therefore different files have different data tpyes for these fields (float in some cases, int in some cases)\n",
    "    # and spark enforces strict type checking on read so the only option we have is to either\n",
    "    # fix this outside of spark or delete these columns.\n",
    "    comments = comments.select(comments_included_cols)\n",
    "    submissions = submissions.select(submissions_included_cols)\n",
    "    \n",
    "    submissions_filtered = submissions.where(lower(col(\"subreddit\")).isin(subreddits))\n",
    "    comments_filtered = comments.where(lower(col(\"subreddit\")).isin(subreddits))\n",
    "    \n",
    "    # save the filtered dataframes so that these files can now be used for future analysis\n",
    "    s3_path = f\"s3://{args.s3_output_bucket}/{args.s3_output_prefix}/comments_3\"\n",
    "    logger.info(f\"going to write comments for {subreddits} in {s3_path}\")\n",
    "    logger.info(f\"shape of the comments_filtered dataframe is {comments_filtered.count():,}x{len(comments_filtered.columns)}\")\n",
    "    comments_filtered.write.mode(\"overwrite\").parquet(s3_path)\n",
    "    \n",
    "    s3_path = f\"s3://{args.s3_output_bucket}/{args.s3_output_prefix}/submissions_3\"\n",
    "    logger.info(f\"going to write submissions for {subreddits} in {s3_path}\")\n",
    "    logger.info(f\"shape of the submissions_filtered dataframe is {submissions_filtered.count():,}x{len(submissions_filtered.columns)}\")\n",
    "    submissions_filtered.write.mode(\"overwrite\").parquet(s3_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833dd696-5aa1-4b9d-b149-747f6a4af989",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating processing-job with name sm-spark-project-2024-11-08-17-33-24-773\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".................................................................................................................................................................................................................................................................................................................................................................................................................................!CPU times: user 2.18 s, sys: 210 ms, total: 2.39 s\n",
      "Wall time: 35min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import sagemaker\n",
    "from sagemaker.spark.processing import PySparkProcessor\n",
    "\n",
    "# Setup the PySpark processor to run the job. Note the instance type and instance count parameters. SageMaker will create these many instances of this type for the spark job.\n",
    "role = sagemaker.get_execution_role()\n",
    "spark_processor = PySparkProcessor(\n",
    "    base_job_name=\"sm-spark-project\",\n",
    "    framework_version=\"3.3\",\n",
    "    role=role,\n",
    "    instance_count=4,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    max_runtime_in_seconds=3600,\n",
    ")\n",
    "\n",
    "# s3 paths\n",
    "session = sagemaker.Session()\n",
    "bucket = session.default_bucket()\n",
    "s3_dataset_path_commments = \"s3://bigdatateaching/reddit-project/reddit/parquet/comments/yyyy=*/mm=*/*.parquet\"\n",
    "s3_dataset_path_submissions = \"s3://bigdatateaching/reddit-project/reddit/parquet/submissions/yyyy=*/mm=*/*.parquet\"\n",
    "output_prefix_data = \"project\"\n",
    "output_prefix_logs = f\"spark_logs\"\n",
    "\n",
    "# modify this comma separated list to choose the subreddits of interest\n",
    "#subreddits = \"technology,chatgpt\"\n",
    "subreddits = \"newhampshire,newjersey,newmexico,newyork,northcarolina,northdakota,ohio,oklahoma,oregon,pennsylvania,rhodeisland,southcarolina,southdakota,tennessee,texas,utah,vermont,virginia,washington,westvirginia,wisconsin,wyoming\"\n",
    "    \n",
    "# run the job now, the arguments array is provided as command line to the Python script (Spark code in this case).\n",
    "spark_processor.run(\n",
    "    submit_app=\"./pull_data/process.py\",\n",
    "    arguments=[\n",
    "        \"--s3_dataset_path_commments\",\n",
    "        s3_dataset_path_commments,\n",
    "        \"--s3_dataset_path_submissions\",\n",
    "        s3_dataset_path_submissions,\n",
    "        \"--s3_output_bucket\",\n",
    "              bucket,\n",
    "        \"--s3_output_prefix\",\n",
    "        output_prefix_data,\n",
    "        \"--subreddits\",\n",
    "        subreddits,\n",
    "    ],\n",
    "    spark_event_logs_s3_uri=\"s3://{}/{}/spark_event_logs\".format(bucket, output_prefix_logs),\n",
    "    logs=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f5fd90-df00-4d7f-ad90-9f7b4bc91b8d",
   "metadata": {},
   "source": [
    "## Read and combine the filtered data\n",
    "\n",
    "Now that we have filtered the data to only keep submissions and comments from subreddits of interest. Let us read data from the s3 path where we saved the filtered data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a12a84b-8342-4bc5-95b1-c80d63dea79e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading comments from s3a://sagemaker-us-east-1-562201516459/project/comments\n",
      "reading comments from s3a://sagemaker-us-east-1-562201516459/project/comments_2\n",
      "reading comments from s3a://sagemaker-us-east-1-562201516459/project/comments_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 18:=====================================================>(361 + 1) / 362]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------+\n",
      "|    subreddit|  count|\n",
      "+-------------+-------+\n",
      "|       travel|1692634|\n",
      "|    usatravel|   3541|\n",
      "|AskAnAmerican|1484279|\n",
      "|       Hawaii| 229058|\n",
      "|     Arkansas| 206619|\n",
      "|       alaska| 155213|\n",
      "|  Connecticut| 596502|\n",
      "|     maryland| 374793|\n",
      "|     Nebraska|  92045|\n",
      "|massachusetts| 532337|\n",
      "|       Nevada|  43471|\n",
      "|  mississippi| 120069|\n",
      "|     Delaware| 121224|\n",
      "|      arizona| 212745|\n",
      "|     illinois| 151436|\n",
      "|      florida| 983754|\n",
      "|      Georgia| 289066|\n",
      "|      Montana|  94978|\n",
      "|     Michigan| 417908|\n",
      "|      Alabama| 134993|\n",
      "+-------------+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      "CPU times: user 821 ms, sys: 179 ms, total: 1e+03 ms\n",
      "Wall time: 50min 29s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Read comments data\n",
    "s3_path_1 = f\"s3a://{bucket}/{output_prefix_data}/comments\"\n",
    "s3_path_2 = f\"s3a://{bucket}/{output_prefix_data}/comments_2\"\n",
    "s3_path_3 = f\"s3a://{bucket}/{output_prefix_data}/comments_3\"\n",
    "\n",
    "print(f\"reading comments from {s3_path_1}\")\n",
    "comments_1 = spark.read.parquet(s3_path_1, header=True)\n",
    "print(f\"reading comments from {s3_path_2}\")\n",
    "comments_2 = spark.read.parquet(s3_path_2, header=True)\n",
    "print(f\"reading comments from {s3_path_3}\")\n",
    "comments_3 = spark.read.parquet(s3_path_3, header=True)\n",
    "\n",
    "# Append comments data\n",
    "comments = comments_1.union(comments_2)\n",
    "comments = comments.union(comments_3)\n",
    "print(f\"shape of the comments dataframe is {comments.count():,}x{len(comments.columns)}\")\n",
    "\n",
    "# Print schema\n",
    "print(comments.printSchema)\n",
    "comments.groupBy('subreddit').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3f689cd4-505f-4242-8e6e-4d7664cc6e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading submissions from s3a://sagemaker-us-east-1-562201516459/project/submissions\n",
      "reading comments from s3a://sagemaker-us-east-1-562201516459/project/submissions_2\n",
      "reading comments from s3a://sagemaker-us-east-1-562201516459/project/submissions_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the submissions dataframe is 697,043x21\n",
      "<bound method DataFrame.printSchema of DataFrame[author: string, author_flair_css_class: string, author_flair_text: string, created_utc: bigint, distinguished: string, domain: string, edited: double, id: string, is_self: boolean, locked: boolean, num_comments: bigint, over_18: boolean, quarantine: boolean, retrieved_on: bigint, score: bigint, selftext: string, stickied: boolean, subreddit: string, subreddit_id: string, title: string, url: string]>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 33:======================================================> (54 + 1) / 55]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------+\n",
      "|    subreddit| count|\n",
      "+-------------+------+\n",
      "|       travel|169797|\n",
      "|    usatravel|   880|\n",
      "|AskAnAmerican| 30216|\n",
      "|       Hawaii| 11515|\n",
      "|     Arkansas|  7360|\n",
      "|       alaska|  7925|\n",
      "|  Connecticut| 21102|\n",
      "|     maryland| 15686|\n",
      "|     Nebraska|  2991|\n",
      "|massachusetts| 16189|\n",
      "|       Nevada|  2084|\n",
      "|  mississippi|  4362|\n",
      "|     Delaware|  6207|\n",
      "|      arizona| 10510|\n",
      "|     illinois|  6594|\n",
      "|      florida| 29461|\n",
      "|      Georgia| 10063|\n",
      "|      Montana|  4392|\n",
      "|     Michigan| 13109|\n",
      "|      Alabama|  5505|\n",
      "+-------------+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "CPU times: user 133 ms, sys: 20.8 ms, total: 154 ms\n",
      "Wall time: 6min 56s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Read submissions data\n",
    "s3_path_1 = f\"s3a://{bucket}/{output_prefix_data}/submissions\"\n",
    "s3_path_2 = f\"s3a://{bucket}/{output_prefix_data}/submissions_2\"\n",
    "s3_path_3 = f\"s3a://{bucket}/{output_prefix_data}/submissions_3\"\n",
    "\n",
    "print(f\"reading submissions from {s3_path_1}\")\n",
    "submissions_1 = spark.read.parquet(s3_path_1, header=True)\n",
    "print(f\"reading comments from {s3_path_2}\")\n",
    "submissions_2 = spark.read.parquet(s3_path_2, header=True)\n",
    "print(f\"reading comments from {s3_path_3}\")\n",
    "submissions_3 = spark.read.parquet(s3_path_3, header=True)\n",
    "\n",
    "# Append submissions data\n",
    "submissions = submissions_1.union(submissions_2)\n",
    "submissions = submissions.union(submissions_3)\n",
    "print(f\"shape of the submissions dataframe is {submissions.count():,}x{len(submissions.columns)}\")\n",
    "\n",
    "# Print schema\n",
    "print(submissions.printSchema)\n",
    "submissions.groupBy('subreddit').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080e51cf-3720-47aa-b0c8-7eb68e784718",
   "metadata": {},
   "source": [
    "### Save data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25231e57-993c-4ac0-ab1c-2c5e71481e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "output_path_1 = f\"s3a://{bucket}/{output_prefix_data}/comments_filt\"\n",
    "output_path_2 = f\"s3a://{bucket}/{output_prefix_data}/submissions_filt\"\n",
    "comments.write.mode(\"overwrite\").parquet(output_path_1)\n",
    "submissions.write.mode(\"overwrite\").parquet(output_path_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "48d1afca-4aaa-495e-a395-4436a2cb9825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading submissions from s3a://sagemaker-us-east-1-562201516459/project/comments_filt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 41:======================================================> (32 + 1) / 33]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the comments dataframe is 17,667,099x17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Test that data was saved correctly\n",
    "s3_path_1 = f\"s3a://{bucket}/{output_prefix_data}/comments_filt\"\n",
    "print(f\"reading submissions from {s3_path_1}\")\n",
    "comments = spark.read.parquet(s3_path_1, header=True)\n",
    "print(f\"shape of the comments dataframe is {comments.count():,}x{len(comments.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "64af796d-9a1b-497f-8c04-220fd09ac33c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading submissions from s3a://sagemaker-us-east-1-562201516459/project/submissions_filt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 45:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the comments dataframe is 697,043x21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Test that data was saved correctly\n",
    "s3_path_1 = f\"s3a://{bucket}/{output_prefix_data}/submissions_filt\"\n",
    "print(f\"reading submissions from {s3_path_1}\")\n",
    "comments = spark.read.parquet(s3_path_1, header=True)\n",
    "print(f\"shape of the comments dataframe is {comments.count():,}x{len(comments.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1831e73e-e9e3-4c90-b620-625336e6bf2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
